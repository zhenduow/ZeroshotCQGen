{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 0. imports, definitions, processing, hyperparameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'base' requires ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import json\n",
        "import sys\n",
        "import time\n",
        "import math\n",
        "import pprint\n",
        "import numpy as np\n",
        "import torch as T\n",
        "import openai\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
        "from transformers import GPT2LMHeadModel,  GPT2Tokenizer, GPT2Config\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from transformers import PhrasalConstraint\n",
        "from transformers import pipeline, set_seed\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from word_forms.word_forms import get_word_forms\n",
        "\n",
        "import rouge \n",
        "rs = rouge.Rouge()\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk.data\n",
        "nltk_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "\n",
        "import logging\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.CRITICAL)\n",
        "\n",
        "import spacy\n",
        "spacy.prefer_gpu()\n",
        "\n",
        "pos_tagger = spacy.load('en_core_web_sm')\n",
        "ps = PorterStemmer()\n",
        "rs = rouge.Rouge()\n",
        "\n",
        "template_len = 4\n",
        "batch_size = 32\n",
        "sample_every = 100\n",
        "epochs = 8\n",
        "learning_rate = 5e-5\n",
        "warmup_steps = 1e2\n",
        "epsilon = 1e-8\n",
        "max_length = 128\n",
        "device = T.device(\"cuda\")\n",
        "\n",
        "SEP = '[SEP]'\n",
        "PAD = '[PAD]'\n",
        "BOS = '[BOS]'\n",
        "EOS = '[EOS]'\n",
        "\n",
        "starting_texts = [\n",
        "                ## template from this paper https://dl.acm.org/doi/abs/10.1145/3409256.3409817#:~:text=Recent%20research%20on%20conversational%20search%20highlights%20the%20importance,lexical%20baseline%2Cthat%20significantly%20outperforms%20the%20existing%20naive%20baselines.\n",
        "                \"[SEP] are you looking for\",\n",
        "                \"[SEP] do you want to know\",\n",
        "                \"[SEP] would you like to\",\n",
        "                \"[SEP] are you interested in\",\n",
        "                \"[SEP] do you need information\",\n",
        "                \"[SEP] do you want information\",\n",
        "                \"[SEP] do you need to\",\n",
        "                \"[SEP] do you want to\",\n",
        "            ]\n",
        "\n",
        "openai.api_key = ''\n",
        "gpt3_examples = [\"Find condos in Florida. Ask a question that contains words in the list ['specific', 'city']. Are you interested in any specific city in florida?\",\n",
        "        \"What should I know about living in India? Ask a question that contains words in the list ['challenges']. Would you like to know about the economic challenges of living in India?\"\n",
        "        \"Tell me more about Euclid. Ask a question that contains words in the list ['greece', 'math']. would you like to know what impact Euclid had on mathematics in ancient Greece?\"\n",
        "]\n",
        "\n",
        "seed_val = 2022\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "T.manual_seed(seed_val)\n",
        "T.cuda.manual_seed_all(seed_val)\n",
        "set_seed(seed_val)\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', bos_token=BOS, eos_token=EOS, pad_token=PAD)\n",
        "configuration = GPT2Config.from_pretrained('gpt2', output_hidden_states=False, bos_token=BOS, eos_token=EOS, pad_token=PAD)\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=configuration) \n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model.cuda()\n",
        "\n",
        "ppl_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "ppl_configuration = GPT2Config.from_pretrained('gpt2', output_hidden_states=False)\n",
        "ppl_model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=ppl_configuration) \n",
        "ppl_model.resize_token_embeddings(len(ppl_tokenizer))\n",
        "ppl_model.cuda()\n",
        "\n",
        "## create train/test split for reproduction, RUN ONLY ONCE!\n",
        "if not (os.path.exists('data/question_cases_answered_test.csv') and os.path.exists('data/question_cases_answered_train_dev.csv')):\n",
        "    print(\"Generating Usi train/test split for reproduction.\")\n",
        "    usi_train_file = 'data/question_cases_answered.csv'\n",
        "    usi_train_data = pd.read_csv(usi_train_file) \n",
        "\n",
        "    topic_id_set = set(usi_train_data['topic_id'])\n",
        "    test_topic = random.sample(topic_id_set, int(0.2 * len(topic_id_set)))\n",
        "\n",
        "    usi_test = usi_train_data.loc[usi_train_data['topic_id'].isin(test_topic)]\n",
        "    usi_train_dev = usi_train_data.drop(usi_test.index)\n",
        "\n",
        "    usi_test = usi_test.sort_values(by='topic_id')\n",
        "    usi_train_dev = usi_train_dev.sort_values(by='topic_id')\n",
        "\n",
        "    usi_test.to_csv('data/question_cases_answered_test.csv', index=False)\n",
        "    usi_train_dev.to_csv('data/question_cases_answered_train_dev.csv', index=False)\n",
        "\n",
        "## create train/test split for reproduction, RUN ONLY ONCE!\n",
        "if not (os.path.exists('data/clariq_f/ClariQ-FKw-trial.tsv') and os.path.exists('data/clariq_f/ClariQ-FKw-train_no_trial.tsv') ):\n",
        "    print(\"Generating clariq-f train/test split for reproduction.\")\n",
        "    train_file = 'data/clariq_f/ClariQ-FKw-train.tsv'\n",
        "    train_data = pd.read_csv(train_file, sep='\\t') \n",
        "\n",
        "    topic_id_set = set(train_data['topic_id'])\n",
        "    random.seed(17)\n",
        "    topics = random.sample(topic_id_set, int(0.056 * len(topic_id_set)))\n",
        "\n",
        "    trial_data = train_data.loc[train_data['topic_id'].isin(topics)]\n",
        "    train_no_trial = train_data.drop(trial_data.index)\n",
        "\n",
        "    trial_data = trial_data.sort_values(by='topic_id')\n",
        "    train_no_trial = train_no_trial.sort_values(by='topic_id')\n",
        "\n",
        "    trial_data.to_csv('data/clariq_f/ClariQ-FKw-trial.tsv', index=False, sep = '\\t')\n",
        "    train_no_trial.to_csv('data/clariq_f/ClariQ-FKw-train_no_trial.tsv', index=False, sep = '\\t')\n",
        "\n",
        "def process_clariq_f(data):\n",
        "    data_dict = {}\n",
        "    data = data.dropna(subset=['question', 'initial_request'])\n",
        "    for iter, row in data.iterrows():\n",
        "        q = str(data.at[iter, 'initial_request'])\n",
        "        cq = str(data.at[iter, 'question'])\n",
        "        f = str(data.at[iter, 'facet_desc'])\n",
        "\n",
        "        data.at[iter, 'f_q'] = f + SEP + q\n",
        "        data.at[iter, 'f_q_cq'] = f + SEP + q + BOS + cq + EOS\n",
        "        data.at[iter, 'q_f'] = q + SEP + f\n",
        "        data.at[iter, 'q_f_cq'] = q + SEP + f + BOS + cq + EOS\n",
        "        data.at[iter, 'instructional_q_f_cq'] = q + ' '+ \"Ask a question that contains words in the list\" + ' ' + \"[\" + \", \".join([\"'\"+w+\"'\" for w in f.split()])  + '].' + ' ' + cq\n",
        "        data.at[iter, 'instructional_q_f'] = q + ' '+ \"Ask a question that contains words in the list\" + ' ' + \"[\" + \", \".join([\"'\"+w+\"'\" for w in f.split()])  + '].'\n",
        "        \n",
        "    return data_dict, data\n",
        "\n",
        "def compute_average_rouge(rouge_list):\n",
        "    '''\n",
        "    this function computes the average rouge f,p,r of a list of rouge scores\n",
        "    rouge_list is a list of dictionaries in the following format:\n",
        "    {\n",
        "        \"rouge-1\": {\n",
        "            \"f\": 0.4786324739396596,\n",
        "            \"p\": 0.6363636363636364,\n",
        "            \"r\": 0.3835616438356164\n",
        "            },\n",
        "        \"rouge-2\": {\n",
        "            \"f\": 0.2608695605353498,\n",
        "            \"p\": 0.3488372093023256,\n",
        "            \"r\": 0.20833333333333334\n",
        "            },\n",
        "        \"rouge-l\": {\n",
        "            \"f\": 0.44705881864636676,\n",
        "            \"p\": 0.5277777777777778,\n",
        "            \"r\": 0.3877551020408163\n",
        "            }\n",
        "    }\n",
        "    '''\n",
        "    # if length of rouge_list is 1 or it is not cast as a list of dicts\n",
        "    if isinstance(rouge_list, dict):\n",
        "        return rouge_list\n",
        "    r_dict = {\n",
        "        \"rouge-1\": {\n",
        "            \"f\": 0,\n",
        "            \"p\": 0,\n",
        "            \"r\": 0\n",
        "            },\n",
        "        \"rouge-2\": {\n",
        "            \"f\": 0,\n",
        "            \"p\": 0,\n",
        "            \"r\": 0\n",
        "            },\n",
        "        \"rouge-l\": {\n",
        "            \"f\": 0,\n",
        "            \"p\": 0,\n",
        "            \"r\": 0\n",
        "            }\n",
        "    }\n",
        "    for d in rouge_list:\n",
        "        for k_len in d.keys():\n",
        "            for k in d[k_len].keys():\n",
        "                r_dict[k_len][k] += d[k_len][k]\n",
        "    n_hyps = len(rouge_list)\n",
        "    for k_len in r_dict.keys():\n",
        "        for k in r_dict[k_len].keys():\n",
        "            r_dict[k_len][k] /= n_hyps\n",
        "    return r_dict\n",
        "\n",
        "def calculatePerplexity(sentence,model,tokenizer):\n",
        "    tokenize_input = tokenizer.tokenize(sentence)\n",
        "    tensor_input = T.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)]).to(device)\n",
        "    loss=model(tensor_input, labels=tensor_input)[0]\n",
        "    return math.exp(loss)\n",
        "\n",
        "def process_generation(generation):\n",
        "    processed_generation = re.sub(r'\\[SEP\\]',' ', generation)  # remove [SEP]\n",
        "    processed_generation = re.sub(r'[^\\w\\s]',' ', processed_generation)  # remove punctuation\n",
        "    return processed_generation\n",
        "\n",
        "def calculate_WSDM(query, doc_list):\n",
        "    lambda_t = 1\n",
        "    lambda_o = 1\n",
        "    lambda_u = 1\n",
        "    mu = 25\n",
        "    collection = ' '.join(doc_list)\n",
        "    collection_size = len(collection)\n",
        "\n",
        "    def tfq(word, doc):\n",
        "        many_forms = get_word_forms(word)\n",
        "        word_forms = [word for k in many_forms.keys() for word in many_forms[k]] \n",
        "        return sum( [sum([1 if w == wf else 0 for w in doc]) for wf in word_forms])\n",
        "\n",
        "    def tf1(qk, qk1, doc):    \n",
        "        many_formsk = get_word_forms(qk)\n",
        "        word_formsk = list(set([word for k in many_formsk.keys() for word in many_formsk[k]] + [qk])) \n",
        "        many_formsk1 = get_word_forms(qk1)\n",
        "        word_formsk1 = list(set([word for k in many_formsk1.keys() for word in many_formsk1[k]] + [qk1]))\n",
        "        return sum( [sum([1 if qkf == doc[k] and qk1f == doc[k+1] else 0 for k in range(len(doc)-1)]) for qkf in word_formsk for qk1f in word_formsk1])\n",
        "    \n",
        "    def tfuw(qk, qj, doc):\n",
        "        wsz = 2\n",
        "        many_formsk = get_word_forms(qk)\n",
        "        word_formsk = list(set([word for k in many_formsk.keys() for word in many_formsk[k]] + [qk])) \n",
        "        many_formsj = get_word_forms(qj)\n",
        "        word_formsj = list(set([word for k in many_formsj.keys() for word in many_formsj[k]] + [qj])) \n",
        "        return sum( [sum([1 if qkf == doc[k] and qjf in doc[max(k-wsz,0):min(k+wsz,len(doc))] else 0 for k in range(len(doc))]) for qkf in word_formsk for qjf in word_formsj])\n",
        "\n",
        "    def f_t(query, doc, collection):\n",
        "        res = sum([(tfq(word, doc.split()) + mu * tfq(word, collection.split())/collection_size) / (len(doc.split()) + mu) for word in query.split() ])\n",
        "        #print(query, doc)\n",
        "        #print(\"ft\", res)\n",
        "        return res\n",
        "    \n",
        "    def f_o(query, doc, collection):\n",
        "        query = query.split()\n",
        "        if len(query) < 2:\n",
        "            return 0\n",
        "        res = sum([(tf1(query[k], query[k+1], doc.split()) + mu * tf1(query[k], query[k+1], collection.split())/collection_size) / (len(doc.split()) + mu)  for k in range(len(query)-1)])\n",
        "        \n",
        "        #print(query, doc)\n",
        "        #print(\"fo\", res)\n",
        "        return res\n",
        "\n",
        "    def f_u(query, doc, collection):\n",
        "        query = list(set(query.split()))\n",
        "        l = len(query)\n",
        "        if l < 2:\n",
        "            return 0\n",
        "        res = sum([(tfuw(query[k], query[j], doc.split()) + mu * tfuw(query[k], query[j], collection.split())/collection_size) / (len(doc.split()) + mu)  for k in range(l) for j in range(k+1, l)])\n",
        "        #print(query, doc)\n",
        "        #print(\"fu\", res)\n",
        "        return res\n",
        "\n",
        "    return {\n",
        "        doc:lambda_t * f_t(query, doc, collection) + \\\n",
        "            lambda_o * f_o(query, doc, collection) + \\\n",
        "            lambda_u * f_u(query, doc, collection) \n",
        "        for doc in doc_list\n",
        "    }\n",
        "\n",
        "def round_metric(num):\n",
        "    return round(num * 100, 2)\n",
        "\n",
        "def auto_evaluation(ref, hyp, facet):\n",
        "    ref = ref.strip()\n",
        "    hyp = hyp.strip()\n",
        "    tokenized_ref = word_tokenize(ref)\n",
        "    tokenized_hyp = word_tokenize(hyp)\n",
        "\n",
        "    rouge_score = rs.get_scores(hyp, ref)[0]['rouge-l']['f'] if hyp != '' else 0\n",
        "\n",
        "    return sentence_bleu([tokenized_ref], \n",
        "                            tokenized_hyp, \n",
        "                            weights=(1, 0, 0, 0),\n",
        "                            smoothing_function = SmoothingFunction().method1),\\\n",
        "            sentence_bleu([tokenized_ref], \n",
        "                            tokenized_hyp, \n",
        "                            weights=(1, 1, 0, 0),\n",
        "                            smoothing_function = SmoothingFunction().method1),\\\n",
        "            sentence_bleu([tokenized_ref], \n",
        "                            tokenized_hyp, \n",
        "                            weights=(1, 1, 1, 0),\n",
        "                            smoothing_function = SmoothingFunction().method1),\\\n",
        "            sentence_bleu([tokenized_ref], \n",
        "                            tokenized_hyp, \n",
        "                            weights=(1, 1, 1, 1),\n",
        "                            smoothing_function = SmoothingFunction().method1),\\\n",
        "            meteor_score([' '.join(tokenized_ref)], ' '.join(tokenized_hyp)),\\\n",
        "            rouge_score,\\\n",
        "            1 - sum([1 if ps.stem(constraint) not in set([ps.stem(w) for w in word_tokenize(hyp)]) else 0 for constraint in facet.split() ]) / len(facet.split())\n",
        "\n",
        "def evaluate_from_output(model_output):\n",
        "    \n",
        "    b1, b2, b3, b4 = [], [], [], []\n",
        "    m = []\n",
        "    r = []\n",
        "    c = []\n",
        "\n",
        "    t_b1, t_b2, t_b3, t_b4 = [], [], [], []\n",
        "    t_m = []\n",
        "    t_r = []\n",
        "    t_c = []\n",
        "\n",
        "    model_output_data = pd.read_csv(model_output)\n",
        "    for iter, row in model_output_data.iterrows():\n",
        "        query = model_output_data.at[iter, 'query']\n",
        "        facet = model_output_data.at[iter, 'facet']\n",
        "        ref = model_output_data.at[iter, 'reference']\n",
        "        generated_cq = model_output_data.at[iter, 'candidate']\n",
        "        \n",
        "        if iter % sample_every == 0: \n",
        "            print(iter, query, \"-\", facet, '-', generated_cq)\n",
        "        \n",
        "        # full reference evaluation\n",
        "        hyp_b1, hyp_b2, hyp_b3, hyp_b4, hyp_m, hyp_r, hyp_c = auto_evaluation(ref, generated_cq, facet)\n",
        "\n",
        "        b1.append(hyp_b1)\n",
        "        b2.append(hyp_b2)\n",
        "        b3.append(hyp_b3)\n",
        "        b4.append(hyp_b4)\n",
        "        m.append(hyp_m)\n",
        "        r.append(hyp_r)\n",
        "        c.append(hyp_c)\n",
        "\n",
        "        # question body evaluation\n",
        "        truncate_ref = ' '.join(ref.split()[template_len:])\n",
        "        truncate_generated_cq = ' '.join(generated_cq.split()[template_len:])\n",
        "        \n",
        "        t_hyp_b1, t_hyp_b2, t_hyp_b3, t_hyp_b4, t_hyp_m, t_hyp_r, t_hyp_c = auto_evaluation(truncate_ref, truncate_generated_cq, facet)\n",
        "\n",
        "        t_b1.append(t_hyp_b1)\n",
        "        t_b2.append(t_hyp_b2)\n",
        "        t_b3.append(t_hyp_b3)\n",
        "        t_b4.append(t_hyp_b4)\n",
        "        t_m.append(t_hyp_m)\n",
        "        t_r.append(t_hyp_r)\n",
        "        t_c.append(t_hyp_c)\n",
        "    \n",
        "    return b1, b2, b3, b4, m, r, c,\\\n",
        "           t_b1, t_b2, t_b3, t_b4, t_m, t_r, t_c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# RQ1.  How effective is facet information for clarifying question generation?\n",
        "\n",
        "To answer this question, we compare our proposed zero-shot facet-constrained approach with a similar method but using query subject instead of facet for constraints."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 1.1 No-facet (query subject) + neurologic decoding + WSDM ranker\n",
        "### 1.1.1 Generate inputs for neurologic decoding. \n",
        "\n",
        "* Generate constraints file from query subjects\n",
        "* Generate generation inputs file in the form of {query} + {template}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from word_forms.word_forms import get_word_forms\n",
        "facet_test_file = 'data/clariq_f/ClariQ-FKw-dev.tsv'\n",
        "facet_test_data = pd.read_csv(facet_test_file, sep='\\t')\n",
        "\n",
        "write_to_file = 'neurologic_decoding/dataset/clean/constraint/test.constraint.json'\n",
        "prompt_write_to_file = 'neurologic_decoding/dataset/clean/init/commongen.test.init.txt'\n",
        "no_prompt_write_to_file = 'neurologic_decoding/dataset/clean/init/commongen_no_prompt.test.init.txt'\n",
        "\n",
        "pos_tagger = spacy.load('en_core_web_sm')\n",
        "\n",
        "all_queries= []\n",
        "all_constraints = []\n",
        "for iter, row in facet_test_data.iterrows():\n",
        "    facet = facet_test_data.at[iter, 'facet_desc']\n",
        "    query = facet_test_data.at[iter, 'initial_request']\n",
        "    noun_in_query = [token.text for token in pos_tagger(query) if token.pos_ == 'NOUN']\n",
        "    propn_in_query = ' '.join([token.text.lower() for token in pos_tagger(query) if token.pos_ == 'PROPN'])\n",
        "    all_queries.append(query)\n",
        "    constraints = [[term] for term in noun_in_query]\n",
        "    if propn_in_query != '':\n",
        "        constraints += [[propn_in_query]] \n",
        "    all_constraints.append(constraints)\n",
        "\n",
        "\n",
        "with open(write_to_file, 'w') as output:\n",
        "    for constraints in all_constraints:\n",
        "        for k, prompt in enumerate(starting_texts):\n",
        "            json_str = json.dumps(constraints)\n",
        "            output.write(json_str)\n",
        "            output.write('\\n')\n",
        "\n",
        "with open(prompt_write_to_file, 'w') as output:\n",
        "    for query in all_queries:\n",
        "        for k, prompt in enumerate(starting_texts):\n",
        "            output.write(query + prompt)\n",
        "            output.write('\\n')\n",
        "\n",
        "with open(no_prompt_write_to_file, 'w') as output:\n",
        "    for query in all_queries:\n",
        "        output.write(query)\n",
        "        output.write('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 1.1.2 Run neurologic decoding.\n",
        "In AML terminal:\n",
        "\n",
        "Set up neurologic decoding environment.\n",
        "```\n",
        "cd neurologic_decoding\n",
        "conda create -n hug python=3.7\n",
        "conda activate hug\n",
        "pip install -r huggingface.txt\n",
        "```\n",
        "Run the generation code.\n",
        "```\n",
        "cd neurologic_decoding/zero_shot\n",
        "conda activate hug\n",
        "export PYTHONPATH=/home/azureuser/cloudfiles/code/Users/t-zhendwang/srconvsearch/neurologic_decoding\n",
        "bash decode_pt.sh 0 test gpt2nofacet\n",
        "``` \n",
        "\n",
        "Make sure we get the generation file 'gpt2nofacet'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 1.1.3 Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 tell me about cass county missouri - list homes sale - do you want to know what cass county cass county is cass county\n",
            "100 Find information on ontario california airport. - directions location - would you like to visit ontario ontario information ontario\n",
            "200 Where can I buy pressure washers? - washer - are you looking for washers or pressure washer\n",
            "300 Tell me more about Rocky Mountain News - recent events historical - are you looking for news about rocky mountain news\n",
            "400 Where should I order dog clean-up bags - specif bag type - would you like to order dog cleaning bags\n",
            "================================================================\n",
            "Full reference evaluation\n",
            "================================================================\n",
            "b1 0.2915715948377834 b2 0.12806654260239403 b3 0.07602362595745463 b4 0.051360580789430024\n",
            "rouge-L 0.34784465264878195\n",
            "m 0.28592940767560077\n",
            "c 0.09819607843137255\n",
            "================================================================\n",
            "Question body evaluation\n",
            "================================================================\n",
            "b1 0.1451359435328646 b2 0.043740224796047354 b3 0.020391620599505958 b4 0.025673300004377137\n",
            "rouge-L 0.1939367199929307\n",
            "m 0.14507277229967497\n",
            "c 0.09447058823529413\n"
          ]
        }
      ],
      "source": [
        "facet_test_file = 'data/clariq_f/ClariQ-FKw-dev.tsv'\n",
        "facet_test_data = pd.read_csv(facet_test_file, sep='\\t')\n",
        "_, facet_test_data = process_clariq_f(facet_test_data)\n",
        "\n",
        "r = []\n",
        "b1, b2, b3, b4 = [], [], [], []\n",
        "m = []\n",
        "c = []\n",
        "\n",
        "t_r = []\n",
        "t_b1, t_b2, t_b3, t_b4 = [], [], [], []\n",
        "t_m = []\n",
        "t_c = []\n",
        "\n",
        "model_output = 'zeroshot_subject_nd_wsdm.csv'\n",
        "generated_file = 'neurologic_decoding/zero_shot/gpt2nofacet'\n",
        "\n",
        "if os.path.isfile(model_output):\n",
        "    b1, b2, b3, b4, m, r, c, t_b1, t_b2, t_b3, t_b4, t_m, t_r, t_c = evaluate_from_output(model_output)\n",
        "\n",
        "else:\n",
        "    generated_cq_all_templates = open(generated_file, 'r').readlines()\n",
        "    generated_cq_grouped = [[generated_cq_all_templates[len(starting_texts) * k + l] \n",
        "                                for l in range(len(starting_texts))] \n",
        "                                for k in range(int(len(generated_cq_all_templates)/8))]\n",
        "    for iter, row in facet_test_data.iterrows():\n",
        "        query = facet_test_data.at[iter, 'initial_request']\n",
        "        facet = facet_test_data.at[iter, 'facet_desc']\n",
        "        ref = facet_test_data.at[iter, 'question']\n",
        "\n",
        "        generated_cqs = []\n",
        "        for full_sentence in generated_cq_grouped[iter]:\n",
        "            query = re.sub('\\[SEP\\]', '&', full_sentence).split('&')[0].strip()\n",
        "            generated_follow_up = re.sub('\\[SEP\\]', '&', full_sentence).split('&')[1].strip()\n",
        "            generated_cq = re.sub('[.?]', '&', generated_follow_up).split('&')[0].strip()\n",
        "            generated_cqs.append(generated_cq)\n",
        "        \n",
        "        noun_in_query = [token.text for token in pos_tagger(query) if token.pos_ == 'NOUN']\n",
        "        propn_in_query = [token.text.lower() for token in pos_tagger(query) if token.pos_ == 'PROPN']\n",
        "\n",
        "        template_scores = calculate_WSDM(query=' '.join(noun_in_query+propn_in_query), doc_list=generated_cqs)\n",
        "        sorted_template_scores = sorted(template_scores.items(), key = lambda x: x[1], reverse=True)\n",
        "        generated_cq = sorted(template_scores.keys(), key = lambda x: template_scores[x], reverse=True)[0] # Tie breaker? \n",
        "        facet_test_data.at[iter, 'generated'] = generated_cq\n",
        "        \n",
        "        if iter % sample_every == 0: \n",
        "            print(iter, query, \"-\", facet, '-', generated_cq)\n",
        "            #pprint.pprint(sorted_template_scores)\n",
        "\n",
        "        # full reference evaluation\n",
        "        hyp_b1, hyp_b2, hyp_b3, hyp_b4, hyp_m, hyp_r, hyp_c = auto_evaluation(ref, generated_cq, facet)\n",
        "\n",
        "        b1.append(hyp_b1)\n",
        "        b2.append(hyp_b2)\n",
        "        b3.append(hyp_b3)\n",
        "        b4.append(hyp_b4)\n",
        "        m.append(hyp_m)\n",
        "        r.append(hyp_r)\n",
        "        c.append(hyp_c)\n",
        "\n",
        "        # question body evaluation\n",
        "        truncate_ref = ' '.join(ref.split()[template_len:])\n",
        "        truncate_generated_cq = ' '.join(generated_cq.split()[template_len:])\n",
        "        \n",
        "        t_hyp_b1, t_hyp_b2, t_hyp_b3, t_hyp_b4, t_hyp_m, t_hyp_r, t_hyp_c = auto_evaluation(truncate_ref, truncate_generated_cq, facet)\n",
        "\n",
        "        t_b1.append(t_hyp_b1)\n",
        "        t_b2.append(t_hyp_b2)\n",
        "        t_b3.append(t_hyp_b3)\n",
        "        t_b4.append(t_hyp_b4)\n",
        "        t_m.append(t_hyp_m)\n",
        "        t_r.append(t_hyp_r)\n",
        "        t_c.append(t_hyp_c)\n",
        "    \n",
        "    output_df = facet_test_data[['initial_request', 'facet_desc', 'question', 'generated']]\n",
        "    output_df.columns = ['query', 'facet', 'reference', 'candidate']\n",
        "    output_df.to_csv(model_output)\n",
        "\n",
        "# full reference results\n",
        "print(\"================================================================\")\n",
        "print(\"Full reference evaluation\")\n",
        "print(\"================================================================\")\n",
        "print(\"b1\", np.mean(b1), \"b2\", np.mean(b2), \"b3\", np.mean(b3), \"b4\", np.mean(b4))\n",
        "print(\"rouge-L\", np.mean(r))\n",
        "print(\"m\", np.mean(m))\n",
        "print(\"c\", np.mean(c))\n",
        "\n",
        "zero_subject_nd_b1 = np.mean(b1)\n",
        "zero_subject_nd_b2 = np.mean(b2)\n",
        "zero_subject_nd_b3 = np.mean(b3)\n",
        "zero_subject_nd_b4 = np.mean(b4)\n",
        "zero_subject_nd_m = np.mean(m)\n",
        "zero_subject_nd_r = np.mean(r)\n",
        "zero_subject_nd_c = np.mean(c)\n",
        "\n",
        "# question body results\n",
        "print(\"================================================================\")\n",
        "print(\"Question body evaluation\")\n",
        "print(\"================================================================\")\n",
        "print(\"b1\", np.mean(t_b1), \"b2\", np.mean(t_b2), \"b3\", np.mean(t_b3), \"b4\", np.mean(t_b4))\n",
        "print(\"rouge-L\", np.mean(t_r))\n",
        "print(\"m\", np.mean(t_m))\n",
        "print(\"c\", np.mean(t_c))\n",
        "\n",
        "t_zero_subject_nd_b1 = np.mean(t_b1)\n",
        "t_zero_subject_nd_b2 = np.mean(t_b2)\n",
        "t_zero_subject_nd_b3 = np.mean(t_b3)\n",
        "t_zero_subject_nd_b4 = np.mean(t_b4)\n",
        "t_zero_subject_nd_m = np.mean(t_m)\n",
        "t_zero_subject_nd_r = np.mean(t_r)\n",
        "t_zero_subject_nd_c = np.mean(t_c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 1.2 Using facet\n",
        "### 1.2.1 Generate inputs for neurologic decoding. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from word_forms.word_forms import get_word_forms\n",
        "facet_test_file = 'data/clariq_f/ClariQ-FKw-dev.tsv'\n",
        "facet_test_data = pd.read_csv(facet_test_file, sep='\\t')\n",
        "\n",
        "write_to_file = 'neurologic_decoding/dataset/clean/constraint/test.constraint.json'\n",
        "prompt_write_to_file = 'neurologic_decoding/dataset/clean/init/commongen.test.init.txt'\n",
        "no_prompt_write_to_file = 'neurologic_decoding/dataset/clean/init/commongen_no_prompt.test.init.txt'\n",
        "\n",
        "pos_tagger = spacy.load('en_core_web_sm')\n",
        "\n",
        "all_queries= []\n",
        "all_constraints = []\n",
        "for iter, row in facet_test_data.iterrows():\n",
        "    facet = facet_test_data.at[iter, 'facet_desc']\n",
        "    query = facet_test_data.at[iter, 'initial_request']\n",
        "    noun_in_query = [token.text for token in pos_tagger(query) if token.pos_ == 'NOUN']\n",
        "    propn_in_query = [token.text for token in pos_tagger(query) if token.pos_ == 'PROPN']\n",
        "    all_queries.append(query)\n",
        "\n",
        "    constraints = [[term] for term in facet.split()]\n",
        "    #if propn_in_query != []: constraints += [[' '.join(propn_in_query)]]\n",
        "    #for facet_word in facet.split():\n",
        "    #for facet_word in facet.split():\n",
        "    #    many_forms = get_word_forms(facet_word)\n",
        "    #    constraints.append(list(set([word for k in many_forms.keys() for word in many_forms[k] ]+[facet_word])))\n",
        "    all_constraints.append(constraints)\n",
        "\n",
        "with open(write_to_file, 'w') as output:\n",
        "    for constraints in all_constraints:\n",
        "        for k, prompt in enumerate(starting_texts):\n",
        "            json_str = json.dumps(constraints)\n",
        "            output.write(json_str)\n",
        "            output.write('\\n')\n",
        "\n",
        "with open(prompt_write_to_file, 'w') as output:\n",
        "    for query in all_queries:\n",
        "        for k, prompt in enumerate(starting_texts):\n",
        "            output.write(query + prompt)\n",
        "            output.write('\\n')\n",
        "\n",
        "with open(no_prompt_write_to_file, 'w') as output:\n",
        "    for query in all_queries:\n",
        "        output.write(query)\n",
        "        output.write('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 1.2.2 Run neurologic decoding.\n",
        "In a terminal:\n",
        "\n",
        "Run the generation code.\n",
        "```\n",
        "cd neurologic_decoding/zero\n",
        "conda activate hug\n",
        "export PYTHONPATH=/home/azureuser/cloudfiles/code/Users/t-zhendwang/srconvsearch/neurologic_decoding\n",
        "bash decode_pt.sh 0 test gpt2facet\n",
        "``` \n",
        "\n",
        "Make sure we get the generation file 'gpt2facet'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 1.2.3 Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 tell me about cass county missouri - list homes sale - do you need information on the homes sale list\n",
            "100 Find information on ontario california airport. - directions location - do you want information and directions to your location\n",
            "200 Where can I buy pressure washers? - washer - would you like to buy washer washers\n",
            "300 Tell me more about Rocky Mountain News - recent events historical - are you interested in recent historical events\n",
            "400 Where should I order dog clean-up bags - specif bag type - do you want to know specifical bag type\n",
            "================================================================\n",
            "Full reference evaluation\n",
            "================================================================\n",
            "b1 0.41802206389295793 b2 0.17567385672035452 b3 0.0970553332319387 b4 0.0644596743271716\n",
            "rouge-L 0.4418867567785013\n",
            "m 0.3851975009273996\n",
            "c 0.9896078431372548\n",
            "================================================================\n",
            "Question body evaluation\n",
            "================================================================\n",
            "b1 0.3850744050543095 b2 0.18168457375984648 b3 0.1080585210854704 b4 0.08461452029751804\n",
            "rouge-L 0.43791545548265426\n",
            "m 0.3746952017378225\n",
            "c 0.9866666666666666\n"
          ]
        }
      ],
      "source": [
        "facet_test_file = 'data/clariq_f/ClariQ-FKw-dev.tsv'\n",
        "facet_test_data = pd.read_csv(facet_test_file, sep='\\t')\n",
        "_, facet_test_data = process_clariq_f(facet_test_data)\n",
        "\n",
        "b1, b2, b3, b4 = [], [], [], []\n",
        "m = []\n",
        "r = []\n",
        "c = []\n",
        "\n",
        "t_b1, t_b2, t_b3, t_b4 = [], [], [], []\n",
        "t_m = []\n",
        "t_r = []\n",
        "t_c = []\n",
        "\n",
        "model_output = 'zeroshot_nd_wsdm.csv'\n",
        "generated_file = 'neurologic_decoding/zero_shot/gpt2facet'\n",
        "\n",
        "if os.path.isfile(model_output):\n",
        "    b1, b2, b3, b4, m, r, c, t_b1, t_b2, t_b3, t_b4, t_m, t_r, t_c = evaluate_from_output(model_output)\n",
        "\n",
        "else:\n",
        "    generated_cq_all_templates = open(generated_file, 'r').readlines()\n",
        "    generated_cq_grouped = [[generated_cq_all_templates[len(starting_texts) * k + l] \n",
        "                                for l in range(len(starting_texts))] \n",
        "                                for k in range(int(len(generated_cq_all_templates)/8))]\n",
        "    for iter, row in facet_test_data.iterrows():\n",
        "        facet = facet_test_data.at[iter, 'facet_desc']\n",
        "        ref = facet_test_data.at[iter, 'question']\n",
        "        tokenized_ref = word_tokenize(ref)\n",
        "        facet_list = facet.split()\n",
        "\n",
        "        generated_cqs = []\n",
        "        for full_sentence in generated_cq_grouped[iter]:\n",
        "            query = re.sub('\\[SEP\\]', '&', full_sentence).split('&')[0].strip()\n",
        "            generated_follow_up = re.sub('\\[SEP\\]', '&', full_sentence).split('&')[1].strip()\n",
        "            generated_cq = re.sub('[.?]', '&', generated_follow_up).split('&')[0].strip()\n",
        "            generated_cqs.append(generated_cq)\n",
        "        \n",
        "        noun_in_query = [token.text for token in pos_tagger(query) if token.pos_ == 'NOUN']\n",
        "        propn_in_query = [token.text.lower() for token in pos_tagger(query) if token.pos_ == 'PROPN']\n",
        "\n",
        "        template_scores = calculate_WSDM(query=' '.join(noun_in_query+propn_in_query+facet_list), doc_list=generated_cqs)\n",
        "        sorted_template_scores = sorted(template_scores.items(), key = lambda x: x[1], reverse=True)\n",
        "        generated_cq = sorted(template_scores.keys(), key = lambda x: template_scores[x], reverse=True)[0] \n",
        "        facet_test_data.at[iter, 'generated'] = generated_cq\n",
        "        tokenized_hyp = word_tokenize(facet_test_data.at[iter, 'generated'])\n",
        "        \n",
        "        if iter % sample_every == 0: \n",
        "            print(iter, query, \"-\", facet, '-', generated_cq)\n",
        "            #pprint.pprint(sorted_template_scores)\n",
        "\n",
        "        # full reference evaluation\n",
        "        hyp_b1, hyp_b2, hyp_b3, hyp_b4, hyp_m, hyp_r, hyp_c = auto_evaluation(ref, generated_cq, facet)\n",
        "\n",
        "        b1.append(hyp_b1)\n",
        "        b2.append(hyp_b2)\n",
        "        b3.append(hyp_b3)\n",
        "        b4.append(hyp_b4)\n",
        "        m.append(hyp_m)\n",
        "        r.append(hyp_r)\n",
        "        c.append(hyp_c)\n",
        "\n",
        "        # question body evaluation\n",
        "        truncate_ref = ' '.join(ref.split()[template_len:])\n",
        "        truncate_generated_cq = ' '.join(generated_cq.split()[template_len:])\n",
        "        \n",
        "        t_hyp_b1, t_hyp_b2, t_hyp_b3, t_hyp_b4, t_hyp_m, t_hyp_r, t_hyp_c = auto_evaluation(truncate_ref, truncate_generated_cq, facet)\n",
        "\n",
        "        t_b1.append(t_hyp_b1)\n",
        "        t_b2.append(t_hyp_b2)\n",
        "        t_b3.append(t_hyp_b3)\n",
        "        t_b4.append(t_hyp_b4)\n",
        "        t_m.append(t_hyp_m)\n",
        "        t_r.append(t_hyp_r)\n",
        "        t_c.append(t_hyp_c)\n",
        "\n",
        "    output_df = facet_test_data[['initial_request', 'facet_desc', 'question', 'generated']]\n",
        "    output_df.columns = ['query', 'facet', 'reference', 'candidate']\n",
        "    output_df.to_csv('zeroshot_nd_wsdm.csv')\n",
        "\n",
        "# full reference results\n",
        "print(\"================================================================\")\n",
        "print(\"Full reference evaluation\")\n",
        "print(\"================================================================\")\n",
        "print(\"b1\", np.mean(b1), \"b2\", np.mean(b2), \"b3\", np.mean(b3), \"b4\", np.mean(b4))\n",
        "print(\"rouge-L\", np.mean(r))\n",
        "print(\"m\", np.mean(m))\n",
        "print(\"c\", np.mean(c))\n",
        "\n",
        "zero_nd_wsdm_b1 = np.mean(b1)\n",
        "zero_nd_wsdm_b2 = np.mean(b2)\n",
        "zero_nd_wsdm_b3 = np.mean(b3)\n",
        "zero_nd_wsdm_b4 = np.mean(b4)\n",
        "zero_nd_wsdm_m = np.mean(m)\n",
        "zero_nd_wsdm_r = np.mean(r)\n",
        "zero_nd_wsdm_c = np.mean(c)\n",
        "\n",
        "\n",
        "# question body results\n",
        "print(\"================================================================\")\n",
        "print(\"Question body evaluation\")\n",
        "print(\"================================================================\")\n",
        "print(\"b1\", np.mean(t_b1), \"b2\", np.mean(t_b2), \"b3\", np.mean(t_b3), \"b4\", np.mean(t_b4))\n",
        "print(\"rouge-L\", np.mean(t_r))\n",
        "print(\"m\", np.mean(t_m))\n",
        "print(\"c\", np.mean(t_c))\n",
        "\n",
        "t_zero_nd_wsdm_b1 = np.mean(t_b1)\n",
        "t_zero_nd_wsdm_b2 = np.mean(t_b2)\n",
        "t_zero_nd_wsdm_b3 = np.mean(t_b3)\n",
        "t_zero_nd_wsdm_b4 = np.mean(t_b4)\n",
        "t_zero_nd_wsdm_m = np.mean(t_m)\n",
        "t_zero_nd_wsdm_r = np.mean(t_r)\n",
        "t_zero_nd_wsdm_c = np.mean(t_c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 1.3 Comparing 1.1 and 1.2, and get the conclusion of RQ1: \"Facet is indeed very useful for clarifying question generation\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------------------------------\n",
            "|                               Full reference evaluation                               |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|MODEL                    BLEU1    BLEU2    BLEU3    BLEU4    METEOR   ROUGE    COVERAGE|\n",
            "-----------------------------------------------------------------------------------------\n",
            "|No facet                 29.16    12.81    7.6      5.14     28.59    34.78    9.82    |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|With facet               41.8     17.57    9.71     6.45     38.52    44.19    98.96   |\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "|                                Question body evaluation                               |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|MODEL                    BLEU1    BLEU2    BLEU3    BLEU4    METEOR   ROUGE    COVERAGE|\n",
            "-----------------------------------------------------------------------------------------\n",
            "|No facet                 14.51    4.37     2.04     2.57     14.51    19.39    9.45    |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|With facet               38.51    18.17    10.81    8.46     37.47    43.79    98.67   |\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|                               Full reference evaluation                               |\")\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('MODEL', 'BLEU1','BLEU2','BLEU3','BLEU4','METEOR','ROUGE','COVERAGE'))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('No facet', \n",
        "                                                            round_metric(zero_subject_nd_b1), \n",
        "                                                            round_metric(zero_subject_nd_b2), \n",
        "                                                            round_metric(zero_subject_nd_b3), \n",
        "                                                            round_metric(zero_subject_nd_b4), \n",
        "                                                            round_metric(zero_subject_nd_m), \n",
        "                                                            round_metric(zero_subject_nd_r), \n",
        "                                                            round_metric(zero_subject_nd_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('With facet', \n",
        "                                                            round_metric(zero_nd_wsdm_b1), \n",
        "                                                            round_metric(zero_nd_wsdm_b2), \n",
        "                                                            round_metric(zero_nd_wsdm_b3), \n",
        "                                                            round_metric(zero_nd_wsdm_b4), \n",
        "                                                            round_metric(zero_nd_wsdm_m), \n",
        "                                                            round_metric(zero_nd_wsdm_r),\n",
        "                                                            round_metric(zero_nd_wsdm_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|                                Question body evaluation                               |\")\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('MODEL', 'BLEU1','BLEU2','BLEU3','BLEU4','METEOR','ROUGE','COVERAGE'))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('No facet', \n",
        "                                                            round_metric(t_zero_subject_nd_b1), \n",
        "                                                            round_metric(t_zero_subject_nd_b2), \n",
        "                                                            round_metric(t_zero_subject_nd_b3), \n",
        "                                                            round_metric(t_zero_subject_nd_b4), \n",
        "                                                            round_metric(t_zero_subject_nd_m), \n",
        "                                                            round_metric(t_zero_subject_nd_r), \n",
        "                                                            round_metric(t_zero_subject_nd_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('With facet', \n",
        "                                                            round_metric(t_zero_nd_wsdm_b1), \n",
        "                                                            round_metric(t_zero_nd_wsdm_b2), \n",
        "                                                            round_metric(t_zero_nd_wsdm_b3), \n",
        "                                                            round_metric(t_zero_nd_wsdm_b4), \n",
        "                                                            round_metric(t_zero_nd_wsdm_m), \n",
        "                                                            round_metric(t_zero_nd_wsdm_r),\n",
        "                                                            round_metric(t_zero_nd_wsdm_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# RQ2. How does our zero-shot facet-constrained approach compare to existing facet-driven baselines?\n",
        "\n",
        "To answer this research question, we include some existing methods and a few other reasonable solutions not mentioned by previous works as our baseline models. Some of them are zero-shot, while others are not. However, we still compare their performances altogether to demonstrate the power of our zero-shot approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 2.1 Our approach.\n",
        "The same as in Section 1.2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 2.2 Template append facet approach\n",
        "This method appends facet words directly to question templates. This baseline is not ideal. Admittedly, it can generate good questions such as:\n",
        "\n",
        ": \"I am looking for information about South Africa.\"\n",
        "\n",
        " : \"population\"\n",
        "\n",
        ": \"Are you interested in \\[population\\]\"\n",
        "\n",
        "However, sometimes the case is the facet itself cannot form a meaningful question:\n",
        "\n",
        ": \"I am interested in poker tournaments.\"\n",
        "\n",
        " : \"online\"\n",
        "\n",
        ": \"Are you interested in \\[online\\]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 tell me about cass county missouri - list homes sale - do you want to know list homes sale\n",
            "100 Find information on ontario california airport. - directions location - do you want to know directions location\n",
            "200 Where can I buy pressure washers? - washer - would you like to washer\n",
            "300 Tell me more about Rocky Mountain News - recent events historical - do you want to know recent events historical\n",
            "400 Where should I order dog clean-up bags - specif bag type - do you want to know specif bag type\n",
            "================================================================\n",
            "Full reference evaluation\n",
            "================================================================\n",
            "b1 0.3855760869404698 b2 0.14068762090838577 b3 0.09981932597720988 b4 0.07806508022694261\n",
            "rouge-L 0.4602337846738828\n",
            "m 0.338690733001687\n",
            "c 1.0\n",
            "================================================================\n",
            "Question body evaluation\n",
            "================================================================\n",
            "b1 0.2548890473046932 b2 0.05751338297329412 b3 0.03272406390161522 b4 0.03350955982568657\n",
            "rouge-L 0.38575429251434623\n",
            "m 0.2208677078751569\n",
            "c 1.0\n"
          ]
        }
      ],
      "source": [
        "facet_test_file = 'data/clariq_f/ClariQ-FKw-dev.tsv'\n",
        "facet_test_data = pd.read_csv(facet_test_file, sep='\\t')\n",
        "_, facet_test_data = process_clariq_f(facet_test_data)\n",
        "\n",
        "b1, b2, b3, b4 = [], [], [], []\n",
        "m = []\n",
        "r = []\n",
        "c = []\n",
        "\n",
        "t_b1, t_b2, t_b3, t_b4 = [], [], [], []\n",
        "t_m = []\n",
        "t_r = []\n",
        "t_c = []\n",
        "\n",
        "model_output = 'template_facet.csv'\n",
        "\n",
        "if os.path.isfile(model_output):\n",
        "    b1, b2, b3, b4, m, r, c, t_b1, t_b2, t_b3, t_b4, t_m, t_r, t_c = evaluate_from_output(model_output)\n",
        "\n",
        "else:\n",
        "    for iter, row in facet_test_data.iterrows():\n",
        "        facet = facet_test_data.at[iter, 'facet_desc']\n",
        "        query = facet_test_data.at[iter, 'initial_request']\n",
        "        ref = facet_test_data.at[iter, 'question']\n",
        "        tokenized_ref = word_tokenize(ref)\n",
        "\n",
        "        template_scores = {}\n",
        "        for s_t in starting_texts:\n",
        "            generated_cq = s_t + ' ' + facet\n",
        "            generated_cq = process_generation(generated_cq)\n",
        "            generated_cq = ' '.join(word_tokenize(generated_cq))\n",
        "            template_scores[generated_cq] = calculatePerplexity(sentence=generated_cq, model=ppl_model, tokenizer=ppl_tokenizer)\n",
        "        \n",
        "        generated_cq = sorted(template_scores.keys(), key = lambda x: template_scores[x])[0] \n",
        "        facet_test_data.at[iter, 'generated'] = generated_cq\n",
        "\n",
        "        if iter % sample_every == 0:\n",
        "            print(iter, query, '-', facet, '-', generated_cq)\n",
        "            #pprint.pprint(template_scores)\n",
        "        \n",
        "        # full reference evaluation\n",
        "        hyp_b1, hyp_b2, hyp_b3, hyp_b4, hyp_m, hyp_r, hyp_c = auto_evaluation(ref, generated_cq, facet)\n",
        "\n",
        "        b1.append(hyp_b1)\n",
        "        b2.append(hyp_b2)\n",
        "        b3.append(hyp_b3)\n",
        "        b4.append(hyp_b4)\n",
        "        m.append(hyp_m)\n",
        "        r.append(hyp_r)\n",
        "        c.append(hyp_c)\n",
        "\n",
        "        # question body evaluation\n",
        "        truncate_ref = ' '.join(ref.split()[template_len:])\n",
        "        truncate_generated_cq = ' '.join(generated_cq.split()[template_len:])\n",
        "        \n",
        "        t_hyp_b1, t_hyp_b2, t_hyp_b3, t_hyp_b4, t_hyp_m, t_hyp_r, t_hyp_c = auto_evaluation(truncate_ref, truncate_generated_cq, facet)\n",
        "\n",
        "        t_b1.append(t_hyp_b1)\n",
        "        t_b2.append(t_hyp_b2)\n",
        "        t_b3.append(t_hyp_b3)\n",
        "        t_b4.append(t_hyp_b4)\n",
        "        t_m.append(t_hyp_m)\n",
        "        t_r.append(t_hyp_r)\n",
        "        t_c.append(t_hyp_c)\n",
        "\n",
        "    output_df = facet_test_data[['initial_request', 'facet_desc', 'question', 'generated']]\n",
        "    output_df.columns = ['query', 'facet', 'reference', 'candidate']\n",
        "    output_df.to_csv(model_output)\n",
        "\n",
        "# full reference results\n",
        "print(\"================================================================\")\n",
        "print(\"Full reference evaluation\")\n",
        "print(\"================================================================\")\n",
        "print(\"b1\", np.mean(b1), \"b2\", np.mean(b2), \"b3\", np.mean(b3), \"b4\", np.mean(b4))\n",
        "print(\"rouge-L\", np.mean(r))\n",
        "print(\"m\", np.mean(m))\n",
        "print(\"c\", np.mean(c))\n",
        "\n",
        "template_facet_b1 = np.mean(b1)\n",
        "template_facet_b2 = np.mean(b2)\n",
        "template_facet_b3 = np.mean(b3)\n",
        "template_facet_b4 = np.mean(b4)\n",
        "template_facet_m = np.mean(m)\n",
        "template_facet_r = np.mean(r)\n",
        "template_facet_c = np.mean(c)\n",
        "\n",
        "# question body results\n",
        "print(\"================================================================\")\n",
        "print(\"Question body evaluation\")\n",
        "print(\"================================================================\")\n",
        "print(\"b1\", np.mean(t_b1), \"b2\", np.mean(t_b2), \"b3\", np.mean(t_b3), \"b4\", np.mean(t_b4))\n",
        "print(\"rouge-L\", np.mean(t_r))\n",
        "print(\"m\", np.mean(t_m))\n",
        "print(\"c\", np.mean(t_c))\n",
        "\n",
        "t_template_facet_b1 = np.mean(t_b1)\n",
        "t_template_facet_b2 = np.mean(t_b2)\n",
        "t_template_facet_b3 = np.mean(t_b3)\n",
        "t_template_facet_b4 = np.mean(t_b4)\n",
        "t_template_facet_m = np.mean(t_m)\n",
        "t_template_facet_r = np.mean(t_r)\n",
        "t_template_facet_c = np.mean(t_c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 2.3 Finetuned GPT2 approach ([Previous SOTA by Sekulic](https://dl.acm.org/doi/abs/10.1145/3471158.3472257)), which uses inputs structured as:\n",
        "\n",
        "## {facet} \\[SEP\\] {query} \\[BOS\\] {clarifying question} \\[EOS\\]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 tell me about cass county missouri - list homes sale - are you looking for a specific list of the county homes sale in south america\n",
            "100 Find information on ontario california airport. - directions location - do you want to know about the location of the ontario cal\n",
            "200 Where can I buy pressure washers? - washer - do you want to know the difference between a vacuum was\n",
            "300 Tell me more about Rocky Mountain News - recent events historical -  would you like to know about historical events that happened\n",
            "400 Where should I order dog clean-up bags - specif bag type - would you like to know the type of bag\n",
            "================================================================\n",
            "Full reference evaluation\n",
            "================================================================\n",
            "b1 0.27745683326901827 b2 0.10785140305649912 b3 0.0603472849754279 b4 0.038306506703975875\n",
            "rouge-L 0.3170536641435762\n",
            "m 0.2855930527921367\n",
            "c 0.20854901960784314\n",
            "================================================================\n",
            "Question body evaluation\n",
            "================================================================\n",
            "b1 0.16383397692101645 b2 0.05368179453201418 b3 0.026705204131886495 b4 0.023738660852511858\n",
            "rouge-L 0.20138411844336415\n",
            "m 0.18413326944858474\n",
            "c 0.20207843137254902\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "sample_every = 100\n",
        "epochs = 8\n",
        "learning_rate = 5e-5\n",
        "warmup_steps = 1e2\n",
        "epsilon = 1e-8\n",
        "max_length = 128\n",
        "prompt_instruction = ''\n",
        "\n",
        "facet_test_file = 'data/clariq_f/ClariQ-FKw-dev.tsv'\n",
        "facet_test_data = pd.read_csv(facet_test_file, sep='\\t')\n",
        "_, facet_test_data = process_clariq_f(facet_test_data)\n",
        "\n",
        "b1, b2, b3, b4 = [], [], [], []\n",
        "m = []\n",
        "r = []\n",
        "c = []\n",
        "\n",
        "t_b1, t_b2, t_b3, t_b4 = [], [], [], []\n",
        "t_m = []\n",
        "t_r = []\n",
        "t_c = []\n",
        "\n",
        "model_output = 'sekulic.csv'\n",
        "\n",
        "if os.path.isfile(model_output):\n",
        "    b1, b2, b3, b4, m, r, c, t_b1, t_b2, t_b3, t_b4, t_m, t_r, t_c = evaluate_from_output(model_output)\n",
        "\n",
        "else:\n",
        "    clariq_f_train_file = 'data/clariq_f/ClariQ-FKw-train.tsv'\n",
        "    clariq_f_train_data = pd.read_csv(clariq_f_train_file, sep='\\t') \n",
        "    clariq_f_train_dict, clariq_f_train_data = process_clariq_f(clariq_f_train_data)\n",
        "    clariq_f_train_text_list = clariq_f_train_data['f_q_cq']\n",
        "\n",
        "    class GPT2Dataset(Dataset):\n",
        "        def __init__(self, txt_list, tokenizer, gpt2_type=\"gpt2\", max_length=768):\n",
        "            self.tokenizer = tokenizer\n",
        "            self.input_ids = []\n",
        "            self.attn_masks = []\n",
        "        \n",
        "            print(\"training text example\", txt_list[0])\n",
        "            for txt in txt_list:\n",
        "                encodings_dict = tokenizer(txt, truncation=True, max_length=max_length, padding=\"max_length\")\n",
        "                self.input_ids.append(T.tensor(encodings_dict['input_ids']))\n",
        "                self.attn_masks.append(T.tensor(encodings_dict['attention_mask']))\n",
        "        \n",
        "        def __len__(self):\n",
        "            return len(self.input_ids)\n",
        "        \n",
        "        def __getitem__(self, idx):\n",
        "            return self.input_ids[idx], self.attn_masks[idx] \n",
        "            \n",
        "    dataset = GPT2Dataset(clariq_f_train_text_list, tokenizer, max_length=max_length)\n",
        "\n",
        "    train_size = int(0.99 * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "\n",
        "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "    print('{:>5,} train /{:>5,} val'.format(train_size, val_size))\n",
        "\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset,\n",
        "        sampler = RandomSampler(train_dataset),\n",
        "        batch_size = batch_size\n",
        "        )\n",
        "\n",
        "    validation_dataloader = DataLoader(\n",
        "        val_dataset,\n",
        "        sampler = SequentialSampler(val_dataset),\n",
        "        batch_size = batch_size\n",
        "        )\n",
        "\n",
        "    device = T.device(\"cuda\")\n",
        "    model.cuda()\n",
        "\n",
        "    optimizer = AdamW(model.parameters(),\n",
        "        lr = learning_rate,\n",
        "        eps = epsilon\n",
        "        )\n",
        "\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "        num_warmup_steps = warmup_steps, \n",
        "        num_training_steps = total_steps\n",
        "        )\n",
        "\n",
        "    training_stats = []\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    for epoch_i in range(0, epochs):\n",
        "        # ========================================\n",
        "        #               Training\n",
        "        # ========================================\n",
        "        print(\"\")\n",
        "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "        print('Training...')\n",
        "\n",
        "        total_train_loss = 0\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "            b_input_ids = batch[0].to(device)\n",
        "            b_labels = batch[0].to(device)\n",
        "            b_masks = batch[1].to(device)\n",
        "\n",
        "            model.zero_grad()        \n",
        "\n",
        "            outputs = model(  b_input_ids,\n",
        "                            labels=b_labels, \n",
        "                            attention_mask = b_masks,\n",
        "                            token_type_ids=None\n",
        "                            )\n",
        "            loss = outputs[0]  \n",
        "\n",
        "            batch_loss = loss.item()\n",
        "            total_train_loss += batch_loss\n",
        "\n",
        "            # Get sample every x batches.\n",
        "            if step % sample_every == 0 and not step == 0:\n",
        "                print('  Batch {:>5,}  of  {:>5,}. Loss: {:>5,}.'.format(step, len(train_dataloader), batch_loss))\n",
        "                model.eval()\n",
        "                model.train()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "        # Calculate the average loss over all of the batches.\n",
        "        avg_train_loss = total_train_loss / len(train_dataloader)       \n",
        "        \n",
        "        # Measure how long this epoch took.\n",
        "\n",
        "        print(\"\")\n",
        "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))       \n",
        "        # ========================================\n",
        "        #               Validation\n",
        "        # ========================================\n",
        "        print(\"\")\n",
        "        print(\"Running Validation...\")\n",
        "        model.eval()\n",
        "        total_eval_loss = 0\n",
        "        nb_eval_steps = 0\n",
        "\n",
        "        # Evaluate data for one epoch\n",
        "        for batch in validation_dataloader:       \n",
        "            b_input_ids = batch[0].to(device)\n",
        "            b_labels = batch[0].to(device)\n",
        "            b_masks = batch[1].to(device)\n",
        "            \n",
        "            with T.no_grad():        \n",
        "                outputs  = model(b_input_ids, \n",
        "                                attention_mask = b_masks,\n",
        "                                labels=b_labels)         \n",
        "                loss = outputs[0]  \n",
        "                \n",
        "            batch_loss = loss.item()\n",
        "            total_eval_loss += batch_loss        \n",
        "\n",
        "        avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "\n",
        "        print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "\n",
        "        # Record all statistics from this epoch.\n",
        "        training_stats.append(\n",
        "            {\n",
        "                'epoch': epoch_i + 1,\n",
        "                'Training Loss': avg_train_loss,\n",
        "                'Valid. Loss': avg_val_loss,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Training complete!\")\n",
        "\n",
        "    output_dir = './model_save/'+prompt_instruction+str(epochs)+'/'\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "    # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "    # They can then be reloaded using `from_pretrained()`\n",
        "    model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "    model_to_save.save_pretrained(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "    clariq_f_test_file = 'data/clariq_f/ClariQ-FKw-dev.tsv'\n",
        "    clariq_f_test_data = pd.read_csv(clariq_f_test_file, sep='\\t') \n",
        "    clariq_f_test_dict, clariq_f_test_data = process_clariq_f(clariq_f_test_data)\n",
        "\n",
        "    rs = rouge.Rouge()\n",
        "    rs_list = []\n",
        "    b1, b2, b3, b4 = [], [], [], []\n",
        "    m = []\n",
        "    \n",
        "    for iter, row in clariq_f_test_data.iterrows():\n",
        "        query = clariq_f_test_data.at[iter, 'f_q']\n",
        "        ref = clariq_f_test_data.at[iter, 'question']\n",
        "        prompt_input = prompt_instruction + query\n",
        "        prompt_input_BOS = prompt_input + BOS\n",
        "        tokenized_prompt_input = T.tensor(tokenizer.encode(prompt_input_BOS)).unsqueeze(0)\n",
        "        tokenized_prompt_input = tokenized_prompt_input.to(device)\n",
        "        generated_text = ''\n",
        "        generated_cq = ''\n",
        "        attempt, max_attempt = 0, 4\n",
        "        while generated_cq == '' and attempt <= max_attempt: # to ensure the generation is not empty\n",
        "            attempt += 1\n",
        "            sample_outputs = model.generate(\n",
        "                tokenized_prompt_input,\n",
        "                do_sample=True,   \n",
        "                top_k=0, \n",
        "                max_length = len(tokenized_prompt_input[0]) + 10,\n",
        "                top_p=0.9, \n",
        "                temperature = 0.7,\n",
        "                num_return_sequences=1,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "                )\n",
        "\n",
        "            generated_text = tokenizer.decode(sample_outputs[0], skip_special_tokens=True)\n",
        "            generated_cq = generated_text[len(prompt_input):]\n",
        "            \n",
        "            clariq_f_test_data.at[iter, 'generated'] = process_generation(generated_cq)\n",
        "\n",
        "            if generated_cq == '':\n",
        "                generated_cq = 'nan'\n",
        "\n",
        "        # full reference evaluation\n",
        "        hyp_b1, hyp_b2, hyp_b3, hyp_b4, hyp_m, hyp_r, hyp_c = auto_evaluation(ref, generated_cq, facet)\n",
        "\n",
        "        b1.append(hyp_b1)\n",
        "        b2.append(hyp_b2)\n",
        "        b3.append(hyp_b3)\n",
        "        b4.append(hyp_b4)\n",
        "        m.append(hyp_m)\n",
        "        r.append(hyp_r)\n",
        "        c.append(hyp_c)\n",
        "\n",
        "        # question body evaluation\n",
        "        truncate_ref = ' '.join(ref.split()[template_len:])\n",
        "        truncate_generated_cq = ' '.join(generated_cq.split()[template_len:])\n",
        "        \n",
        "        t_hyp_b1, t_hyp_b2, t_hyp_b3, t_hyp_b4, t_hyp_m, t_hyp_r, t_hyp_c = auto_evaluation(truncate_ref, truncate_generated_cq, facet)\n",
        "\n",
        "        t_b1.append(t_hyp_b1)\n",
        "        t_b2.append(t_hyp_b2)\n",
        "        t_b3.append(t_hyp_b3)\n",
        "        t_b4.append(t_hyp_b4)\n",
        "        t_m.append(t_hyp_m)\n",
        "        t_r.append(t_hyp_r)\n",
        "        t_c.append(t_hyp_c)\n",
        "\n",
        "    output_df = clariq_f_test_data[['initial_request', 'facet_desc', 'question', 'generated']]\n",
        "    output_df.columns = ['query', 'facet', 'reference', 'candidate']\n",
        "    output_df.to_csv(model_output)\n",
        "\n",
        "# full reference results\n",
        "print(\"================================================================\")\n",
        "print(\"Full reference evaluation\")\n",
        "print(\"================================================================\")\n",
        "print(\"b1\", np.mean(b1), \"b2\", np.mean(b2), \"b3\", np.mean(b3), \"b4\", np.mean(b4))\n",
        "print(\"rouge-L\", np.mean(r))\n",
        "print(\"m\", np.mean(m))\n",
        "print(\"c\", np.mean(c))\n",
        "\n",
        "sekulic_b1 = np.mean(b1)\n",
        "sekulic_b2 = np.mean(b2)\n",
        "sekulic_b3 = np.mean(b3)\n",
        "sekulic_b4 = np.mean(b4)\n",
        "sekulic_m = np.mean(m)\n",
        "sekulic_r = np.mean(r)\n",
        "sekulic_c = np.mean(c)\n",
        "\n",
        "# question body results\n",
        "print(\"================================================================\")\n",
        "print(\"Question body evaluation\")\n",
        "print(\"================================================================\")\n",
        "print(\"b1\", np.mean(t_b1), \"b2\", np.mean(t_b2), \"b3\", np.mean(t_b3), \"b4\", np.mean(t_b4))\n",
        "print(\"rouge-L\", np.mean(t_r))\n",
        "print(\"m\", np.mean(t_m))\n",
        "print(\"c\", np.mean(t_c))\n",
        "\n",
        "t_sekulic_b1 = np.mean(t_b1)\n",
        "t_sekulic_b2 = np.mean(t_b2)\n",
        "t_sekulic_b3 = np.mean(t_b3)\n",
        "t_sekulic_b4 = np.mean(t_b4)\n",
        "t_sekulic_m = np.mean(t_m)\n",
        "t_sekulic_r = np.mean(t_r)\n",
        "t_sekulic_c = np.mean(t_c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 2.4 Prompt-based finetuned GPT2 approach, which uses inputs structured as:\n",
        "\n",
        "## {query} Ask a question that contains words in the list \\[{facet}\\] {clarifying question}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 tell me about cass county missouri - list homes sale - are you looking for homes in the city of missouri\n",
            "100 Find information on ontario california airport. - directions location - are you looking for directions to the nearest airport\n",
            "200 Where can I buy pressure washers? - washer - are you looking for a pressure was are you looking for a washing machine or a washing machine\n",
            "300 Tell me more about Rocky Mountain News - recent events historical - are you interested in historical events in the Rocky Mountain National park\n",
            "400 Where should I order dog clean-up bags - specif bag type - are you looking for a type of bag for the dog that contains the specif referring to\n",
            "================================================================\n",
            "Full reference evaluation\n",
            "================================================================\n",
            "b1 0.3278795935788096 b2 0.1457659141540518 b3 0.08566811950120864 b4 0.05633487804157568\n",
            "rouge-L 0.40809193442439157\n",
            "m 0.37547626523857963\n",
            "c 0.7254901960784315\n",
            "================================================================\n",
            "Question body evaluation\n",
            "================================================================\n",
            "b1 0.24126668552229252 b2 0.10373898361432152 b3 0.05732044392802645 b4 0.04493044603526353\n",
            "rouge-L 0.32992141402571434\n",
            "m 0.3184044180182667\n",
            "c 0.7141176470588235\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "sample_every = 100\n",
        "epochs = 8\n",
        "learning_rate = 5e-5\n",
        "warmup_steps = 1e2\n",
        "epsilon = 1e-8\n",
        "max_length = 128\n",
        "prompt_instruction = ''\n",
        "\n",
        "temperature = 0.1\n",
        "\n",
        "facet_test_file = 'data/clariq_f/ClariQ-FKw-dev.tsv'\n",
        "facet_test_data = pd.read_csv(facet_test_file, sep='\\t')\n",
        "_, facet_test_data = process_clariq_f(facet_test_data)\n",
        "\n",
        "b1, b2, b3, b4 = [], [], [], []\n",
        "m = []\n",
        "r = []\n",
        "c = []\n",
        "\n",
        "t_b1, t_b2, t_b3, t_b4 = [], [], [], []\n",
        "t_m = []\n",
        "t_r = []\n",
        "t_c = []\n",
        "\n",
        "model_output = 'ftgpt2_prompt' + '_temp' + str(temperature) + '.csv'\n",
        "\n",
        "if os.path.isfile(model_output):\n",
        "    b1, b2, b3, b4, m, r, c, t_b1, t_b2, t_b3, t_b4, t_m, t_r, t_c = evaluate_from_output(model_output)\n",
        "\n",
        "else:  \n",
        "    print(\"Output file not found, generating output.\") \n",
        "    model_dir = './model_save/'+str(epochs)+'/'\n",
        "    if os.path.exists(model_dir):\n",
        "        tokenizer = GPT2Tokenizer.from_pretrained(model_dir, bos_token=BOS, eos_token=EOS, pad_token=PAD) \n",
        "        configuration = GPT2Config.from_pretrained(model_dir, output_hidden_states=False)\n",
        "        model = GPT2LMHeadModel.from_pretrained(model_dir, config=configuration)\n",
        "        model.resize_token_embeddings(len(tokenizer))\n",
        "        model.cuda()\n",
        "    else:\n",
        "        print(\"Model checkpoint not found, finetuning.\")\n",
        "        clariq_f_train_file = 'data/clariq_f/ClariQ-FKw-train.tsv'\n",
        "        clariq_f_train_data = pd.read_csv(clariq_f_train_file, sep='\\t') \n",
        "        clariq_f_train_dict, clariq_f_train_data = process_clariq_f(clariq_f_train_data)\n",
        "        clariq_f_train_text_list = clariq_f_train_data['instructional_q_f_cq']\n",
        "\n",
        "        class GPT2Dataset(Dataset):\n",
        "            def __init__(self, txt_list, tokenizer, gpt2_type=\"gpt2\", max_length=768):\n",
        "                self.tokenizer = tokenizer\n",
        "                self.input_ids = []\n",
        "                self.attn_masks = []\n",
        "            \n",
        "                print(\"training text example\", txt_list[0])\n",
        "                for txt in txt_list:\n",
        "                    encodings_dict = tokenizer(txt, truncation=True, max_length=max_length, padding=\"max_length\")\n",
        "                    self.input_ids.append(T.tensor(encodings_dict['input_ids']))\n",
        "                    self.attn_masks.append(T.tensor(encodings_dict['attention_mask']))\n",
        "            \n",
        "            def __len__(self):\n",
        "                return len(self.input_ids)\n",
        "            \n",
        "            def __getitem__(self, idx):\n",
        "                return self.input_ids[idx], self.attn_masks[idx] \n",
        "            \n",
        "        dataset = GPT2Dataset(clariq_f_train_text_list, tokenizer, max_length=max_length)\n",
        "\n",
        "        train_size = int(0.99 * len(dataset))\n",
        "        val_size = len(dataset) - train_size\n",
        "\n",
        "        train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "        print('{:>5,} train /{:>5,} val'.format(train_size, val_size))\n",
        "\n",
        "        train_dataloader = DataLoader(\n",
        "        train_dataset,\n",
        "            sampler = RandomSampler(train_dataset),\n",
        "            batch_size = batch_size\n",
        "        )\n",
        "\n",
        "        validation_dataloader = DataLoader(\n",
        "        val_dataset,\n",
        "            sampler = SequentialSampler(val_dataset),\n",
        "            batch_size = batch_size\n",
        "        )\n",
        "\n",
        "        optimizer = AdamW(model.parameters(),\n",
        "            lr = learning_rate,\n",
        "            eps = epsilon\n",
        "        )\n",
        "\n",
        "        total_steps = len(train_dataloader) * epochs\n",
        "        scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "            num_warmup_steps = warmup_steps, \n",
        "            num_training_steps = total_steps\n",
        "        )\n",
        "\n",
        "        training_stats = []\n",
        "\n",
        "        for epoch_i in range(0, epochs):\n",
        "            # ========================================\n",
        "            #               Training\n",
        "            # ========================================\n",
        "            print(\"\")\n",
        "            print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "            print('Training...')\n",
        "\n",
        "            total_train_loss = 0\n",
        "\n",
        "            model.train()\n",
        "\n",
        "            for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "                b_input_ids = batch[0].to(device)\n",
        "                b_labels = batch[0].to(device)\n",
        "                b_masks = batch[1].to(device)\n",
        "\n",
        "                model.zero_grad()        \n",
        "\n",
        "                outputs = model(  b_input_ids,\n",
        "                                labels=b_labels, \n",
        "                                attention_mask = b_masks,\n",
        "                                token_type_ids=None\n",
        "                                )\n",
        "                loss = outputs[0]  \n",
        "\n",
        "                batch_loss = loss.item()\n",
        "                total_train_loss += batch_loss\n",
        "\n",
        "                # Get sample every x batches.\n",
        "                if step % sample_every == 0 and not step == 0:\n",
        "                    print('  Batch {:>5,}  of  {:>5,}. Loss: {:>5,}.'.format(step, len(train_dataloader), batch_loss))\n",
        "                    model.eval()\n",
        "                    model.train()\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "\n",
        "            # Calculate the average loss over all of the batches.\n",
        "            avg_train_loss = total_train_loss / len(train_dataloader)       \n",
        "            \n",
        "            # Measure how long this epoch took.\n",
        "\n",
        "            print(\"\")\n",
        "            print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))       \n",
        "            # ========================================\n",
        "            #               Validation\n",
        "            # ========================================\n",
        "            print(\"\")\n",
        "            print(\"Running Validation...\")\n",
        "            model.eval()\n",
        "            total_eval_loss = 0\n",
        "            nb_eval_steps = 0\n",
        "\n",
        "            # Evaluate data for one epoch\n",
        "            for batch in validation_dataloader:       \n",
        "                b_input_ids = batch[0].to(device)\n",
        "                b_labels = batch[0].to(device)\n",
        "                b_masks = batch[1].to(device)\n",
        "                \n",
        "                with T.no_grad():        \n",
        "                    outputs  = model(b_input_ids, \n",
        "                                    attention_mask = b_masks,\n",
        "                                    labels=b_labels)         \n",
        "                    loss = outputs[0]  \n",
        "                    \n",
        "                batch_loss = loss.item()\n",
        "                total_eval_loss += batch_loss        \n",
        "\n",
        "            avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "\n",
        "\n",
        "            print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "\n",
        "            # Record all statistics from this epoch.\n",
        "            training_stats.append(\n",
        "                {\n",
        "                    'epoch': epoch_i + 1,\n",
        "                    'Training Loss': avg_train_loss,\n",
        "                    'Valid. Loss': avg_val_loss,\n",
        "                }\n",
        "            )\n",
        "\n",
        "        print(\"\")\n",
        "        print(\"Training complete!\")\n",
        "\n",
        "        output_dir = model_dir\n",
        "        if not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "\n",
        "        print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "        # They can then be reloaded using `from_pretrained()`\n",
        "        model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "        model_to_save.save_pretrained(output_dir)\n",
        "        tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "    clariq_f_test_file = 'data/clariq_f/ClariQ-FKw-dev.tsv'\n",
        "    clariq_f_test_data = pd.read_csv(clariq_f_test_file, sep='\\t') \n",
        "    clariq_f_test_dict, clariq_f_test_data = process_clariq_f(clariq_f_test_data)\n",
        "\n",
        "    rs = rouge.Rouge()\n",
        "    rs_list = []\n",
        "    b1, b2, b3, b4 = [], [], [], []\n",
        "    m = []\n",
        "    \n",
        "    for iter, row in clariq_f_test_data.iterrows():\n",
        "        query = clariq_f_test_data.at[iter, 'instructional_q_f']\n",
        "        ref = clariq_f_test_data.at[iter, 'question']\n",
        "        tokenized_input = T.tensor(tokenizer.encode(query)).unsqueeze(0).to(device)\n",
        "        generated_text = ''\n",
        "        generated_cq = ''\n",
        "        \n",
        "        sample_outputs = model.generate(\n",
        "                tokenized_input,\n",
        "                do_sample=True,   \n",
        "                top_k=20, \n",
        "                max_length = len(tokenized_input[0]) + 32,\n",
        "                top_p=0.9, \n",
        "                temperature = temperature,\n",
        "                num_return_sequences=1,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        generated_text = tokenizer.decode(sample_outputs[0], skip_special_tokens=True)\n",
        "        \n",
        "        generated_cq = generated_text[len(query):].strip()\n",
        "        generated_cq = re.sub('\\[SEP\\]', ' ', generated_cq).strip()\n",
        "        generated_cq = re.sub('[.?]', '&', generated_cq).split('&')[0].strip()\n",
        "\n",
        "        clariq_f_test_data.at[iter, 'generated'] = generated_cq\n",
        "\n",
        "        if iter % sample_every == 0: \n",
        "            print(iter, query, '-', generated_cq)\n",
        "\n",
        "        # full reference evaluation\n",
        "        hyp_b1, hyp_b2, hyp_b3, hyp_b4, hyp_m, hyp_r, hyp_c = auto_evaluation(ref, generated_cq, facet)\n",
        "\n",
        "        b1.append(hyp_b1)\n",
        "        b2.append(hyp_b2)\n",
        "        b3.append(hyp_b3)\n",
        "        b4.append(hyp_b4)\n",
        "        m.append(hyp_m)\n",
        "        r.append(hyp_r)\n",
        "        c.append(hyp_c)\n",
        "\n",
        "        # question body evaluation\n",
        "        truncate_ref = ' '.join(ref.split()[template_len:])\n",
        "        truncate_generated_cq = ' '.join(generated_cq.split()[template_len:])\n",
        "        \n",
        "        t_hyp_b1, t_hyp_b2, t_hyp_b3, t_hyp_b4, t_hyp_m, t_hyp_r, t_hyp_c = auto_evaluation(truncate_ref, truncate_generated_cq, facet)\n",
        "\n",
        "        t_b1.append(t_hyp_b1)\n",
        "        t_b2.append(t_hyp_b2)\n",
        "        t_b3.append(t_hyp_b3)\n",
        "        t_b4.append(t_hyp_b4)\n",
        "        t_m.append(t_hyp_m)\n",
        "        t_r.append(t_hyp_r)\n",
        "        t_c.append(t_hyp_c)\n",
        "\n",
        "    output_df = clariq_f_test_data[['initial_request', 'facet_desc', 'question', 'generated']]\n",
        "    output_df.columns = ['query', 'facet', 'reference', 'candidate']\n",
        "    output_df.to_csv(model_output)\n",
        "\n",
        "\n",
        "# full reference results\n",
        "print(\"================================================================\")\n",
        "print(\"Full reference evaluation\")\n",
        "print(\"================================================================\")\n",
        "print(\"b1\", np.mean(b1), \"b2\", np.mean(b2), \"b3\", np.mean(b3), \"b4\", np.mean(b4))\n",
        "print(\"rouge-L\", np.mean(r))\n",
        "print(\"m\", np.mean(m))\n",
        "print(\"c\", np.mean(c))\n",
        "\n",
        "ftgpt2_prompt_b1 = np.mean(b1)\n",
        "ftgpt2_prompt_b2 = np.mean(b2)\n",
        "ftgpt2_prompt_b3 = np.mean(b3)\n",
        "ftgpt2_prompt_b4 = np.mean(b4)\n",
        "ftgpt2_prompt_m = np.mean(m)\n",
        "ftgpt2_prompt_r = np.mean(r)\n",
        "ftgpt2_prompt_c = np.mean(c)\n",
        "\n",
        "# question body results\n",
        "print(\"================================================================\")\n",
        "print(\"Question body evaluation\")\n",
        "print(\"================================================================\")\n",
        "print(\"b1\", np.mean(t_b1), \"b2\", np.mean(t_b2), \"b3\", np.mean(t_b3), \"b4\", np.mean(t_b4))\n",
        "print(\"rouge-L\", np.mean(t_r))\n",
        "print(\"m\", np.mean(t_m))\n",
        "print(\"c\", np.mean(t_c))\n",
        "\n",
        "t_ftgpt2_prompt_b1 = np.mean(t_b1)\n",
        "t_ftgpt2_prompt_b2 = np.mean(t_b2)\n",
        "t_ftgpt2_prompt_b3 = np.mean(t_b3)\n",
        "t_ftgpt2_prompt_b4 = np.mean(t_b4)\n",
        "t_ftgpt2_prompt_m = np.mean(t_m)\n",
        "t_ftgpt2_prompt_r = np.mean(t_r)\n",
        "t_ftgpt2_prompt_c = np.mean(t_c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 2.5 Comparing 2.1-2.4, and get the conclusion for RQ2. \"Our zero-shot facet-constrained approach significantly improve baseline methods.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------------------------------\n",
            "|                               Full reference evaluation                               |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|MODEL                    BLEU1    BLEU2    BLEU3    BLEU4    METEOR   ROUGE    COVERAGE|\n",
            "-----------------------------------------------------------------------------------------\n",
            "|Template append facet    38.56    14.07    9.98     7.81     33.87    46.02    100.0   |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|Existing finetuned GPT2  27.75    10.79    6.03     3.83     28.56    31.71    20.85   |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|Prompt finetuned GPT2    32.79    14.58    8.57     5.63     37.55    40.81    72.55   |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|Ours                     41.8     17.57    9.71     6.45     38.52    44.19    98.96   |\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "|                                Question body evaluation                               |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|MODEL                    BLEU1    BLEU2    BLEU3    BLEU4    METEOR   ROUGE    COVERAGE|\n",
            "-----------------------------------------------------------------------------------------\n",
            "|Template append facet    25.49    5.75     3.27     3.35     22.09    38.58    100.0   |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|Existing finetuned GPT2  16.38    5.37     2.67     2.37     18.41    20.14    20.21   |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|Prompt finetuned GPT2    24.13    10.37    5.73     4.49     31.84    32.99    71.41   |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|Ours                     38.51    18.17    10.81    8.46     37.47    43.79    98.67   |\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|                               Full reference evaluation                               |\")\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('MODEL', 'BLEU1','BLEU2','BLEU3','BLEU4','METEOR','ROUGE','COVERAGE'))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('Template append facet', \n",
        "                                                            round_metric(template_facet_b1), \n",
        "                                                            round_metric(template_facet_b2), \n",
        "                                                            round_metric(template_facet_b3), \n",
        "                                                            round_metric(template_facet_b4), \n",
        "                                                            round_metric(template_facet_m), \n",
        "                                                            round_metric(template_facet_r), \n",
        "                                                            round_metric(template_facet_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('Existing finetuned GPT2', \n",
        "                                                            round_metric(sekulic_b1), \n",
        "                                                            round_metric(sekulic_b2), \n",
        "                                                            round_metric(sekulic_b3), \n",
        "                                                            round_metric(sekulic_b4), \n",
        "                                                            round_metric(sekulic_m), \n",
        "                                                            round_metric(sekulic_r),\n",
        "                                                            round_metric(sekulic_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('Prompt finetuned GPT2', \n",
        "                                                            round_metric(ftgpt2_prompt_b1), \n",
        "                                                            round_metric(ftgpt2_prompt_b2), \n",
        "                                                            round_metric(ftgpt2_prompt_b3), \n",
        "                                                            round_metric(ftgpt2_prompt_b4), \n",
        "                                                            round_metric(ftgpt2_prompt_m), \n",
        "                                                            round_metric(ftgpt2_prompt_r),\n",
        "                                                            round_metric(ftgpt2_prompt_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('Ours', \n",
        "                                                            round_metric(zero_nd_wsdm_b1), \n",
        "                                                            round_metric(zero_nd_wsdm_b2), \n",
        "                                                            round_metric(zero_nd_wsdm_b3), \n",
        "                                                            round_metric(zero_nd_wsdm_b4), \n",
        "                                                            round_metric(zero_nd_wsdm_m), \n",
        "                                                            round_metric(zero_nd_wsdm_r),\n",
        "                                                            round_metric(zero_nd_wsdm_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|                                Question body evaluation                               |\")\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('MODEL', 'BLEU1','BLEU2','BLEU3','BLEU4','METEOR','ROUGE','COVERAGE'))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('Template append facet', \n",
        "                                                            round_metric(t_template_facet_b1), \n",
        "                                                            round_metric(t_template_facet_b2), \n",
        "                                                            round_metric(t_template_facet_b3), \n",
        "                                                            round_metric(t_template_facet_b4), \n",
        "                                                            round_metric(t_template_facet_m), \n",
        "                                                            round_metric(t_template_facet_r), \n",
        "                                                            round_metric(t_template_facet_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('Existing finetuned GPT2', \n",
        "                                                            round_metric(t_sekulic_b1), \n",
        "                                                            round_metric(t_sekulic_b2), \n",
        "                                                            round_metric(t_sekulic_b3), \n",
        "                                                            round_metric(t_sekulic_b4), \n",
        "                                                            round_metric(t_sekulic_m), \n",
        "                                                            round_metric(t_sekulic_r),\n",
        "                                                            round_metric(t_sekulic_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('Prompt finetuned GPT2', \n",
        "                                                            round_metric(t_ftgpt2_prompt_b1), \n",
        "                                                            round_metric(t_ftgpt2_prompt_b2), \n",
        "                                                            round_metric(t_ftgpt2_prompt_b3), \n",
        "                                                            round_metric(t_ftgpt2_prompt_b4), \n",
        "                                                            round_metric(t_ftgpt2_prompt_m), \n",
        "                                                            round_metric(t_ftgpt2_prompt_r),\n",
        "                                                            round_metric(t_ftgpt2_prompt_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('Ours', \n",
        "                                                            round_metric(t_zero_nd_wsdm_b1), \n",
        "                                                            round_metric(t_zero_nd_wsdm_b2), \n",
        "                                                            round_metric(t_zero_nd_wsdm_b3), \n",
        "                                                            round_metric(t_zero_nd_wsdm_b4), \n",
        "                                                            round_metric(t_zero_nd_wsdm_m), \n",
        "                                                            round_metric(t_zero_nd_wsdm_r),\n",
        "                                                            round_metric(t_zero_nd_wsdm_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# RQ3. How does our question ranking model compare to other methods?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 3.1 Perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 tell me about cass county missouri - list homes sale - do you want to know the list of sale homes\n",
            "100 Find information on ontario california airport. - directions location - do you want to know the location and directions of the airport\n",
            "200 Where can I buy pressure washers? - washer - are you interested in buying washer/dryer washers or do you have any questions\n",
            "300 Tell me more about Rocky Mountain News - recent events historical - would you like to know more about recent historical events\n",
            "400 Where should I order dog clean-up bags - specif bag type - do you need to be specifical about the type of bag you want to order\n",
            "================================================================\n",
            "Full reference evaluation\n",
            "================================================================\n",
            "b1 0.4277656503772235 b2 0.19007325089147045 b3 0.11408309937495881 b4 0.07559809979039746\n",
            "rouge-L 0.45158240338278777\n",
            "m 0.40982020211837566\n",
            "c 0.9782352941176471\n",
            "================================================================\n",
            "Question body evaluation\n",
            "================================================================\n",
            "b1 0.36796210388220796 b2 0.1661403144295007 b3 0.0912581858918981 b4 0.06500624937497966\n",
            "rouge-L 0.40577732395656707\n",
            "m 0.37771407676902025\n",
            "c 0.9747058823529412\n"
          ]
        }
      ],
      "source": [
        "facet_test_file = 'data/clariq_f/ClariQ-FKw-dev.tsv'\n",
        "facet_test_data = pd.read_csv(facet_test_file, sep='\\t')\n",
        "_, facet_test_data = process_clariq_f(facet_test_data)\n",
        "\n",
        "r = []\n",
        "b1, b2, b3, b4 = [], [], [], []\n",
        "m = []\n",
        "c = []\n",
        "\n",
        "t_r = []\n",
        "t_b1, t_b2, t_b3, t_b4 = [], [], [], []\n",
        "t_m = []\n",
        "t_c = []\n",
        "\n",
        "model_output = 'zeroshot_nd_pp.csv'\n",
        "generated_file = 'neurologic_decoding/zero_shot/gpt2facet'\n",
        "\n",
        "if os.path.isfile(model_output):\n",
        "    b1, b2, b3, b4, m, r, c, t_b1, t_b2, t_b3, t_b4, t_m, t_r, t_c = evaluate_from_output(model_output)\n",
        "\n",
        "else:\n",
        "    generated_cq_all_templates = open(generated_file, 'r').readlines()\n",
        "    generated_cq_grouped = [[generated_cq_all_templates[len(starting_texts) * k + l] \n",
        "                                for l in range(len(starting_texts))] \n",
        "                                for k in range(int(len(generated_cq_all_templates)/8))]\n",
        "\n",
        "    for iter, row in facet_test_data.iterrows():\n",
        "        facet = facet_test_data.at[iter, 'facet_desc']\n",
        "        ref = facet_test_data.at[iter, 'question']\n",
        "\n",
        "        template_scores = {}\n",
        "        for full_sentence in generated_cq_grouped[iter]:\n",
        "            query = re.sub('\\[SEP\\]', '&', full_sentence).split('&')[0].strip()\n",
        "            generated_follow_up = re.sub('\\[SEP\\]', '&', full_sentence).split('&')[1].strip()\n",
        "            generated_cq = re.sub('[.?]', '&', generated_follow_up).split('&')[0].strip()\n",
        "            constraint_penalty = 1\n",
        "            for constraint in force_flexible:\n",
        "                if ps.stem(constraint) not in set([ps.stem(w) for w in word_tokenize(generated_cq)]):\n",
        "                    constraint_penalty *= 2\n",
        "            template_scores[generated_cq] = calculatePerplexity(sentence=full_sentence, model=ppl_model, tokenizer=ppl_tokenizer) * constraint_penalty\n",
        "        \n",
        "        generated_cq = sorted(template_scores.keys(), key = lambda x: template_scores[x])[0] \n",
        "        facet_test_data.at[iter, 'generated'] = generated_cq\n",
        "        \n",
        "        # full reference evaluation\n",
        "        hyp_b1, hyp_b2, hyp_b3, hyp_b4, hyp_m, hyp_r, hyp_c = auto_evaluation(ref, generated_cq, facet)\n",
        "\n",
        "        b1.append(hyp_b1)\n",
        "        b2.append(hyp_b2)\n",
        "        b3.append(hyp_b3)\n",
        "        b4.append(hyp_b4)\n",
        "        m.append(hyp_m)\n",
        "        r.append(hyp_r)\n",
        "        c.append(hyp_c)\n",
        "\n",
        "        # question body evaluation\n",
        "        truncate_ref = ' '.join(ref.split()[template_len:])\n",
        "        truncate_generated_cq = ' '.join(generated_cq.split()[template_len:])\n",
        "        \n",
        "        t_hyp_b1, t_hyp_b2, t_hyp_b3, t_hyp_b4, t_hyp_m, t_hyp_r, t_hyp_c = auto_evaluation(truncate_ref, truncate_generated_cq, facet)\n",
        "\n",
        "        t_b1.append(t_hyp_b1)\n",
        "        t_b2.append(t_hyp_b2)\n",
        "        t_b3.append(t_hyp_b3)\n",
        "        t_b4.append(t_hyp_b4)\n",
        "        t_m.append(t_hyp_m)\n",
        "        t_r.append(t_hyp_r)\n",
        "        t_c.append(t_hyp_c)\n",
        "    \n",
        "    output_df = facet_test_data[['initial_request', 'facet_desc', 'question', 'generated']]\n",
        "    output_df.columns = ['query', 'facet', 'reference', 'candidate']\n",
        "    output_df.to_csv(model_output)\n",
        "\n",
        "\n",
        "# full reference results\n",
        "print(\"================================================================\")\n",
        "print(\"Full reference evaluation\")\n",
        "print(\"================================================================\")\n",
        "print(\"b1\", np.mean(b1), \"b2\", np.mean(b2), \"b3\", np.mean(b3), \"b4\", np.mean(b4))\n",
        "print(\"rouge-L\", np.mean(r))\n",
        "print(\"m\", np.mean(m))\n",
        "print(\"c\", np.mean(c))\n",
        "\n",
        "zero_nd_pp_b1 = np.mean(b1)\n",
        "zero_nd_pp_b2 = np.mean(b2)\n",
        "zero_nd_pp_b3 = np.mean(b3)\n",
        "zero_nd_pp_b4 = np.mean(b4)\n",
        "zero_nd_pp_m = np.mean(m)\n",
        "zero_nd_pp_r = np.mean(r)\n",
        "zero_nd_pp_c = np.mean(c)\n",
        "\n",
        "# question body results\n",
        "print(\"================================================================\")\n",
        "print(\"Question body evaluation\")\n",
        "print(\"================================================================\")\n",
        "print(\"b1\", np.mean(t_b1), \"b2\", np.mean(t_b2), \"b3\", np.mean(t_b3), \"b4\", np.mean(t_b4))\n",
        "print(\"rouge-L\", np.mean(t_r))\n",
        "print(\"m\", np.mean(t_m))\n",
        "print(\"c\", np.mean(t_c))\n",
        "\n",
        "t_zero_nd_pp_b1 = np.mean(t_b1)\n",
        "t_zero_nd_pp_b2 = np.mean(t_b2)\n",
        "t_zero_nd_pp_b3 = np.mean(t_b3)\n",
        "t_zero_nd_pp_b4 = np.mean(t_b4)\n",
        "t_zero_nd_pp_m = np.mean(t_m)\n",
        "t_zero_nd_pp_r = np.mean(t_r)\n",
        "t_zero_nd_pp_c = np.mean(t_c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 3.2 AutoScores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 tell me about cass county missouri - list homes sale - are you looking for a list of sale homes\n",
            "100 Find information on ontario california airport. - directions location - would you like to send directions to your location\n",
            "200 Where can I buy pressure washers? - washer - are you looking for a washer\n",
            "300 Tell me more about Rocky Mountain News - recent events historical - do you need information on recent historical events\n",
            "400 Where should I order dog clean-up bags - specif bag type - do you want to know specifical bag type\n",
            "================================================================\n",
            "Full reference evaluation\n",
            "================================================================\n",
            "b1 0.4342383247669103 b2 0.20474687017114682 b3 0.12927541734987133 b4 0.09641742120931246\n",
            "rouge-L 0.4786832211408991\n",
            "m 0.41033188202381593\n",
            "c 0.9827843137254902\n",
            "================================================================\n",
            "Question body evaluation\n",
            "================================================================\n",
            "b1 0.3488173362287711 b2 0.1624716052304113 b3 0.10472807656908299 b4 0.08069872668358835\n",
            "rouge-L 0.434214050123522\n",
            "m 0.3388249958920614\n",
            "c 0.9582745098039215\n"
          ]
        }
      ],
      "source": [
        "facet_test_file = 'data/clariq_f/ClariQ-FKw-dev.tsv'\n",
        "facet_test_data = pd.read_csv(facet_test_file, sep='\\t')\n",
        "_, facet_test_data = process_clariq_f(facet_test_data)\n",
        "\n",
        "r = []\n",
        "b1, b2, b3, b4 = [], [], [], []\n",
        "m = []\n",
        "c = []\n",
        "\n",
        "t_r = []\n",
        "t_b1, t_b2, t_b3, t_b4 = [], [], [], []\n",
        "t_m = []\n",
        "t_c = []\n",
        "\n",
        "model_output = 'zeroshot_nd_auto.csv'\n",
        "generated_file = 'neurologic_decoding/zero_shot/gpt2facet'\n",
        "\n",
        "if os.path.isfile(model_output):\n",
        "    b1, b2, b3, b4, m, r, c, t_b1, t_b2, t_b3, t_b4, t_m, t_r, t_c = evaluate_from_output(model_output)\n",
        "\n",
        "else:\n",
        "    generated_cq_all_templates = open(generated_file, 'r').readlines()\n",
        "    generated_cq_grouped = [[generated_cq_all_templates[len(starting_texts) * k + l] \n",
        "                                for l in range(len(starting_texts))] \n",
        "                                for k in range(int(len(generated_cq_all_templates)/8))]\n",
        "\n",
        "    for iter, row in facet_test_data.iterrows():\n",
        "        query = facet_test_data.at[iter, 'initial_request']\n",
        "        facet = facet_test_data.at[iter, 'facet_desc']\n",
        "        ref = facet_test_data.at[iter, 'question']\n",
        "        facet_list = facet.split()\n",
        "\n",
        "        noun_in_query = [token.text for token in pos_tagger(query) if token.pos_ == 'NOUN']\n",
        "        propn_in_query = [token.text.lower() for token in pos_tagger(query) if token.pos_ == 'PROPN']\n",
        "\n",
        "        template_scores = {}\n",
        "        for full_sentence in generated_cq_grouped[iter]:\n",
        "            query = re.sub('\\[SEP\\]', '&', full_sentence).split('&')[0].strip()\n",
        "            generated_follow_up = re.sub('\\[SEP\\]', '&', full_sentence).split('&')[1].strip()\n",
        "            generated_cq = re.sub('[.?]', '&', generated_follow_up).split('&')[0].strip()  \n",
        "            template_scores[generated_cq] = rs.get_scores(generated_cq, ' '.join(noun_in_query + propn_in_query + facet_list))[0]['rouge-l']['f'] \\\n",
        "                                + sentence_bleu([word_tokenize(' '.join(noun_in_query + propn_in_query + facet_list))], word_tokenize(generated_cq), \n",
        "                                weights=(1, 1, 1, 1), smoothing_function = SmoothingFunction().method1) + \\\n",
        "                                meteor_score([word_tokenize(' '.join(noun_in_query + propn_in_query + facet_list))], word_tokenize(generated_cq))\n",
        "        \n",
        "        sorted_template_scores = sorted(template_scores.items(), key = lambda x: x[1], reverse=True)\n",
        "        generated_cq = sorted(template_scores.keys(), key = lambda x: template_scores[x], reverse=True)[0] \n",
        "        facet_test_data.at[iter, 'generated'] = generated_cq\n",
        "\n",
        "        # full reference evaluation\n",
        "        hyp_b1, hyp_b2, hyp_b3, hyp_b4, hyp_m, hyp_r, hyp_c = auto_evaluation(ref, generated_cq, facet)\n",
        "\n",
        "        b1.append(hyp_b1)\n",
        "        b2.append(hyp_b2)\n",
        "        b3.append(hyp_b3)\n",
        "        b4.append(hyp_b4)\n",
        "        m.append(hyp_m)\n",
        "        r.append(hyp_r)\n",
        "        c.append(hyp_c)\n",
        "\n",
        "        # question body evaluation\n",
        "        truncate_ref = ' '.join(ref.split()[template_len:])\n",
        "        truncate_generated_cq = ' '.join(generated_cq.split()[template_len:])\n",
        "        \n",
        "        t_hyp_b1, t_hyp_b2, t_hyp_b3, t_hyp_b4, t_hyp_m, t_hyp_r, t_hyp_c = auto_evaluation(truncate_ref, truncate_generated_cq, facet)\n",
        "\n",
        "        t_b1.append(t_hyp_b1)\n",
        "        t_b2.append(t_hyp_b2)\n",
        "        t_b3.append(t_hyp_b3)\n",
        "        t_b4.append(t_hyp_b4)\n",
        "        t_m.append(t_hyp_m)\n",
        "        t_r.append(t_hyp_r)\n",
        "        t_c.append(t_hyp_c)\n",
        "\n",
        "    output_df = facet_test_data[['initial_request', 'facet_desc', 'question', 'generated']]\n",
        "    output_df.columns = ['query', 'facet', 'reference', 'candidate']\n",
        "    output_df.to_csv(model_output)\n",
        "\n",
        "# full reference results\n",
        "print(\"================================================================\")\n",
        "print(\"Full reference evaluation\")\n",
        "print(\"================================================================\")\n",
        "print(\"b1\", np.mean(b1), \"b2\", np.mean(b2), \"b3\", np.mean(b3), \"b4\", np.mean(b4))\n",
        "print(\"rouge-L\", np.mean(r))\n",
        "print(\"m\", np.mean(m))\n",
        "print(\"c\", np.mean(c))\n",
        "\n",
        "zero_nd_auto_b1 = np.mean(b1)\n",
        "zero_nd_auto_b2 = np.mean(b2)\n",
        "zero_nd_auto_b3 = np.mean(b3)\n",
        "zero_nd_auto_b4 = np.mean(b4)\n",
        "zero_nd_auto_m = np.mean(m)\n",
        "zero_nd_auto_r = np.mean(r)\n",
        "zero_nd_auto_c = np.mean(c)\n",
        "\n",
        "# question body results\n",
        "print(\"================================================================\")\n",
        "print(\"Question body evaluation\")\n",
        "print(\"================================================================\")\n",
        "print(\"b1\", np.mean(t_b1), \"b2\", np.mean(t_b2), \"b3\", np.mean(t_b3), \"b4\", np.mean(t_b4))\n",
        "print(\"rouge-L\", np.mean(t_r))\n",
        "print(\"m\", np.mean(t_m))\n",
        "print(\"c\", np.mean(t_c))\n",
        "\n",
        "t_zero_nd_auto_b1 = np.mean(t_b1)\n",
        "t_zero_nd_auto_b2 = np.mean(t_b2)\n",
        "t_zero_nd_auto_b3 = np.mean(t_b3)\n",
        "t_zero_nd_auto_b4 = np.mean(t_b4)\n",
        "t_zero_nd_auto_m = np.mean(t_m)\n",
        "t_zero_nd_auto_r = np.mean(t_r)\n",
        "t_zero_nd_auto_c = np.mean(t_c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 3.3 [Cross-encoder](https://www.bing.com/search?q=poly+encoder+paper&cvid=46293035bd454d6d9745d27396391cfc&aqs=edge..69i57j0l2j69i59j69i64j69i11004.7939j0j1&pglt=41&FORM=ANNAB1&PC=LCTS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 tell me about cass county missouri - list homes sale - do you want to know the list of sale homes\n",
            "100 Find information on ontario california airport. - directions location - do you want to get directions to this location\n",
            "200 Where can I buy pressure washers? - washer - do you want to know which washer to use\n",
            "300 Tell me more about Rocky Mountain News - recent events historical - are you looking for historical information about recent events\n",
            "400 Where should I order dog clean-up bags - specif bag type - are you looking for specifc type bag\n",
            "================================================================\n",
            "Full reference evaluation\n",
            "================================================================\n",
            "b1 0.41373244461884806 b2 0.176513247123985 b3 0.10313598447532396 b4 0.06910221676005299\n",
            "rouge-L 0.4417860678647769\n",
            "m 0.3977539356223055\n",
            "c 0.9165490196078431\n",
            "================================================================\n",
            "Question body evaluation\n",
            "================================================================\n",
            "b1 0.3625420522173297 b2 0.1688791182506196 b3 0.09992607000759766 b4 0.07243788969725326\n",
            "rouge-L 0.40447814113736086\n",
            "m 0.3724821276449544\n",
            "c 0.9087058823529411\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.append(\"/home/azureuser/cloudfiles/code/Users/t-zhendwang/srconvsearch\")\n",
        "sys.path.append(\"/home/azureuser/cloudfiles/code/Users/t-zhendwang/srconvsearch/conversationalQA/ParlAI\")\n",
        "from conversationalQA.ParlAI.parlai.scripts.interactive import Interactive, rerank\n",
        "\n",
        "facet_test_file = 'data/clariq_f/ClariQ-FKw-dev.tsv'\n",
        "facet_test_data = pd.read_csv(facet_test_file, sep='\\t')\n",
        "_, facet_test_data = process_clariq_f(facet_test_data)\n",
        "\n",
        "r = []\n",
        "b1, b2, b3, b4 = [], [], [], []\n",
        "m = []\n",
        "c = []\n",
        "\n",
        "t_r = []\n",
        "t_b1, t_b2, t_b3, t_b4 = [], [], [], []\n",
        "t_m = []\n",
        "t_c = []\n",
        "\n",
        "model_output = 'zeroshot_nd_cross.csv'\n",
        "generated_file = 'neurologic_decoding/zero_shot/gpt2facet'\n",
        "\n",
        "if os.path.isfile(model_output):\n",
        "    b1, b2, b3, b4, m, r, c, t_b1, t_b2, t_b3, t_b4, t_m, t_r, t_c = evaluate_from_output(model_output)\n",
        "    \n",
        "else:\n",
        "    question_reranker = Interactive.main(model = 'transformer/crossencoder', \\\n",
        "                            model_file = 'zoo:pretrained_transformers/cross_model_huge_reddit/model',  \\\n",
        "                            encode_candidate_vecs = False,  eval_candidates = 'inline', interactive_candidates = 'inline',\n",
        "                            return_cand_scores = True)\n",
        "\n",
        "    generated_cq_all_templates = open(generated_file, 'r').readlines()\n",
        "    generated_cq_grouped = [[generated_cq_all_templates[len(starting_texts) * k + l] \n",
        "                                for l in range(len(starting_texts))] \n",
        "                                for k in range(int(len(generated_cq_all_templates)/8))]\n",
        "\n",
        "    for iter, row in facet_test_data.iterrows():\n",
        "        query = facet_test_data.at[iter, 'initial_request']\n",
        "        facet = facet_test_data.at[iter, 'facet_desc']\n",
        "        ref = facet_test_data.at[iter, 'question']\n",
        "        \n",
        "        generated_follow_ups = [re.sub('\\[SEP\\]', '&', full_sentence).split('&')[1].strip() for full_sentence in generated_cq_grouped[iter]]\n",
        "        generated_cqs = [re.sub('[.?]', '&', generated_follow_up).split('&')[0].strip() for generated_follow_up in generated_follow_ups]\n",
        "\n",
        "        questions, questions_scores = rerank(question_reranker, query, '', generated_cqs)\n",
        "        generated_cq = questions[0]\n",
        "        facet_test_data.at[iter, 'generated'] = generated_cq\n",
        "\n",
        "        # full reference evaluation\n",
        "        hyp_b1, hyp_b2, hyp_b3, hyp_b4, hyp_m, hyp_r, hyp_c = auto_evaluation(ref, generated_cq, facet)\n",
        "\n",
        "        b1.append(hyp_b1)\n",
        "        b2.append(hyp_b2)\n",
        "        b3.append(hyp_b3)\n",
        "        b4.append(hyp_b4)\n",
        "        m.append(hyp_m)\n",
        "        r.append(hyp_r)\n",
        "        c.append(hyp_c)\n",
        "\n",
        "        # question body evaluation\n",
        "        truncate_ref = ' '.join(ref.split()[template_len:])\n",
        "        truncate_generated_cq = ' '.join(generated_cq.split()[template_len:])\n",
        "        \n",
        "        t_hyp_b1, t_hyp_b2, t_hyp_b3, t_hyp_b4, t_hyp_m, t_hyp_r, t_hyp_c = auto_evaluation(truncate_ref, truncate_generated_cq, facet)\n",
        "\n",
        "        t_b1.append(t_hyp_b1)\n",
        "        t_b2.append(t_hyp_b2)\n",
        "        t_b3.append(t_hyp_b3)\n",
        "        t_b4.append(t_hyp_b4)\n",
        "        t_m.append(t_hyp_m)\n",
        "        t_r.append(t_hyp_r)\n",
        "        t_c.append(t_hyp_c)\n",
        "\n",
        "    output_df = facet_test_data[['initial_request', 'facet_desc', 'question', 'generated']]\n",
        "    output_df.columns = ['query', 'facet', 'reference', 'candidate']\n",
        "    output_df.to_csv(model_output)\n",
        "    \n",
        "# full reference results\n",
        "print(\"================================================================\")\n",
        "print(\"Full reference evaluation\")\n",
        "print(\"================================================================\")\n",
        "print(\"b1\", np.mean(b1), \"b2\", np.mean(b2), \"b3\", np.mean(b3), \"b4\", np.mean(b4))\n",
        "print(\"rouge-L\", np.mean(r))\n",
        "print(\"m\", np.mean(m))\n",
        "print(\"c\", np.mean(c))\n",
        "\n",
        "zero_nd_cross_b1 = np.mean(b1)\n",
        "zero_nd_cross_b2 = np.mean(b2)\n",
        "zero_nd_cross_b3 = np.mean(b3)\n",
        "zero_nd_cross_b4 = np.mean(b4)\n",
        "zero_nd_cross_m = np.mean(m)\n",
        "zero_nd_cross_r = np.mean(r)\n",
        "zero_nd_cross_c = np.mean(c)\n",
        "\n",
        "# question body results\n",
        "print(\"================================================================\")\n",
        "print(\"Question body evaluation\")\n",
        "print(\"================================================================\")\n",
        "print(\"b1\", np.mean(t_b1), \"b2\", np.mean(t_b2), \"b3\", np.mean(t_b3), \"b4\", np.mean(t_b4))\n",
        "print(\"rouge-L\", np.mean(t_r))\n",
        "print(\"m\", np.mean(t_m))\n",
        "print(\"c\", np.mean(t_c))\n",
        "\n",
        "t_zero_nd_cross_b1 = np.mean(t_b1)\n",
        "t_zero_nd_cross_b2 = np.mean(t_b2)\n",
        "t_zero_nd_cross_b3 = np.mean(t_b3)\n",
        "t_zero_nd_cross_b4 = np.mean(t_b4)\n",
        "t_zero_nd_cross_m = np.mean(t_m)\n",
        "t_zero_nd_cross_r = np.mean(t_r)\n",
        "t_zero_nd_cross_c = np.mean(t_c)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 3.4 [NTES](https://arxiv.org/pdf/2010.14202.pdf) (Pretrained clarifying question ranker)\n",
        "In AML terminal, run\n",
        "\n",
        "```\n",
        "cd Clariq_System\n",
        "python rank.py\n",
        "```\n",
        "Make sure you get the generated question output \"zero\\_nd\\_ntes.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 tell me about cass county missouri - list homes sale - would you like to list the sale of homes\n",
            "\n",
            "100 Find information on ontario california airport. - directions location - are you interested in getting directions to this location\n",
            "\n",
            "200 Where can I buy pressure washers? - washer - are you interested in buying washer/dryer washers or do you have any questions\n",
            "\n",
            "300 Tell me more about Rocky Mountain News - recent events historical - do you want information about recent historical events\n",
            "\n",
            "400 Where should I order dog clean-up bags - specif bag type - are you looking for specifc type bag\n",
            "\n",
            "================================================================\n",
            "Full reference evaluation\n",
            "================================================================\n",
            "b1 0.36920461276522515 b2 0.16492584626033616 b3 0.09993981802772525 b4 0.0738805427686033\n",
            "rouge-L 0.41492680176106006\n",
            "m 0.3569165665608735\n",
            "c 0.7732156862745099\n",
            "================================================================\n",
            "Question body evaluation\n",
            "================================================================\n",
            "b1 0.29458733795768594 b2 0.1403471043416165 b3 0.09190463977437048 b4 0.07100761161471664\n",
            "rouge-L 0.33816654471658264\n",
            "m 0.2796704582312601\n",
            "c 0.7732156862745099\n"
          ]
        }
      ],
      "source": [
        "facet_test_file = 'data/clariq_f/ClariQ-FKw-dev.tsv'\n",
        "facet_test_data = pd.read_csv(facet_test_file, sep='\\t')\n",
        "_, facet_test_data = process_clariq_f(facet_test_data)\n",
        "\n",
        "r = []\n",
        "b1, b2, b3, b4 = [], [], [], []\n",
        "m = []\n",
        "c = []\n",
        "\n",
        "t_r = []\n",
        "t_b1, t_b2, t_b3, t_b4 = [], [], [], []\n",
        "t_m = []\n",
        "t_c = []\n",
        "\n",
        "model_output = 'zeroshot_nd_ntes.csv'\n",
        "generated_file = 'neurologic_decoding/zero_shot/gpt2facet'\n",
        "\n",
        "if os.path.isfile(model_output):\n",
        "    b1, b2, b3, b4, m, r, c, t_b1, t_b2, t_b3, t_b4, t_m, t_r, t_c = evaluate_from_output(model_output)\n",
        "else:\n",
        "    print(\"missing output file from NTES code, please run the ranker first\")\n",
        "\n",
        "\n",
        "# full reference results\n",
        "print(\"================================================================\")\n",
        "print(\"Full reference evaluation\")\n",
        "print(\"================================================================\")\n",
        "print(\"b1\", np.mean(b1), \"b2\", np.mean(b2), \"b3\", np.mean(b3), \"b4\", np.mean(b4))\n",
        "print(\"rouge-L\", np.mean(r))\n",
        "print(\"m\", np.mean(m))\n",
        "print(\"c\", np.mean(c))\n",
        "\n",
        "zero_nd_ntes_b1 = np.mean(b1)\n",
        "zero_nd_ntes_b2 = np.mean(b2)\n",
        "zero_nd_ntes_b3 = np.mean(b3)\n",
        "zero_nd_ntes_b4 = np.mean(b4)\n",
        "zero_nd_ntes_m = np.mean(m)\n",
        "zero_nd_ntes_r = np.mean(r)\n",
        "zero_nd_ntes_c = np.mean(c)\n",
        "\n",
        "# question body results\n",
        "print(\"================================================================\")\n",
        "print(\"Question body evaluation\")\n",
        "print(\"================================================================\")\n",
        "print(\"b1\", np.mean(t_b1), \"b2\", np.mean(t_b2), \"b3\", np.mean(t_b3), \"b4\", np.mean(t_b4))\n",
        "print(\"rouge-L\", np.mean(t_r))\n",
        "print(\"m\", np.mean(t_m))\n",
        "print(\"c\", np.mean(t_c))\n",
        "\n",
        "t_zero_nd_ntes_b1 = np.mean(t_b1)\n",
        "t_zero_nd_ntes_b2 = np.mean(t_b2)\n",
        "t_zero_nd_ntes_b3 = np.mean(t_b3)\n",
        "t_zero_nd_ntes_b4 = np.mean(t_b4)\n",
        "t_zero_nd_ntes_m = np.mean(t_m)\n",
        "t_zero_nd_ntes_r = np.mean(t_r)\n",
        "t_zero_nd_ntes_c = np.mean(t_c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 3.5 WSDM (Ours)\n",
        "\n",
        "Same as 1.2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 3.6 Oracle ranker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 tell me about cass county missouri - list homes sale - are you interested in a list of sale homes\n",
            "100 Find information on ontario california airport. - directions location - would you like to send directions to your location\n",
            "200 Where can I buy pressure washers? - washer - are you looking for a washer\n",
            "300 Tell me more about Rocky Mountain News - recent events historical - do you want information about recent historical events\n",
            "400 Where should I order dog clean-up bags - specif bag type - are you looking for specifc type bag\n",
            "================================================================\n",
            "Full reference evaluation\n",
            "================================================================\n",
            "b1 0.6160936744403871 b2 0.43807769108461336 b3 0.3346436810798287 b4 0.2414089762434254\n",
            "rouge-L 0.6897123207824376\n",
            "m 0.6517379490599675\n",
            "c 0.9269019607843136\n",
            "================================================================\n",
            "Question body evaluation\n",
            "================================================================\n",
            "b1 0.4074544213117534 b2 0.21095173470334003 b3 0.13615480970344712 b4 0.09920045448032966\n",
            "rouge-L 0.479522132674582\n",
            "m 0.4090943786312325\n",
            "c 0.9123921568627451\n"
          ]
        }
      ],
      "source": [
        "facet_test_file = 'data/clariq_f/ClariQ-FKw-dev.tsv'\n",
        "facet_test_data = pd.read_csv(facet_test_file, sep='\\t')\n",
        "_, facet_test_data = process_clariq_f(facet_test_data)\n",
        "\n",
        "r = []\n",
        "b1, b2, b3, b4 = [], [], [], []\n",
        "m = []\n",
        "c = []\n",
        "\n",
        "t_r = []\n",
        "t_b1, t_b2, t_b3, t_b4 = [], [], [], []\n",
        "t_m = []\n",
        "t_c = []\n",
        "\n",
        "model_output = 'zeroshot_nd_oracle.csv'\n",
        "generated_file = 'neurologic_decoding/zero_shot/gpt2facet'\n",
        "\n",
        "if os.path.isfile(model_output):\n",
        "    b1, b2, b3, b4, m, r, c, t_b1, t_b2, t_b3, t_b4, t_m, t_r, t_c = evaluate_from_output(model_output)\n",
        "\n",
        "else:\n",
        "    generated_cq_all_templates = open(generated_file, 'r').readlines()\n",
        "    generated_cq_grouped = [[generated_cq_all_templates[len(starting_texts) * k + l] \n",
        "                                for l in range(len(starting_texts))] \n",
        "                                for k in range(int(len(generated_cq_all_templates)/8))]\n",
        "\n",
        "    for iter, row in facet_test_data.iterrows():\n",
        "        facet = facet_test_data.at[iter, 'facet_desc']\n",
        "        ref = facet_test_data.at[iter, 'question']\n",
        "\n",
        "        template_scores = {}\n",
        "        for full_sentence in generated_cq_grouped[iter]:\n",
        "            query = re.sub('\\[SEP\\]', '&', full_sentence).split('&')[0].strip()\n",
        "            generated_cq = re.sub('[.?]', '&', generated_follow_up).split('&')[0].strip()  \n",
        "            template_scores[generated_cq] = rs.get_scores(generated_cq, ref)[0]['rouge-l']['f']\n",
        "        \n",
        "        generated_cq = sorted(template_scores.keys(), key = lambda x: template_scores[x], reverse=True)[0] \n",
        "        facet_test_data.at[iter, 'generated'] = generated_cq\n",
        "\n",
        "        # full reference evaluation\n",
        "        hyp_b1, hyp_b2, hyp_b3, hyp_b4, hyp_m, hyp_r, hyp_c = auto_evaluation(ref, generated_cq, facet)\n",
        "\n",
        "        b1.append(hyp_b1)\n",
        "        b2.append(hyp_b2)\n",
        "        b3.append(hyp_b3)\n",
        "        b4.append(hyp_b4)\n",
        "        m.append(hyp_m)\n",
        "        r.append(hyp_r)\n",
        "        c.append(hyp_c)\n",
        "\n",
        "        # question body evaluation\n",
        "        truncate_ref = ' '.join(ref.split()[template_len:])\n",
        "        truncate_generated_cq = ' '.join(generated_cq.split()[template_len:])\n",
        "        \n",
        "        t_hyp_b1, t_hyp_b2, t_hyp_b3, t_hyp_b4, t_hyp_m, t_hyp_r, t_hyp_c = auto_evaluation(truncate_ref, truncate_generated_cq, facet)\n",
        "\n",
        "        t_b1.append(t_hyp_b1)\n",
        "        t_b2.append(t_hyp_b2)\n",
        "        t_b3.append(t_hyp_b3)\n",
        "        t_b4.append(t_hyp_b4)\n",
        "        t_m.append(t_hyp_m)\n",
        "        t_r.append(t_hyp_r)\n",
        "        t_c.append(t_hyp_c)\n",
        "\n",
        "    output_df = facet_test_data[['initial_request', 'facet_desc', 'question', 'generated']]\n",
        "    output_df.columns = ['query', 'facet', 'reference', 'candidate']\n",
        "    output_df.to_csv(model_output)\n",
        "\n",
        "# full reference results\n",
        "print(\"================================================================\")\n",
        "print(\"Full reference evaluation\")\n",
        "print(\"================================================================\")\n",
        "print(\"b1\", np.mean(b1), \"b2\", np.mean(b2), \"b3\", np.mean(b3), \"b4\", np.mean(b4))\n",
        "print(\"rouge-L\", np.mean(r))\n",
        "print(\"m\", np.mean(m))\n",
        "print(\"c\", np.mean(c))\n",
        "\n",
        "zero_nd_oracle_b1 = np.mean(b1)\n",
        "zero_nd_oracle_b2 = np.mean(b2)\n",
        "zero_nd_oracle_b3 = np.mean(b3)\n",
        "zero_nd_oracle_b4 = np.mean(b4)\n",
        "zero_nd_oracle_m = np.mean(m)\n",
        "zero_nd_oracle_r = np.mean(r)\n",
        "zero_nd_oracle_c = np.mean(c)\n",
        "\n",
        "# question body results\n",
        "print(\"================================================================\")\n",
        "print(\"Question body evaluation\")\n",
        "print(\"================================================================\")\n",
        "print(\"b1\", np.mean(t_b1), \"b2\", np.mean(t_b2), \"b3\", np.mean(t_b3), \"b4\", np.mean(t_b4))\n",
        "print(\"rouge-L\", np.mean(t_r))\n",
        "print(\"m\", np.mean(t_m))\n",
        "print(\"c\", np.mean(t_c))\n",
        "\n",
        "t_zero_nd_oracle_b1 = np.mean(t_b1)\n",
        "t_zero_nd_oracle_b2 = np.mean(t_b2)\n",
        "t_zero_nd_oracle_b3 = np.mean(t_b3)\n",
        "t_zero_nd_oracle_b4 = np.mean(t_b4)\n",
        "t_zero_nd_oracle_m = np.mean(t_m)\n",
        "t_zero_nd_oracle_r = np.mean(t_r)\n",
        "t_zero_nd_oracle_c = np.mean(t_c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 3.7 Comparing 3.1-3.6, getting the conclusion that \"WSDM\" is the best ranker choice in terms of question body quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------------------------------\n",
            "|                               Full reference evaluation                               |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|MODEL                    BLEU1    BLEU2    BLEU3    BLEU4    METEOR   ROUGE    COVERAGE|\n",
            "-----------------------------------------------------------------------------------------\n",
            "|Perplexity               42.78    19.01    11.41    7.56     40.98    45.16    97.82   |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|AutoScore                43.42    20.47    12.93    9.64     41.03    47.87    98.28   |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|Cross encoder            41.37    17.65    10.31    6.91     39.78    44.18    91.65   |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|NTES                     36.92    16.49    9.99     7.39     35.69    41.49    77.32   |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|WSDM                     41.8     17.57    9.71     6.45     38.52    44.19    98.96   |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|Oracle                   61.61    43.81    33.46    24.14    65.17    68.97    92.69   |\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "|                                Question body evaluation                               |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|MODEL                    BLEU1    BLEU2    BLEU3    BLEU4    METEOR   ROUGE    COVERAGE|\n",
            "-----------------------------------------------------------------------------------------\n",
            "|Perplexity               36.8     16.61    9.13     6.5      37.77    40.58    97.47   |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|AutoScore                34.88    16.25    10.47    8.07     33.88    43.42    95.83   |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|Cross encoder            36.25    16.89    9.99     7.24     37.25    40.45    90.87   |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|NTES                     29.46    14.03    9.19     7.1      27.97    33.82    77.32   |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|WSDM                     38.51    18.17    10.81    8.46     37.47    43.79    98.67   |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|Oracle                   40.75    21.1     13.62    9.92     40.91    47.95    91.24   |\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|                               Full reference evaluation                               |\")\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('MODEL', 'BLEU1','BLEU2','BLEU3','BLEU4','METEOR','ROUGE','COVERAGE'))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('Perplexity', \n",
        "                                                            round_metric(zero_nd_pp_b1), \n",
        "                                                            round_metric(zero_nd_pp_b2), \n",
        "                                                            round_metric(zero_nd_pp_b3), \n",
        "                                                            round_metric(zero_nd_pp_b4), \n",
        "                                                            round_metric(zero_nd_pp_m), \n",
        "                                                            round_metric(zero_nd_pp_r), \n",
        "                                                            round_metric(zero_nd_pp_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('AutoScore', \n",
        "                                                            round_metric(zero_nd_auto_b1), \n",
        "                                                            round_metric(zero_nd_auto_b2), \n",
        "                                                            round_metric(zero_nd_auto_b3), \n",
        "                                                            round_metric(zero_nd_auto_b4), \n",
        "                                                            round_metric(zero_nd_auto_m), \n",
        "                                                            round_metric(zero_nd_auto_r), \n",
        "                                                            round_metric(zero_nd_auto_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('Cross encoder', \n",
        "                                                            round_metric(zero_nd_cross_b1), \n",
        "                                                            round_metric(zero_nd_cross_b2), \n",
        "                                                            round_metric(zero_nd_cross_b3), \n",
        "                                                            round_metric(zero_nd_cross_b4), \n",
        "                                                            round_metric(zero_nd_cross_m), \n",
        "                                                            round_metric(zero_nd_cross_r), \n",
        "                                                            round_metric(zero_nd_cross_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('NTES', \n",
        "                                                            round_metric(zero_nd_ntes_b1), \n",
        "                                                            round_metric(zero_nd_ntes_b2), \n",
        "                                                            round_metric(zero_nd_ntes_b3), \n",
        "                                                            round_metric(zero_nd_ntes_b4), \n",
        "                                                            round_metric(zero_nd_ntes_m), \n",
        "                                                            round_metric(zero_nd_ntes_r), \n",
        "                                                            round_metric(zero_nd_ntes_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('WSDM', \n",
        "                                                            round_metric(zero_nd_wsdm_b1), \n",
        "                                                            round_metric(zero_nd_wsdm_b2), \n",
        "                                                            round_metric(zero_nd_wsdm_b3), \n",
        "                                                            round_metric(zero_nd_wsdm_b4), \n",
        "                                                            round_metric(zero_nd_wsdm_m), \n",
        "                                                            round_metric(zero_nd_wsdm_r),\n",
        "                                                            round_metric(zero_nd_wsdm_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('Oracle', \n",
        "                                                            round_metric(zero_nd_oracle_b1), \n",
        "                                                            round_metric(zero_nd_oracle_b2), \n",
        "                                                            round_metric(zero_nd_oracle_b3), \n",
        "                                                            round_metric(zero_nd_oracle_b4), \n",
        "                                                            round_metric(zero_nd_oracle_m), \n",
        "                                                            round_metric(zero_nd_oracle_r), \n",
        "                                                            round_metric(zero_nd_oracle_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|                                Question body evaluation                               |\")\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('MODEL', 'BLEU1','BLEU2','BLEU3','BLEU4','METEOR','ROUGE','COVERAGE'))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('Perplexity', \n",
        "                                                            round_metric(t_zero_nd_pp_b1), \n",
        "                                                            round_metric(t_zero_nd_pp_b2), \n",
        "                                                            round_metric(t_zero_nd_pp_b3), \n",
        "                                                            round_metric(t_zero_nd_pp_b4), \n",
        "                                                            round_metric(t_zero_nd_pp_m), \n",
        "                                                            round_metric(t_zero_nd_pp_r), \n",
        "                                                            round_metric(t_zero_nd_pp_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('AutoScore', \n",
        "                                                            round_metric(t_zero_nd_auto_b1), \n",
        "                                                            round_metric(t_zero_nd_auto_b2), \n",
        "                                                            round_metric(t_zero_nd_auto_b3), \n",
        "                                                            round_metric(t_zero_nd_auto_b4), \n",
        "                                                            round_metric(t_zero_nd_auto_m), \n",
        "                                                            round_metric(t_zero_nd_auto_r), \n",
        "                                                            round_metric(t_zero_nd_auto_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('Cross encoder', \n",
        "                                                            round_metric(t_zero_nd_cross_b1), \n",
        "                                                            round_metric(t_zero_nd_cross_b2), \n",
        "                                                            round_metric(t_zero_nd_cross_b3), \n",
        "                                                            round_metric(t_zero_nd_cross_b4), \n",
        "                                                            round_metric(t_zero_nd_cross_m), \n",
        "                                                            round_metric(t_zero_nd_cross_r), \n",
        "                                                            round_metric(t_zero_nd_cross_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('NTES', \n",
        "                                                            round_metric(t_zero_nd_ntes_b1), \n",
        "                                                            round_metric(t_zero_nd_ntes_b2), \n",
        "                                                            round_metric(t_zero_nd_ntes_b3), \n",
        "                                                            round_metric(t_zero_nd_ntes_b4), \n",
        "                                                            round_metric(t_zero_nd_ntes_m), \n",
        "                                                            round_metric(t_zero_nd_ntes_r), \n",
        "                                                            round_metric(t_zero_nd_ntes_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('WSDM', \n",
        "                                                            round_metric(t_zero_nd_wsdm_b1), \n",
        "                                                            round_metric(t_zero_nd_wsdm_b2), \n",
        "                                                            round_metric(t_zero_nd_wsdm_b3), \n",
        "                                                            round_metric(t_zero_nd_wsdm_b4), \n",
        "                                                            round_metric(t_zero_nd_wsdm_m), \n",
        "                                                            round_metric(t_zero_nd_wsdm_r),\n",
        "                                                            round_metric(t_zero_nd_wsdm_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('Oracle', \n",
        "                                                            round_metric(t_zero_nd_oracle_b1), \n",
        "                                                            round_metric(t_zero_nd_oracle_b2), \n",
        "                                                            round_metric(t_zero_nd_oracle_b3), \n",
        "                                                            round_metric(t_zero_nd_oracle_b4), \n",
        "                                                            round_metric(t_zero_nd_oracle_m), \n",
        "                                                            round_metric(t_zero_nd_oracle_r), \n",
        "                                                            round_metric(t_zero_nd_oracle_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# RQ4. How good is Neurologic Decoding for facet-driven clarifying? Specifically, how much does it improve over other facet-driven methods and how far is it from perfect?\n",
        "\n",
        "Can be merged with RQ2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# RQ5. How does our GPT-2-based zero-shot facet-constrained approach compare to using Large Language Models such as zero-shot GPT-3? \n",
        "To answer this question, we compare our proposed method with a few-shot prompt guided GPT-3 method, but we mainly compare zero-shot with zero-shot.\n",
        "The few-shot GPT-3 method uses the prompt structure as in our proposed GPT-2 finetune method, which is:\n",
        "\n",
        "## {query} Ask a question that contains words in the list \\[{facet}\\] {clarifying question}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 5.1 Zero-shot GPT3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 tell me about cass county missouri - list homes sale - are you looking for a list of homes for sale in cass county missouri\n",
            "100 Find information on ontario california airport. - directions location - do you need information on the ontario california airport\n",
            "200 Where can I buy pressure washers? - washer - do you want information about a washing machine or a pressure washer\n",
            "300 Tell me more about Rocky Mountain News - recent events historical - do you need information on recent events or historical events\n",
            "400 Where should I order dog clean-up bags - specif bag type - do you want to know what type of bag to use for your dog\n",
            "================================================================\n",
            "Full reference evaluation\n",
            "================================================================\n",
            "b1 0.42959420323127595 b2 0.2144905020365997 b3 0.12975964645585925 b4 0.08756123899640086\n",
            "rouge-L 0.4668552513632458\n",
            "m 0.4595009686068181\n",
            "c 0.8526666666666668\n",
            "================================================================\n",
            "Question body evaluation\n",
            "================================================================\n",
            "b1 0.3943690841916204 b2 0.22815387204349083 b3 0.15282352826624315 b4 0.1193501579145087\n",
            "rouge-L 0.4620705224770397\n",
            "m 0.4636303615409833\n",
            "c 0.829529411764706\n"
          ]
        }
      ],
      "source": [
        "facet_test_file = 'data/clariq_f/ClariQ-FKw-dev.tsv'\n",
        "facet_test_data = pd.read_csv(facet_test_file, sep='\\t')\n",
        "_, facet_test_data = process_clariq_f(facet_test_data)\n",
        "\n",
        "b1, b2, b3, b4 = [], [], [], []\n",
        "m = []\n",
        "r = []\n",
        "c = []\n",
        "\n",
        "t_b1, t_b2, t_b3, t_b4 = [], [], [], []\n",
        "t_m = []\n",
        "t_r = []\n",
        "t_c = []\n",
        "\n",
        "temperature = 0\n",
        "use_examples = 0\n",
        "model_output = 'fewshot_gpt3' + '_examples' + str(use_examples) + '_temp' + str(temperature) + '.csv'\n",
        "model_output_all_templates = 'fewshot_gpt3' + '_examples' + str(use_examples) + '_temp' + str(temperature) +'_all'\n",
        "\n",
        "all_generations = [] # cache the generations to save time and load from calling gpt3\n",
        "\n",
        "if os.path.isfile(model_output):\n",
        "    b1, b2, b3, b4, m, r, c, t_b1, t_b2, t_b3, t_b4, t_m, t_r, t_c = evaluate_from_output(model_output)\n",
        "    \n",
        "else:\n",
        "    if os.path.isfile(model_output_all_templates):\n",
        "        generated_cq_all_templates = open(model_output_all_templates, 'r').readlines()\n",
        "        generated_cq_grouped = [[generated_cq_all_templates[len(starting_texts) * k + l] \n",
        "                                    for l in range(len(starting_texts))] \n",
        "                                    for k in range(int(len(generated_cq_all_templates)/8))]\n",
        "\n",
        "        for iter, row in facet_test_data.iterrows():\n",
        "            facet = facet_test_data.at[iter, 'facet_desc']\n",
        "            ref = facet_test_data.at[iter, 'question']\n",
        "            facet_list = facet.split()\n",
        "\n",
        "            generated_cqs = []\n",
        "            for full_sentence in generated_cq_grouped[iter]:\n",
        "                query = re.sub('\\[SEP\\]', '&', full_sentence).split('&')[0].strip()\n",
        "                generated_follow_up = re.sub('\\[SEP\\]', '&', full_sentence).split('&')[1].strip()\n",
        "                generated_cq = re.sub('[.?]', '&', generated_follow_up).split('&')[0].strip()\n",
        "                generated_cqs.append(generated_cq)\n",
        "            \n",
        "            noun_in_query = [token.text for token in pos_tagger(query) if token.pos_ == 'NOUN']\n",
        "            propn_in_query = [token.text.lower() for token in pos_tagger(query) if token.pos_ == 'PROPN']\n",
        "\n",
        "            template_scores = calculate_WSDM(query=' '.join(noun_in_query+propn_in_query+facet_list), doc_list=generated_cqs)\n",
        "            sorted_template_scores = sorted(template_scores.items(), key = lambda x: x[1], reverse=True)\n",
        "            generated_cq = sorted(template_scores.keys(), key = lambda x: template_scores[x], reverse=True)[0] \n",
        "            facet_test_data.at[iter, 'generated'] = generated_cq\n",
        "            \n",
        "            if iter % sample_every == 0: \n",
        "                print(iter, query, \"-\", facet, '-', generated_cq)\n",
        "                pprint.pprint(sorted_template_scores)\n",
        "\n",
        "            # full reference evaluation\n",
        "            hyp_b1, hyp_b2, hyp_b3, hyp_b4, hyp_m, hyp_r, hyp_c = auto_evaluation(ref, generated_cq, facet)\n",
        "\n",
        "            b1.append(hyp_b1)\n",
        "            b2.append(hyp_b2)\n",
        "            b3.append(hyp_b3)\n",
        "            b4.append(hyp_b4)\n",
        "            m.append(hyp_m)\n",
        "            r.append(hyp_r)\n",
        "            c.append(hyp_c)\n",
        "\n",
        "            # question body evaluation\n",
        "            truncate_ref = ' '.join(ref.split()[template_len:])\n",
        "            truncate_generated_cq = ' '.join(generated_cq.split()[template_len:])\n",
        "            \n",
        "            t_hyp_b1, t_hyp_b2, t_hyp_b3, t_hyp_b4, t_hyp_m, t_hyp_r, t_hyp_c = auto_evaluation(truncate_ref, truncate_generated_cq, facet)\n",
        "\n",
        "            t_b1.append(t_hyp_b1)\n",
        "            t_b2.append(t_hyp_b2)\n",
        "            t_b3.append(t_hyp_b3)\n",
        "            t_b4.append(t_hyp_b4)\n",
        "            t_m.append(t_hyp_m)\n",
        "            t_r.append(t_hyp_r)\n",
        "            t_c.append(t_hyp_c)\n",
        "\n",
        "    else:\n",
        "        for iter, row in facet_test_data.iterrows():\n",
        "            query = facet_test_data.at[iter, 'initial_request']\n",
        "            facet = facet_test_data.at[iter, 'facet_desc']\n",
        "            ref = facet_test_data.at[iter, 'question']\n",
        "            facet_list = facet.split()\n",
        "\n",
        "            generated_cqs = []\n",
        "            for s_t in starting_texts:\n",
        "                s_t = re.sub('\\[SEP\\]', ' ', s_t).strip()\n",
        "                prompt = ' '.join(gpt3_examples[:use_examples]) + ' ' + query + ' ' + \"Ask a question that contains words in the list\" + ' ' + \"[\" + \", \".join([\"'\"+f+\"'\" for f in facet.split()])  + '].' + s_t\n",
        "                response = openai.Completion.create(\n",
        "                    model=\"text-davinci-002\",\n",
        "                    prompt= prompt,\n",
        "                    temperature=temperature,\n",
        "                    max_tokens=32,\n",
        "                    top_p=1,\n",
        "                    frequency_penalty=0.0,\n",
        "                    presence_penalty=0.0,\n",
        "                    stop=[\"\\n\"]\n",
        "                )\n",
        "\n",
        "                generated_cq = s_t + response['choices'][0]['text']\n",
        "                generated_cq = re.sub('\\[SEP\\]', ' ', generated_cq).strip()\n",
        "                generated_cq = re.sub('[.?]', '&', generated_cq).split('&')[0].strip()\n",
        "                generated_cqs.append(generated_cq)\n",
        "                all_generations.append(generated_cq)\n",
        "\n",
        "            noun_in_query = [token.text for token in pos_tagger(query) if token.pos_ == 'NOUN']\n",
        "            propn_in_query = [token.text.lower() for token in pos_tagger(query) if token.pos_ == 'PROPN']\n",
        "\n",
        "            template_scores = calculate_WSDM(query=' '.join(noun_in_query+propn_in_query+facet_list), doc_list=generated_cqs)\n",
        "            sorted_template_scores = sorted(template_scores.items(), key = lambda x: x[1], reverse=True)\n",
        "            generated_cq = sorted(template_scores.keys(), key = lambda x: template_scores[x], reverse=True)[0] \n",
        "            facet_test_data.at[iter, 'generated'] = generated_cq\n",
        "\n",
        "            if iter % sample_every == 0: \n",
        "                print(iter, query, \"-\", facet_list, '-', ' '.join(tokenized_hyp))\n",
        "                pprint.pprint(sorted_template_scores)\n",
        "        \n",
        "            # full reference evaluation\n",
        "            hyp_b1, hyp_b2, hyp_b3, hyp_b4, hyp_m, hyp_r, hyp_c = auto_evaluation(ref, generated_cq, facet)\n",
        "\n",
        "            b1.append(hyp_b1)\n",
        "            b2.append(hyp_b2)\n",
        "            b3.append(hyp_b3)\n",
        "            b4.append(hyp_b4)\n",
        "            m.append(hyp_m)\n",
        "            r.append(hyp_r)\n",
        "            c.append(hyp_c)\n",
        "\n",
        "            # question body evaluation\n",
        "            truncate_ref = ' '.join(ref.split()[template_len:])\n",
        "            truncate_generated_cq = ' '.join(generated_cq.split()[template_len:])\n",
        "            \n",
        "            t_hyp_b1, t_hyp_b2, t_hyp_b3, t_hyp_b4, t_hyp_m, t_hyp_r, t_hyp_c = auto_evaluation(truncate_ref, truncate_generated_cq, facet)\n",
        "\n",
        "            t_b1.append(t_hyp_b1)\n",
        "            t_b2.append(t_hyp_b2)\n",
        "            t_b3.append(t_hyp_b3)\n",
        "            t_b4.append(t_hyp_b4)\n",
        "            t_m.append(t_hyp_m)\n",
        "            t_r.append(t_hyp_r)\n",
        "            t_c.append(t_hyp_c)\n",
        "\n",
        "    output_df = facet_test_data[['initial_request', 'facet_desc', 'question', 'generated']]\n",
        "    output_df.columns = ['query', 'facet', 'reference', 'candidate']\n",
        "    output_df.to_csv(model_output)\n",
        "\n",
        "with open(model_output_all_templates, 'w') as outputfile:\n",
        "    for generation in all_generations:\n",
        "        outputfile.write(generation)\n",
        "        outputfile.write('\\n')\n",
        "\n",
        "# full reference results\n",
        "print(\"================================================================\")\n",
        "print(\"Full reference evaluation\")\n",
        "print(\"================================================================\")\n",
        "print(\"b1\", np.mean(b1), \"b2\", np.mean(b2), \"b3\", np.mean(b3), \"b4\", np.mean(b4))\n",
        "print(\"rouge-L\", np.mean(r))\n",
        "print(\"m\", np.mean(m))\n",
        "print(\"c\", np.mean(c))\n",
        "\n",
        "gpt3_0_b1 = np.mean(b1)\n",
        "gpt3_0_b2 = np.mean(b2)\n",
        "gpt3_0_b3 = np.mean(b3)\n",
        "gpt3_0_b4 = np.mean(b4)\n",
        "gpt3_0_m = np.mean(m)\n",
        "gpt3_0_r = np.mean(r)\n",
        "gpt3_0_c = np.mean(c)\n",
        "\n",
        "# question body results\n",
        "print(\"================================================================\")\n",
        "print(\"Question body evaluation\")\n",
        "print(\"================================================================\")\n",
        "print(\"b1\", np.mean(t_b1), \"b2\", np.mean(t_b2), \"b3\", np.mean(t_b3), \"b4\", np.mean(t_b4))\n",
        "print(\"rouge-L\", np.mean(t_r))\n",
        "print(\"m\", np.mean(t_m))\n",
        "print(\"c\", np.mean(t_c))\n",
        "\n",
        "t_gpt3_0_b1 = np.mean(t_b1)\n",
        "t_gpt3_0_b2 = np.mean(t_b2)\n",
        "t_gpt3_0_b3 = np.mean(t_b3)\n",
        "t_gpt3_0_b4 = np.mean(t_b4)\n",
        "t_gpt3_0_m = np.mean(t_m)\n",
        "t_gpt3_0_r = np.mean(t_r)\n",
        "t_gpt3_0_c = np.mean(t_c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 5.2 One-shot GPT3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 tell me about cass county missouri - list homes sale - do you want information about a specific home or a list of homes for sale\n",
            "100 Find information on ontario california airport. - directions location - do you want information on the ontario california airport location or directions to the ontario california airport\n",
            "200 Where can I buy pressure washers? - washer - do you need to buy a pressure washer\n",
            "300 Tell me more about Rocky Mountain News - recent events historical - are you looking for recent events or historical events\n",
            "400 Where should I order dog clean-up bags - specif bag type - do you want to know about a specific bag or type of bag\n",
            "================================================================\n",
            "Full reference evaluation\n",
            "================================================================\n",
            "b1 0.45450442193977114 b2 0.23043683433896323 b3 0.1453323074712589 b4 0.10288596669634774\n",
            "rouge-L 0.48206610468118216\n",
            "m 0.46551388301551\n",
            "c 0.9543529411764706\n",
            "================================================================\n",
            "Question body evaluation\n",
            "================================================================\n",
            "b1 0.4253424715913936 b2 0.23735638076510057 b3 0.15818386619872948 b4 0.11904117520780316\n",
            "rouge-L 0.47590761996212216\n",
            "m 0.45983232302338706\n",
            "c 0.9382745098039216\n"
          ]
        }
      ],
      "source": [
        "facet_test_file = 'data/clariq_f/ClariQ-FKw-dev.tsv'\n",
        "facet_test_data = pd.read_csv(facet_test_file, sep='\\t')\n",
        "_, facet_test_data = process_clariq_f(facet_test_data)\n",
        "\n",
        "b1, b2, b3, b4 = [], [], [], []\n",
        "m = []\n",
        "r = []\n",
        "c = []\n",
        "\n",
        "t_b1, t_b2, t_b3, t_b4 = [], [], [], []\n",
        "t_m = []\n",
        "t_r = []\n",
        "t_c = []\n",
        "\n",
        "temperature = 0\n",
        "use_examples = 1\n",
        "model_output = 'fewshot_gpt3' + '_examples' + str(use_examples) + '_temp' + str(temperature) + '.csv'\n",
        "model_output_all_templates = 'fewshot_gpt3' + '_examples' + str(use_examples) + '_temp' + str(temperature) +'_all'\n",
        "\n",
        "all_generations = [] # cache the generations to save time and load from calling gpt3\n",
        "\n",
        "if os.path.isfile(model_output):\n",
        "    b1, b2, b3, b4, m, r, c, t_b1, t_b2, t_b3, t_b4, t_m, t_r, t_c = evaluate_from_output(model_output)\n",
        "    \n",
        "else:\n",
        "    if os.path.isfile(model_output_all_templates):\n",
        "        generated_cq_all_templates = open(model_output_all_templates, 'r').readlines()\n",
        "        generated_cq_grouped = [[generated_cq_all_templates[len(starting_texts) * k + l] \n",
        "                                    for l in range(len(starting_texts))] \n",
        "                                    for k in range(int(len(generated_cq_all_templates)/8))]\n",
        "\n",
        "        for iter, row in facet_test_data.iterrows():\n",
        "            facet = facet_test_data.at[iter, 'facet_desc']\n",
        "            ref = facet_test_data.at[iter, 'question']\n",
        "            facet_list = facet.split()\n",
        "\n",
        "            generated_cqs = []\n",
        "            for full_sentence in generated_cq_grouped[iter]:\n",
        "                query = re.sub('\\[SEP\\]', '&', full_sentence).split('&')[0].strip()\n",
        "                generated_follow_up = re.sub('\\[SEP\\]', '&', full_sentence).split('&')[1].strip()\n",
        "                generated_cq = re.sub('[.?]', '&', generated_follow_up).split('&')[0].strip()\n",
        "                generated_cqs.append(generated_cq)\n",
        "            \n",
        "            noun_in_query = [token.text for token in pos_tagger(query) if token.pos_ == 'NOUN']\n",
        "            propn_in_query = [token.text.lower() for token in pos_tagger(query) if token.pos_ == 'PROPN']\n",
        "\n",
        "            template_scores = calculate_WSDM(query=' '.join(noun_in_query+propn_in_query+facet_list), doc_list=generated_cqs)\n",
        "            sorted_template_scores = sorted(template_scores.items(), key = lambda x: x[1], reverse=True)\n",
        "            generated_cq = sorted(template_scores.keys(), key = lambda x: template_scores[x], reverse=True)[0] \n",
        "            facet_test_data.at[iter, 'generated'] = generated_cq\n",
        "            \n",
        "            if iter % sample_every == 0: \n",
        "                print(iter, query, \"-\", facet, '-', generated_cq)\n",
        "                pprint.pprint(sorted_template_scores)\n",
        "\n",
        "            # full reference evaluation\n",
        "            hyp_b1, hyp_b2, hyp_b3, hyp_b4, hyp_m, hyp_r, hyp_c = auto_evaluation(ref, generated_cq, facet)\n",
        "\n",
        "            b1.append(hyp_b1)\n",
        "            b2.append(hyp_b2)\n",
        "            b3.append(hyp_b3)\n",
        "            b4.append(hyp_b4)\n",
        "            m.append(hyp_m)\n",
        "            r.append(hyp_r)\n",
        "            c.append(hyp_c)\n",
        "\n",
        "            # question body evaluation\n",
        "            truncate_ref = ' '.join(ref.split()[template_len:])\n",
        "            truncate_generated_cq = ' '.join(generated_cq.split()[template_len:])\n",
        "            \n",
        "            t_hyp_b1, t_hyp_b2, t_hyp_b3, t_hyp_b4, t_hyp_m, t_hyp_r, t_hyp_c = auto_evaluation(truncate_ref, truncate_generated_cq, facet)\n",
        "\n",
        "            t_b1.append(t_hyp_b1)\n",
        "            t_b2.append(t_hyp_b2)\n",
        "            t_b3.append(t_hyp_b3)\n",
        "            t_b4.append(t_hyp_b4)\n",
        "            t_m.append(t_hyp_m)\n",
        "            t_r.append(t_hyp_r)\n",
        "            t_c.append(t_hyp_c)\n",
        "\n",
        "    else:\n",
        "        for iter, row in facet_test_data.iterrows():\n",
        "            query = facet_test_data.at[iter, 'initial_request']\n",
        "            facet = facet_test_data.at[iter, 'facet_desc']\n",
        "            ref = facet_test_data.at[iter, 'question']\n",
        "            facet_list = facet.split()\n",
        "\n",
        "            generated_cqs = []\n",
        "            for s_t in starting_texts:\n",
        "                s_t = re.sub('\\[SEP\\]', ' ', s_t).strip()\n",
        "                prompt = ' '.join(gpt3_examples[:use_examples]) + ' ' + query + ' ' + \"Ask a question that contains words in the list\" + ' ' + \"[\" + \", \".join([\"'\"+f+\"'\" for f in facet.split()])  + '].' + s_t\n",
        "                response = openai.Completion.create(\n",
        "                    model=\"text-davinci-002\",\n",
        "                    prompt= prompt,\n",
        "                    temperature=temperature,\n",
        "                    max_tokens=32,\n",
        "                    top_p=1,\n",
        "                    frequency_penalty=0.0,\n",
        "                    presence_penalty=0.0,\n",
        "                    stop=[\"\\n\"]\n",
        "                )\n",
        "\n",
        "                generated_cq = s_t + response['choices'][0]['text']\n",
        "                generated_cq = re.sub('\\[SEP\\]', ' ', generated_cq).strip()\n",
        "                generated_cq = re.sub('[.?]', '&', generated_cq).split('&')[0].strip()\n",
        "                generated_cqs.append(generated_cq)\n",
        "                all_generations.append(generated_cq)\n",
        "\n",
        "            noun_in_query = [token.text for token in pos_tagger(query) if token.pos_ == 'NOUN']\n",
        "            propn_in_query = [token.text.lower() for token in pos_tagger(query) if token.pos_ == 'PROPN']\n",
        "\n",
        "            template_scores = calculate_WSDM(query=' '.join(noun_in_query+propn_in_query+facet_list), doc_list=generated_cqs)\n",
        "            sorted_template_scores = sorted(template_scores.items(), key = lambda x: x[1], reverse=True)\n",
        "            generated_cq = sorted(template_scores.keys(), key = lambda x: template_scores[x], reverse=True)[0] \n",
        "            facet_test_data.at[iter, 'generated'] = generated_cq\n",
        "\n",
        "            if iter % sample_every == 0: \n",
        "                print(iter, query, \"-\", facet_list, '-', ' '.join(tokenized_hyp))\n",
        "                pprint.pprint(sorted_template_scores)\n",
        "        \n",
        "            # full reference evaluation\n",
        "            hyp_b1, hyp_b2, hyp_b3, hyp_b4, hyp_m, hyp_r, hyp_c = auto_evaluation(ref, generated_cq, facet)\n",
        "\n",
        "            b1.append(hyp_b1)\n",
        "            b2.append(hyp_b2)\n",
        "            b3.append(hyp_b3)\n",
        "            b4.append(hyp_b4)\n",
        "            m.append(hyp_m)\n",
        "            r.append(hyp_r)\n",
        "            c.append(hyp_c)\n",
        "\n",
        "            # question body evaluation\n",
        "            truncate_ref = ' '.join(ref.split()[template_len:])\n",
        "            truncate_generated_cq = ' '.join(generated_cq.split()[template_len:])\n",
        "            \n",
        "            t_hyp_b1, t_hyp_b2, t_hyp_b3, t_hyp_b4, t_hyp_m, t_hyp_r, t_hyp_c = auto_evaluation(truncate_ref, truncate_generated_cq, facet)\n",
        "\n",
        "            t_b1.append(t_hyp_b1)\n",
        "            t_b2.append(t_hyp_b2)\n",
        "            t_b3.append(t_hyp_b3)\n",
        "            t_b4.append(t_hyp_b4)\n",
        "            t_m.append(t_hyp_m)\n",
        "            t_r.append(t_hyp_r)\n",
        "            t_c.append(t_hyp_c)\n",
        "\n",
        "    output_df = facet_test_data[['initial_request', 'facet_desc', 'question', 'generated']]\n",
        "    output_df.columns = ['query', 'facet', 'reference', 'candidate']\n",
        "    output_df.to_csv(model_output)\n",
        "\n",
        "with open(model_output_all_templates, 'w') as outputfile:\n",
        "    for generation in all_generations:\n",
        "        outputfile.write(generation)\n",
        "        outputfile.write('\\n')\n",
        "\n",
        "# full reference results\n",
        "print(\"================================================================\")\n",
        "print(\"Full reference evaluation\")\n",
        "print(\"================================================================\")\n",
        "print(\"b1\", np.mean(b1), \"b2\", np.mean(b2), \"b3\", np.mean(b3), \"b4\", np.mean(b4))\n",
        "print(\"rouge-L\", np.mean(r))\n",
        "print(\"m\", np.mean(m))\n",
        "print(\"c\", np.mean(c))\n",
        "\n",
        "gpt3_1_b1 = np.mean(b1)\n",
        "gpt3_1_b2 = np.mean(b2)\n",
        "gpt3_1_b3 = np.mean(b3)\n",
        "gpt3_1_b4 = np.mean(b4)\n",
        "gpt3_1_m = np.mean(m)\n",
        "gpt3_1_r = np.mean(r)\n",
        "gpt3_1_c = np.mean(c)\n",
        "\n",
        "# question body results\n",
        "print(\"================================================================\")\n",
        "print(\"Question body evaluation\")\n",
        "print(\"================================================================\")\n",
        "print(\"b1\", np.mean(t_b1), \"b2\", np.mean(t_b2), \"b3\", np.mean(t_b3), \"b4\", np.mean(t_b4))\n",
        "print(\"rouge-L\", np.mean(t_r))\n",
        "print(\"m\", np.mean(t_m))\n",
        "print(\"c\", np.mean(t_c))\n",
        "\n",
        "t_gpt3_1_b1 = np.mean(t_b1)\n",
        "t_gpt3_1_b2 = np.mean(t_b2)\n",
        "t_gpt3_1_b3 = np.mean(t_b3)\n",
        "t_gpt3_1_b4 = np.mean(t_b4)\n",
        "t_gpt3_1_m = np.mean(t_m)\n",
        "t_gpt3_1_r = np.mean(t_r)\n",
        "t_gpt3_1_c = np.mean(t_c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 5.3 Two-shot GPT3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 tell me about cass county missouri - list homes sale - are you looking for a list of homes for sale in cass county missouri\n",
            "100 Find information on ontario california airport. - directions location - do you want information on how to get to the ontario california airport or information about its location\n",
            "200 Where can I buy pressure washers? - washer - are you interested in purchasing a pressure washer\n",
            "300 Tell me more about Rocky Mountain News - recent events historical - are you looking for recent events or historical events\n",
            "400 Where should I order dog clean-up bags - specif bag type - are you interested in a specific type of bag or just any bag that will work for dog clean-up\n",
            "================================================================\n",
            "Full reference evaluation\n",
            "================================================================\n",
            "b1 0.42323949261930216 b2 0.21974406668664828 b3 0.13476144197480297 b4 0.09177524838662333\n",
            "rouge-L 0.4577721705677639\n",
            "m 0.5238679886110711\n",
            "c 0.9356470588235294\n",
            "================================================================\n",
            "Question body evaluation\n",
            "================================================================\n",
            "b1 0.3894379076796257 b2 0.22635225534200076 b3 0.14553205532907226 b4 0.10370774309698337\n",
            "rouge-L 0.4362571834961392\n",
            "m 0.550147025333538\n",
            "c 0.9191764705882353\n"
          ]
        }
      ],
      "source": [
        "facet_test_file = 'data/clariq_f/ClariQ-FKw-dev.tsv'\n",
        "facet_test_data = pd.read_csv(facet_test_file, sep='\\t')\n",
        "_, facet_test_data = process_clariq_f(facet_test_data)\n",
        "\n",
        "b1, b2, b3, b4 = [], [], [], []\n",
        "m = []\n",
        "r = []\n",
        "c = []\n",
        "\n",
        "t_b1, t_b2, t_b3, t_b4 = [], [], [], []\n",
        "t_m = []\n",
        "t_r = []\n",
        "t_c = []\n",
        "\n",
        "temperature = 0\n",
        "use_examples = 2\n",
        "model_output = 'fewshot_gpt3' + '_examples' + str(use_examples) + '_temp' + str(temperature) + '.csv'\n",
        "model_output_all_templates = 'fewshot_gpt3' + '_examples' + str(use_examples) + '_temp' + str(temperature) +'_all'\n",
        "\n",
        "all_generations = [] # cache the generations to save time and load from calling gpt3\n",
        "\n",
        "if os.path.isfile(model_output):\n",
        "    b1, b2, b3, b4, m, r, c, t_b1, t_b2, t_b3, t_b4, t_m, t_r, t_c = evaluate_from_output(model_output)\n",
        "    \n",
        "else:\n",
        "    if os.path.isfile(model_output_all_templates):\n",
        "        generated_cq_all_templates = open(model_output_all_templates, 'r').readlines()\n",
        "        generated_cq_grouped = [[generated_cq_all_templates[len(starting_texts) * k + l] \n",
        "                                    for l in range(len(starting_texts))] \n",
        "                                    for k in range(int(len(generated_cq_all_templates)/8))]\n",
        "\n",
        "        for iter, row in facet_test_data.iterrows():\n",
        "            facet = facet_test_data.at[iter, 'facet_desc']\n",
        "            ref = facet_test_data.at[iter, 'question']\n",
        "            facet_list = facet.split()\n",
        "\n",
        "            generated_cqs = []\n",
        "            for full_sentence in generated_cq_grouped[iter]:\n",
        "                query = re.sub('\\[SEP\\]', '&', full_sentence).split('&')[0].strip()\n",
        "                generated_follow_up = re.sub('\\[SEP\\]', '&', full_sentence).split('&')[1].strip()\n",
        "                generated_cq = re.sub('[.?]', '&', generated_follow_up).split('&')[0].strip()\n",
        "                generated_cqs.append(generated_cq)\n",
        "            \n",
        "            noun_in_query = [token.text for token in pos_tagger(query) if token.pos_ == 'NOUN']\n",
        "            propn_in_query = [token.text.lower() for token in pos_tagger(query) if token.pos_ == 'PROPN']\n",
        "\n",
        "            template_scores = calculate_WSDM(query=' '.join(noun_in_query+propn_in_query+facet_list), doc_list=generated_cqs)\n",
        "            sorted_template_scores = sorted(template_scores.items(), key = lambda x: x[1], reverse=True)\n",
        "            generated_cq = sorted(template_scores.keys(), key = lambda x: template_scores[x], reverse=True)[0] \n",
        "            facet_test_data.at[iter, 'generated'] = generated_cq\n",
        "            \n",
        "            if iter % sample_every == 0: \n",
        "                print(iter, query, \"-\", facet, '-', generated_cq)\n",
        "                pprint.pprint(sorted_template_scores)\n",
        "\n",
        "            # full reference evaluation\n",
        "            hyp_b1, hyp_b2, hyp_b3, hyp_b4, hyp_m, hyp_r, hyp_c = auto_evaluation(ref, generated_cq, facet)\n",
        "\n",
        "            b1.append(hyp_b1)\n",
        "            b2.append(hyp_b2)\n",
        "            b3.append(hyp_b3)\n",
        "            b4.append(hyp_b4)\n",
        "            m.append(hyp_m)\n",
        "            r.append(hyp_r)\n",
        "            c.append(hyp_c)\n",
        "\n",
        "            # question body evaluation\n",
        "            truncate_ref = ' '.join(ref.split()[template_len:])\n",
        "            truncate_generated_cq = ' '.join(generated_cq.split()[template_len:])\n",
        "            \n",
        "            t_hyp_b1, t_hyp_b2, t_hyp_b3, t_hyp_b4, t_hyp_m, t_hyp_r, t_hyp_c = auto_evaluation(truncate_ref, truncate_generated_cq, facet)\n",
        "\n",
        "            t_b1.append(t_hyp_b1)\n",
        "            t_b2.append(t_hyp_b2)\n",
        "            t_b3.append(t_hyp_b3)\n",
        "            t_b4.append(t_hyp_b4)\n",
        "            t_m.append(t_hyp_m)\n",
        "            t_r.append(t_hyp_r)\n",
        "            t_c.append(t_hyp_c)\n",
        "\n",
        "    else:\n",
        "        for iter, row in facet_test_data.iterrows():\n",
        "            query = facet_test_data.at[iter, 'initial_request']\n",
        "            facet = facet_test_data.at[iter, 'facet_desc']\n",
        "            ref = facet_test_data.at[iter, 'question']\n",
        "            facet_list = facet.split()\n",
        "\n",
        "            generated_cqs = []\n",
        "            for s_t in starting_texts:\n",
        "                s_t = re.sub('\\[SEP\\]', ' ', s_t).strip()\n",
        "                prompt = ' '.join(gpt3_examples[:use_examples]) + ' ' + query + ' ' + \"Ask a question that contains words in the list\" + ' ' + \"[\" + \", \".join([\"'\"+f+\"'\" for f in facet.split()])  + '].' + s_t\n",
        "                response = openai.Completion.create(\n",
        "                    model=\"text-davinci-002\",\n",
        "                    prompt= prompt,\n",
        "                    temperature=temperature,\n",
        "                    max_tokens=32,\n",
        "                    top_p=1,\n",
        "                    frequency_penalty=0.0,\n",
        "                    presence_penalty=0.0,\n",
        "                    stop=[\"\\n\"]\n",
        "                )\n",
        "\n",
        "                generated_cq = s_t + response['choices'][0]['text']\n",
        "                generated_cq = re.sub('\\[SEP\\]', ' ', generated_cq).strip()\n",
        "                generated_cq = re.sub('[.?]', '&', generated_cq).split('&')[0].strip()\n",
        "                generated_cqs.append(generated_cq)\n",
        "                all_generations.append(generated_cq)\n",
        "\n",
        "            noun_in_query = [token.text for token in pos_tagger(query) if token.pos_ == 'NOUN']\n",
        "            propn_in_query = [token.text.lower() for token in pos_tagger(query) if token.pos_ == 'PROPN']\n",
        "\n",
        "            template_scores = calculate_WSDM(query=' '.join(noun_in_query+propn_in_query+facet_list), doc_list=generated_cqs)\n",
        "            sorted_template_scores = sorted(template_scores.items(), key = lambda x: x[1], reverse=True)\n",
        "            generated_cq = sorted(template_scores.keys(), key = lambda x: template_scores[x], reverse=True)[0] \n",
        "            facet_test_data.at[iter, 'generated'] = generated_cq\n",
        "\n",
        "            if iter % sample_every == 0: \n",
        "                print(iter, query, \"-\", facet_list, '-', ' '.join(tokenized_hyp))\n",
        "                pprint.pprint(sorted_template_scores)\n",
        "        \n",
        "            # full reference evaluation\n",
        "            hyp_b1, hyp_b2, hyp_b3, hyp_b4, hyp_m, hyp_r, hyp_c = auto_evaluation(ref, generated_cq, facet)\n",
        "\n",
        "            b1.append(hyp_b1)\n",
        "            b2.append(hyp_b2)\n",
        "            b3.append(hyp_b3)\n",
        "            b4.append(hyp_b4)\n",
        "            m.append(hyp_m)\n",
        "            r.append(hyp_r)\n",
        "            c.append(hyp_c)\n",
        "\n",
        "            # question body evaluation\n",
        "            truncate_ref = ' '.join(ref.split()[template_len:])\n",
        "            truncate_generated_cq = ' '.join(generated_cq.split()[template_len:])\n",
        "            \n",
        "            t_hyp_b1, t_hyp_b2, t_hyp_b3, t_hyp_b4, t_hyp_m, t_hyp_r, t_hyp_c = auto_evaluation(truncate_ref, truncate_generated_cq, facet)\n",
        "\n",
        "            t_b1.append(t_hyp_b1)\n",
        "            t_b2.append(t_hyp_b2)\n",
        "            t_b3.append(t_hyp_b3)\n",
        "            t_b4.append(t_hyp_b4)\n",
        "            t_m.append(t_hyp_m)\n",
        "            t_r.append(t_hyp_r)\n",
        "            t_c.append(t_hyp_c)\n",
        "\n",
        "    output_df = facet_test_data[['initial_request', 'facet_desc', 'question', 'generated']]\n",
        "    output_df.columns = ['query', 'facet', 'reference', 'candidate']\n",
        "    output_df.to_csv(model_output)\n",
        "\n",
        "with open(model_output_all_templates, 'w') as outputfile:\n",
        "    for generation in all_generations:\n",
        "        outputfile.write(generation)\n",
        "        outputfile.write('\\n')\n",
        "\n",
        "# full reference results\n",
        "print(\"================================================================\")\n",
        "print(\"Full reference evaluation\")\n",
        "print(\"================================================================\")\n",
        "print(\"b1\", np.mean(b1), \"b2\", np.mean(b2), \"b3\", np.mean(b3), \"b4\", np.mean(b4))\n",
        "print(\"rouge-L\", np.mean(r))\n",
        "print(\"m\", np.mean(m))\n",
        "print(\"c\", np.mean(c))\n",
        "\n",
        "gpt3_2_b1 = np.mean(b1)\n",
        "gpt3_2_b2 = np.mean(b2)\n",
        "gpt3_2_b3 = np.mean(b3)\n",
        "gpt3_2_b4 = np.mean(b4)\n",
        "gpt3_2_m = np.mean(m)\n",
        "gpt3_2_r = np.mean(r)\n",
        "gpt3_2_c = np.mean(c)\n",
        "\n",
        "# question body results\n",
        "print(\"================================================================\")\n",
        "print(\"Question body evaluation\")\n",
        "print(\"================================================================\")\n",
        "print(\"b1\", np.mean(t_b1), \"b2\", np.mean(t_b2), \"b3\", np.mean(t_b3), \"b4\", np.mean(t_b4))\n",
        "print(\"rouge-L\", np.mean(t_r))\n",
        "print(\"m\", np.mean(t_m))\n",
        "print(\"c\", np.mean(t_c))\n",
        "\n",
        "t_gpt3_2_b1 = np.mean(t_b1)\n",
        "t_gpt3_2_b2 = np.mean(t_b2)\n",
        "t_gpt3_2_b3 = np.mean(t_b3)\n",
        "t_gpt3_2_b4 = np.mean(t_b4)\n",
        "t_gpt3_2_m = np.mean(t_m)\n",
        "t_gpt3_2_r = np.mean(t_r)\n",
        "t_gpt3_2_c = np.mean(t_c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 5.4 Three-shot GPT3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 tell me about cass county missouri - list homes sale - are you looking for a list of homes for sale in cass county missouri\n",
            "100 Find information on ontario california airport. - directions location - do you want information on how to get to the ontario california airport or information about its location\n",
            "200 Where can I buy pressure washers? - washer - are you interested in purchasing a pressure washer\n",
            "300 Tell me more about Rocky Mountain News - recent events historical - are you looking for recent events or historical events\n",
            "400 Where should I order dog clean-up bags - specif bag type - are you interested in a specific type of bag or just any bag that will work for dog clean-up\n",
            "================================================================\n",
            "Full reference evaluation\n",
            "================================================================\n",
            "b1 0.42387925535792526 b2 0.2209577594993678 b3 0.13653434343394935 b4 0.09335854470351058\n",
            "rouge-L 0.4580263700523444\n",
            "m 0.5252491996168146\n",
            "c 0.9352549019607843\n",
            "================================================================\n",
            "Question body evaluation\n",
            "================================================================\n",
            "b1 0.3899627493086948 b2 0.22707399306205175 b3 0.1470466676760886 b4 0.10557090206974408\n",
            "rouge-L 0.4363464708990771\n",
            "m 0.5505260866263759\n",
            "c 0.9187843137254902\n"
          ]
        }
      ],
      "source": [
        "facet_test_file = 'data/clariq_f/ClariQ-FKw-dev.tsv'\n",
        "facet_test_data = pd.read_csv(facet_test_file, sep='\\t')\n",
        "_, facet_test_data = process_clariq_f(facet_test_data)\n",
        "\n",
        "b1, b2, b3, b4 = [], [], [], []\n",
        "m = []\n",
        "r = []\n",
        "c = []\n",
        "\n",
        "t_b1, t_b2, t_b3, t_b4 = [], [], [], []\n",
        "t_m = []\n",
        "t_r = []\n",
        "t_c = []\n",
        "\n",
        "temperature = 0\n",
        "use_examples = 3\n",
        "model_output = 'fewshot_gpt3' + '_examples' + str(use_examples) + '_temp' + str(temperature) + '.csv'\n",
        "model_output_all_templates = 'fewshot_gpt3' + '_examples' + str(use_examples) + '_temp' + str(temperature) +'_all'\n",
        "\n",
        "all_generations = [] # cache the generations to save time and load from calling gpt3\n",
        "\n",
        "if os.path.isfile(model_output):\n",
        "    b1, b2, b3, b4, m, r, c, t_b1, t_b2, t_b3, t_b4, t_m, t_r, t_c = evaluate_from_output(model_output)\n",
        "    \n",
        "else:\n",
        "    if os.path.isfile(model_output_all_templates):\n",
        "        generated_cq_all_templates = open(model_output_all_templates, 'r').readlines()\n",
        "        generated_cq_grouped = [[generated_cq_all_templates[len(starting_texts) * k + l] \n",
        "                                    for l in range(len(starting_texts))] \n",
        "                                    for k in range(int(len(generated_cq_all_templates)/8))]\n",
        "\n",
        "        for iter, row in facet_test_data.iterrows():\n",
        "            facet = facet_test_data.at[iter, 'facet_desc']\n",
        "            ref = facet_test_data.at[iter, 'question']\n",
        "            facet_list = facet.split()\n",
        "\n",
        "            generated_cqs = []\n",
        "            for full_sentence in generated_cq_grouped[iter]:\n",
        "                query = re.sub('\\[SEP\\]', '&', full_sentence).split('&')[0].strip()\n",
        "                generated_follow_up = re.sub('\\[SEP\\]', '&', full_sentence).split('&')[1].strip()\n",
        "                generated_cq = re.sub('[.?]', '&', generated_follow_up).split('&')[0].strip()\n",
        "                generated_cqs.append(generated_cq)\n",
        "            \n",
        "            noun_in_query = [token.text for token in pos_tagger(query) if token.pos_ == 'NOUN']\n",
        "            propn_in_query = [token.text.lower() for token in pos_tagger(query) if token.pos_ == 'PROPN']\n",
        "\n",
        "            template_scores = calculate_WSDM(query=' '.join(noun_in_query+propn_in_query+facet_list), doc_list=generated_cqs)\n",
        "            sorted_template_scores = sorted(template_scores.items(), key = lambda x: x[1], reverse=True)\n",
        "            generated_cq = sorted(template_scores.keys(), key = lambda x: template_scores[x], reverse=True)[0] \n",
        "            facet_test_data.at[iter, 'generated'] = generated_cq\n",
        "            \n",
        "            if iter % sample_every == 0: \n",
        "                print(iter, query, \"-\", facet, '-', generated_cq)\n",
        "                pprint.pprint(sorted_template_scores)\n",
        "\n",
        "            # full reference evaluation\n",
        "            hyp_b1, hyp_b2, hyp_b3, hyp_b4, hyp_m, hyp_r, hyp_c = auto_evaluation(ref, generated_cq, facet)\n",
        "\n",
        "            b1.append(hyp_b1)\n",
        "            b2.append(hyp_b2)\n",
        "            b3.append(hyp_b3)\n",
        "            b4.append(hyp_b4)\n",
        "            m.append(hyp_m)\n",
        "            r.append(hyp_r)\n",
        "            c.append(hyp_c)\n",
        "\n",
        "            # question body evaluation\n",
        "            truncate_ref = ' '.join(ref.split()[template_len:])\n",
        "            truncate_generated_cq = ' '.join(generated_cq.split()[template_len:])\n",
        "            \n",
        "            t_hyp_b1, t_hyp_b2, t_hyp_b3, t_hyp_b4, t_hyp_m, t_hyp_r, t_hyp_c = auto_evaluation(truncate_ref, truncate_generated_cq, facet)\n",
        "\n",
        "            t_b1.append(t_hyp_b1)\n",
        "            t_b2.append(t_hyp_b2)\n",
        "            t_b3.append(t_hyp_b3)\n",
        "            t_b4.append(t_hyp_b4)\n",
        "            t_m.append(t_hyp_m)\n",
        "            t_r.append(t_hyp_r)\n",
        "            t_c.append(t_hyp_c)\n",
        "\n",
        "    else:\n",
        "        for iter, row in facet_test_data.iterrows():\n",
        "            query = facet_test_data.at[iter, 'initial_request']\n",
        "            facet = facet_test_data.at[iter, 'facet_desc']\n",
        "            ref = facet_test_data.at[iter, 'question']\n",
        "            facet_list = facet.split()\n",
        "\n",
        "            generated_cqs = []\n",
        "            for s_t in starting_texts:\n",
        "                s_t = re.sub('\\[SEP\\]', ' ', s_t).strip()\n",
        "                prompt = ' '.join(gpt3_examples[:use_examples]) + ' ' + query + ' ' + \"Ask a question that contains words in the list\" + ' ' + \"[\" + \", \".join([\"'\"+f+\"'\" for f in facet.split()])  + '].' + s_t\n",
        "                response = openai.Completion.create(\n",
        "                    model=\"text-davinci-002\",\n",
        "                    prompt= prompt,\n",
        "                    temperature=temperature,\n",
        "                    max_tokens=32,\n",
        "                    top_p=1,\n",
        "                    frequency_penalty=0.0,\n",
        "                    presence_penalty=0.0,\n",
        "                    stop=[\"\\n\"]\n",
        "                )\n",
        "\n",
        "                generated_cq = s_t + response['choices'][0]['text']\n",
        "                generated_cq = re.sub('\\[SEP\\]', ' ', generated_cq).strip()\n",
        "                generated_cq = re.sub('[.?]', '&', generated_cq).split('&')[0].strip()\n",
        "                generated_cqs.append(generated_cq)\n",
        "                all_generations.append(generated_cq)\n",
        "\n",
        "            noun_in_query = [token.text for token in pos_tagger(query) if token.pos_ == 'NOUN']\n",
        "            propn_in_query = [token.text.lower() for token in pos_tagger(query) if token.pos_ == 'PROPN']\n",
        "\n",
        "            template_scores = calculate_WSDM(query=' '.join(noun_in_query+propn_in_query+facet_list), doc_list=generated_cqs)\n",
        "            sorted_template_scores = sorted(template_scores.items(), key = lambda x: x[1], reverse=True)\n",
        "            generated_cq = sorted(template_scores.keys(), key = lambda x: template_scores[x], reverse=True)[0] \n",
        "            facet_test_data.at[iter, 'generated'] = generated_cq\n",
        "\n",
        "            if iter % sample_every == 0: \n",
        "                print(iter, query, \"-\", facet_list, '-', ' '.join(tokenized_hyp))\n",
        "                pprint.pprint(sorted_template_scores)\n",
        "        \n",
        "            # full reference evaluation\n",
        "            hyp_b1, hyp_b2, hyp_b3, hyp_b4, hyp_m, hyp_r, hyp_c = auto_evaluation(ref, generated_cq, facet)\n",
        "\n",
        "            b1.append(hyp_b1)\n",
        "            b2.append(hyp_b2)\n",
        "            b3.append(hyp_b3)\n",
        "            b4.append(hyp_b4)\n",
        "            m.append(hyp_m)\n",
        "            r.append(hyp_r)\n",
        "            c.append(hyp_c)\n",
        "\n",
        "            # question body evaluation\n",
        "            truncate_ref = ' '.join(ref.split()[template_len:])\n",
        "            truncate_generated_cq = ' '.join(generated_cq.split()[template_len:])\n",
        "            \n",
        "            t_hyp_b1, t_hyp_b2, t_hyp_b3, t_hyp_b4, t_hyp_m, t_hyp_r, t_hyp_c = auto_evaluation(truncate_ref, truncate_generated_cq, facet)\n",
        "\n",
        "            t_b1.append(t_hyp_b1)\n",
        "            t_b2.append(t_hyp_b2)\n",
        "            t_b3.append(t_hyp_b3)\n",
        "            t_b4.append(t_hyp_b4)\n",
        "            t_m.append(t_hyp_m)\n",
        "            t_r.append(t_hyp_r)\n",
        "            t_c.append(t_hyp_c)\n",
        "\n",
        "    output_df = facet_test_data[['initial_request', 'facet_desc', 'question', 'generated']]\n",
        "    output_df.columns = ['query', 'facet', 'reference', 'candidate']\n",
        "    output_df.to_csv(model_output)\n",
        "\n",
        "with open(model_output_all_templates, 'w') as outputfile:\n",
        "    for generation in all_generations:\n",
        "        outputfile.write(generation)\n",
        "        outputfile.write('\\n')\n",
        "\n",
        "# full reference results\n",
        "print(\"================================================================\")\n",
        "print(\"Full reference evaluation\")\n",
        "print(\"================================================================\")\n",
        "print(\"b1\", np.mean(b1), \"b2\", np.mean(b2), \"b3\", np.mean(b3), \"b4\", np.mean(b4))\n",
        "print(\"rouge-L\", np.mean(r))\n",
        "print(\"m\", np.mean(m))\n",
        "print(\"c\", np.mean(c))\n",
        "\n",
        "gpt3_3_b1 = np.mean(b1)\n",
        "gpt3_3_b2 = np.mean(b2)\n",
        "gpt3_3_b3 = np.mean(b3)\n",
        "gpt3_3_b4 = np.mean(b4)\n",
        "gpt3_3_m = np.mean(m)\n",
        "gpt3_3_r = np.mean(r)\n",
        "gpt3_3_c = np.mean(c)\n",
        "\n",
        "# question body results\n",
        "print(\"================================================================\")\n",
        "print(\"Question body evaluation\")\n",
        "print(\"================================================================\")\n",
        "print(\"b1\", np.mean(t_b1), \"b2\", np.mean(t_b2), \"b3\", np.mean(t_b3), \"b4\", np.mean(t_b4))\n",
        "print(\"rouge-L\", np.mean(t_r))\n",
        "print(\"m\", np.mean(t_m))\n",
        "print(\"c\", np.mean(t_c))\n",
        "\n",
        "t_gpt3_3_b1 = np.mean(t_b1)\n",
        "t_gpt3_3_b2 = np.mean(t_b2)\n",
        "t_gpt3_3_b3 = np.mean(t_b3)\n",
        "t_gpt3_3_b4 = np.mean(t_b4)\n",
        "t_gpt3_3_m = np.mean(t_m)\n",
        "t_gpt3_3_r = np.mean(t_r)\n",
        "t_gpt3_3_c = np.mean(t_c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 5.5 Ours\n",
        "Same as 1.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 5.6 Comparing 3.1-3.5, get the conclusion for RQ3. \"Our approach is a good replacement for GPT3.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------------------------------\n",
            "|                               Full reference evaluation                               |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|MODEL                    BLEU1    BLEU2    BLEU3    BLEU4    METEOR   ROUGE    COVERAGE|\n",
            "-----------------------------------------------------------------------------------------\n",
            "|Ours-WSDM                41.8     17.57    9.71     6.45     38.52    44.19    98.96   |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|Ours-AutoScore           43.42    20.47    12.93    9.64     41.03    47.87    98.28   |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|Zero-shot GPT3           42.96    21.45    12.98    8.76     45.95    46.69    85.27   |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|One-shot GPT3            45.45    23.04    14.53    10.29    46.55    48.21    95.44   |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|Two-shot GPT3            42.32    21.97    13.48    9.18     52.39    45.78    93.56   |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|Three-shot GPT3          42.39    22.1     13.65    9.34     52.52    45.8     93.53   |\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "|                                Question body evaluation                               |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|MODEL                    BLEU1    BLEU2    BLEU3    BLEU4    METEOR   ROUGE    COVERAGE|\n",
            "-----------------------------------------------------------------------------------------\n",
            "|Ours-WSDM                38.51    18.17    10.81    8.46     37.47    43.79    98.67   |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|Ours-AutoScore           34.88    16.25    10.47    8.07     33.88    43.42    95.83   |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|Zero-shot GPT3           39.44    22.82    15.28    11.94    46.36    46.21    82.95   |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|One-shot GPT3            42.53    23.74    15.82    11.9     45.98    47.59    93.83   |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|Two-shot GPT3            38.94    22.64    14.55    10.37    55.01    43.63    91.92   |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|Three-shot GPT3          39.0     22.71    14.7     10.56    55.05    43.63    91.88   |\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|                               Full reference evaluation                               |\")\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('MODEL', 'BLEU1','BLEU2','BLEU3','BLEU4','METEOR','ROUGE','COVERAGE'))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('Ours-WSDM', \n",
        "                                                            round_metric(zero_nd_wsdm_b1), \n",
        "                                                            round_metric(zero_nd_wsdm_b2), \n",
        "                                                            round_metric(zero_nd_wsdm_b3), \n",
        "                                                            round_metric(zero_nd_wsdm_b4), \n",
        "                                                            round_metric(zero_nd_wsdm_m), \n",
        "                                                            round_metric(zero_nd_wsdm_r), \n",
        "                                                            round_metric(zero_nd_wsdm_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('Ours-AutoScore', \n",
        "                                                            round_metric(zero_nd_auto_b1), \n",
        "                                                            round_metric(zero_nd_auto_b2), \n",
        "                                                            round_metric(zero_nd_auto_b3), \n",
        "                                                            round_metric(zero_nd_auto_b4), \n",
        "                                                            round_metric(zero_nd_auto_m), \n",
        "                                                            round_metric(zero_nd_auto_r), \n",
        "                                                            round_metric(zero_nd_auto_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('Zero-shot GPT3', \n",
        "                                                            round_metric(gpt3_0_b1), \n",
        "                                                            round_metric(gpt3_0_b2), \n",
        "                                                            round_metric(gpt3_0_b3), \n",
        "                                                            round_metric(gpt3_0_b4), \n",
        "                                                            round_metric(gpt3_0_m), \n",
        "                                                            round_metric(gpt3_0_r),\n",
        "                                                            round_metric(gpt3_0_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('One-shot GPT3', \n",
        "                                                            round_metric(gpt3_1_b1), \n",
        "                                                            round_metric(gpt3_1_b2), \n",
        "                                                            round_metric(gpt3_1_b3), \n",
        "                                                            round_metric(gpt3_1_b4), \n",
        "                                                            round_metric(gpt3_1_m), \n",
        "                                                            round_metric(gpt3_1_r),\n",
        "                                                            round_metric(gpt3_1_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('Two-shot GPT3', \n",
        "                                                            round_metric(gpt3_2_b1), \n",
        "                                                            round_metric(gpt3_2_b2), \n",
        "                                                            round_metric(gpt3_2_b3), \n",
        "                                                            round_metric(gpt3_2_b4), \n",
        "                                                            round_metric(gpt3_2_m), \n",
        "                                                            round_metric(gpt3_2_r),\n",
        "                                                            round_metric(gpt3_2_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('Three-shot GPT3', \n",
        "                                                            round_metric(gpt3_3_b1), \n",
        "                                                            round_metric(gpt3_3_b2), \n",
        "                                                            round_metric(gpt3_3_b3), \n",
        "                                                            round_metric(gpt3_3_b4), \n",
        "                                                            round_metric(gpt3_3_m), \n",
        "                                                            round_metric(gpt3_3_r),\n",
        "                                                            round_metric(gpt3_3_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|                                Question body evaluation                               |\")\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('MODEL', 'BLEU1','BLEU2','BLEU3','BLEU4','METEOR','ROUGE','COVERAGE'))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('Ours-WSDM', \n",
        "                                                            round_metric(t_zero_nd_wsdm_b1), \n",
        "                                                            round_metric(t_zero_nd_wsdm_b2), \n",
        "                                                            round_metric(t_zero_nd_wsdm_b3), \n",
        "                                                            round_metric(t_zero_nd_wsdm_b4), \n",
        "                                                            round_metric(t_zero_nd_wsdm_m), \n",
        "                                                            round_metric(t_zero_nd_wsdm_r), \n",
        "                                                            round_metric(t_zero_nd_wsdm_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('Ours-AutoScore', \n",
        "                                                            round_metric(t_zero_nd_auto_b1), \n",
        "                                                            round_metric(t_zero_nd_auto_b2), \n",
        "                                                            round_metric(t_zero_nd_auto_b3), \n",
        "                                                            round_metric(t_zero_nd_auto_b4), \n",
        "                                                            round_metric(t_zero_nd_auto_m), \n",
        "                                                            round_metric(t_zero_nd_auto_r), \n",
        "                                                            round_metric(t_zero_nd_auto_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('Zero-shot GPT3', \n",
        "                                                            round_metric(t_gpt3_0_b1), \n",
        "                                                            round_metric(t_gpt3_0_b2), \n",
        "                                                            round_metric(t_gpt3_0_b3), \n",
        "                                                            round_metric(t_gpt3_0_b4), \n",
        "                                                            round_metric(t_gpt3_0_m), \n",
        "                                                            round_metric(t_gpt3_0_r),\n",
        "                                                            round_metric(t_gpt3_0_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('One-shot GPT3', \n",
        "                                                            round_metric(t_gpt3_1_b1), \n",
        "                                                            round_metric(t_gpt3_1_b2), \n",
        "                                                            round_metric(t_gpt3_1_b3), \n",
        "                                                            round_metric(t_gpt3_1_b4), \n",
        "                                                            round_metric(t_gpt3_1_m), \n",
        "                                                            round_metric(t_gpt3_1_r),\n",
        "                                                            round_metric(t_gpt3_1_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('Two-shot GPT3', \n",
        "                                                            round_metric(t_gpt3_2_b1), \n",
        "                                                            round_metric(t_gpt3_2_b2), \n",
        "                                                            round_metric(t_gpt3_2_b3), \n",
        "                                                            round_metric(t_gpt3_2_b4), \n",
        "                                                            round_metric(t_gpt3_2_m), \n",
        "                                                            round_metric(t_gpt3_2_r),\n",
        "                                                            round_metric(t_gpt3_2_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('Three-shot GPT3', \n",
        "                                                            round_metric(t_gpt3_3_b1), \n",
        "                                                            round_metric(t_gpt3_3_b2), \n",
        "                                                            round_metric(t_gpt3_3_b3), \n",
        "                                                            round_metric(t_gpt3_3_b4), \n",
        "                                                            round_metric(t_gpt3_3_m), \n",
        "                                                            round_metric(t_gpt3_3_r),\n",
        "                                                            round_metric(t_gpt3_3_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# *Previous  experiments\n",
        "## 2.1. Template + Facet + Huggingface constrained decoding + ranked by perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 tell me about cass county missouri - list homes sale - would you like to know how list homes are list homes sale\n",
            "50 tell me about memory - computer - do you want to know what it was like when the computer went down\n",
            "100 Find information on ontario california airport. - directions location - would you like to be notified via e mail when this directions location\n",
            "150 Find me map of USA - roads - do you need information on the roads and highways in your area\n",
            "200 Where can I buy pressure washers? - washer - do you want to know what is the difference between a vacuum sealer and\n",
            "250 Tell me about defender - lyrics - do you want to play with\n",
            "300 Tell me more about Rocky Mountain News - recent events historical - do you want to know what the recent events have been like recent events historical\n",
            "350 I want to know about appraisals. - appraisal cost - do you want to know if the appraisal is valid or not\n",
            "400 Where should I order dog clean-up bags - specif bag type - do you need to know about spec bag and spec bag type\n",
            "b1 0.31507733449615777 b2 0.10312269203212904 b3 0.05897529574443889 b4 0.04021423398750196\n",
            "rouge-L 0.33304286640926145\n",
            "m 0.31188528276503297\n",
            "cons 0.7254901960784315\n"
          ]
        }
      ],
      "source": [
        "facet_test_file = 'data/clariq_f/ClariQ-FKw-dev.tsv'\n",
        "facet_test_data = pd.read_csv(facet_test_file, sep='\\t')\n",
        "_, facet_test_data = process_clariq_f(facet_test_data)\n",
        "\n",
        "rs = rouge.Rouge()\n",
        "rs_list = []\n",
        "b1, b2, b3, b4 = [], [], [], []\n",
        "m = []\n",
        "c = []\n",
        "ps = PorterStemmer()\n",
        "\n",
        "model_output = 'zeroshot_hf.csv'\n",
        "\n",
        "if os.path.isfile(model_output):\n",
        "    model_output_data = pd.read_csv(model_output)\n",
        "\n",
        "    for iter, row in facet_test_data.iterrows():\n",
        "        query = model_output_data.at[iter, 'query']\n",
        "        facet = facet_test_data.at[iter, 'facet_desc']\n",
        "        ref = model_output_data.at[iter, 'reference']\n",
        "        tokenized_ref = word_tokenize(ref)\n",
        "        generated_cq = model_output_data.at[iter, 'candidate']\n",
        "        tokenized_hyp = word_tokenize(generated_cq)\n",
        "        \n",
        "        if iter % 50 == 0: \n",
        "            print(iter, query, \"-\", facet, '-', generated_cq)\n",
        "\n",
        "        rs_list.append(rs.get_scores(generated_cq, ref)[0]['rouge-l']['f'])\n",
        "        \n",
        "        b1.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(1, 0, 0, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b2.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 1, 0, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b3.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 0, 1, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b4.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 0, 0, 1),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "\n",
        "        m.append(meteor_score([tokenized_ref], tokenized_hyp))\n",
        "\n",
        "        constraint_unsatisfied = 0\n",
        "        force_words = facet.split()\n",
        "        for constraint in force_words:\n",
        "            if ps.stem(constraint) not in set([ps.stem(w) for w in word_tokenize(generated_cq)]):\n",
        "                constraint_unsatisfied += 1\n",
        "        c.append(1 - constraint_unsatisfied/len(force_words))\n",
        "\n",
        "else:\n",
        "    for iter, row in facet_test_data.iterrows():\n",
        "        facet = facet_test_data.at[iter, 'facet_desc']\n",
        "        query = facet_test_data.at[iter, 'initial_request']\n",
        "        ref = facet_test_data.at[iter, 'question']\n",
        "        tokenized_ref = word_tokenize(ref)\n",
        "\n",
        "        force_flexible = facet.split()\n",
        "        force_words_ids = [ tokenizer(word, add_prefix_space=True, add_special_tokens=False).input_ids[0]\n",
        "                            for word in force_flexible ]\n",
        "        constraints = [PhrasalConstraint(force_words_ids)] \n",
        "\n",
        "        template_scores = {}\n",
        "        for s_t in starting_texts:\n",
        "            starting_text = query + s_t\n",
        "            input_ids = tokenizer(starting_text, return_tensors=\"pt\").input_ids.to(device)\n",
        "            outputs = model.generate(\n",
        "                input_ids,\n",
        "                constraints=constraints,\n",
        "                num_beams=10,\n",
        "                num_return_sequences=5,\n",
        "                no_repeat_ngram_size=1,\n",
        "                remove_invalid_values=True,\n",
        "                max_length = len(input_ids[0]) + 10,\n",
        "                top_p=0.9, \n",
        "                temperature = 1,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            generated_cq = nltk_tokenizer.tokenize(generated_text[len(query):])[0]\n",
        "            generated_cq = process_generation(generated_cq)\n",
        "            generated_cq = ' '.join(word_tokenize(generated_cq))\n",
        "\n",
        "            constraint_penalty = 1\n",
        "            for constraint in force_flexible:\n",
        "                if ps.stem(constraint) not in set([ps.stem(w) for w in word_tokenize(generated_cq)]):\n",
        "                    constraint_penalty *= 2\n",
        "            template_scores[generated_cq] = calculatePerplexity(sentence=query+generated_cq, model=model, tokenizer=tokenizer) \\\n",
        "                                            * constraint_penalty\n",
        "        \n",
        "        generated_cq = sorted(template_scores.keys(), key = lambda x: template_scores[x])[0] \n",
        "\n",
        "        facet_test_data.at[iter, 'generated'] = generated_cq\n",
        "        tokenized_hyp = word_tokenize(facet_test_data.at[iter, 'generated'])\n",
        "        \n",
        "        if iter % 50 == 0: \n",
        "            print(iter, query, \"-\", force_flexible, '-', ' '.join(tokenized_hyp))\n",
        "            pprint.pprint(template_scores)   \n",
        "\n",
        "        rs_list.append(rs.get_scores(generated_cq, ref)[0]['rouge-l']['f'])\n",
        "        \n",
        "        b1.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(1, 0, 0, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b2.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 1, 0, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b3.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 0, 1, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b4.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 0, 0, 1),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "\n",
        "        m.append(meteor_score([tokenized_ref], tokenized_hyp))\n",
        "        \n",
        "    output_df = facet_test_data[['initial_request', 'facet_desc', 'question', 'generated']]\n",
        "    output_df.columns = ['query', 'facet', 'reference', 'candidate']\n",
        "    output_df.to_csv(model_output)\n",
        "\n",
        "print(\"b1\", np.mean(b1), \"b2\", np.mean(b2), \"b3\", np.mean(b3), \"b4\", np.mean(b4))\n",
        "print(\"rouge-L\", np.mean(rs_list))\n",
        "print(\"m\", np.mean(m))\n",
        "print(\"cons\", np.mean(c))\n",
        "\n",
        "zero_hf_b1 = np.mean(b1)\n",
        "zero_hf_b2 = np.mean(b2)\n",
        "zero_hf_b3 = np.mean(b3)\n",
        "zero_hf_b4 = np.mean(b4)\n",
        "zero_hf_m = np.mean(m)\n",
        "zero_hf_r = np.mean(rs_list)\n",
        "zero_hf_c = np.mean(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 2.2 Only template + ranked by perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 tell me about cass county missouri - list homes sale - do you want to know how many people have been killed in the state of Missouri and what are your thoughts on that\n",
            "50 tell me about memory - computer - are you looking for something else to do with your life\n",
            "100 Find information on ontario california airport. - directions location - would you like to be notified via e mail when this article is published\n",
            "150 Find me map of USA - roads - do you want information on how to get there\n",
            "200 Where can I buy pressure washers? - washer - do you want to know if there are any other types of saws in the world that will work well for this job\n",
            "250 Tell me about defender - lyrics - do you need to be in the team\n",
            "300 Tell me more about Rocky Mountain News - recent events historical - do you need information on how to get involved in the community\n",
            "350 I want to know about appraisals. - appraisal cost - do you want to know how much money is involved in the appraisal process\n",
            "400 Where should I order dog clean-up bags - specif bag type - do you need information on how to take care of your pet\n",
            "b1 0.19921835346676747 b2 0.07062655780333853 b3 0.04681598144219006 b4 0.030840970526708585\n",
            "rouge-L 0.229523412792589\n",
            "m 0.2135149726283488\n",
            "c 0.02223529411764706\n"
          ]
        }
      ],
      "source": [
        "facet_test_file = 'data/clariq_f/ClariQ-FKw-dev.tsv'\n",
        "facet_test_data = pd.read_csv(facet_test_file, sep='\\t')\n",
        "_, facet_test_data = process_clariq_f(facet_test_data)\n",
        "\n",
        "rs = rouge.Rouge()\n",
        "rs_list = []\n",
        "b1, b2, b3, b4 = [], [], [], []\n",
        "m = []\n",
        "c = []\n",
        "\n",
        "model_output = 'template.csv'\n",
        "\n",
        "if os.path.isfile(model_output):\n",
        "    model_output_data = pd.read_csv(model_output)\n",
        "\n",
        "    for iter, row in facet_test_data.iterrows():\n",
        "        query = model_output_data.at[iter, 'query']\n",
        "        facet = facet_test_data.at[iter, 'facet_desc']\n",
        "        ref = model_output_data.at[iter, 'reference']\n",
        "        tokenized_ref = word_tokenize(ref)\n",
        "        generated_cq = model_output_data.at[iter, 'candidate']\n",
        "        tokenized_hyp = word_tokenize(generated_cq)\n",
        "        \n",
        "        if iter % 50 == 0: \n",
        "            print(iter, query, \"-\", facet, '-', generated_cq)\n",
        "        \n",
        "        rs_list.append(rs.get_scores(generated_cq, ref)[0]['rouge-l']['f'])\n",
        "        \n",
        "        b1.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(1, 0, 0, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b2.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 1, 0, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b3.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 0, 1, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b4.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 0, 0, 1),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "\n",
        "        m.append(meteor_score([tokenized_ref], tokenized_hyp))\n",
        "\n",
        "        constraint_unsatisfied = 0\n",
        "        force_words = facet.split()\n",
        "        for constraint in force_words:\n",
        "            if ps.stem(constraint) not in set([ps.stem(w) for w in word_tokenize(generated_cq)]):\n",
        "                constraint_unsatisfied += 1\n",
        "        c.append(1 - constraint_unsatisfied/len(force_words))\n",
        "else:\n",
        "    for iter, row in facet_test_data.iterrows():\n",
        "        facet = facet_test_data.at[iter, 'facet_desc']\n",
        "        query = facet_test_data.at[iter, 'initial_request']\n",
        "        ref = facet_test_data.at[iter, 'question']\n",
        "        tokenized_ref = word_tokenize(ref)\n",
        "\n",
        "        template_scores = {}\n",
        "        for s_t in starting_texts:\n",
        "            starting_text = query + s_t\n",
        "            input_ids = tokenizer(starting_text, return_tensors=\"pt\").input_ids.to(device)\n",
        "            outputs = model.generate(\n",
        "                input_ids,\n",
        "                num_beams=10,\n",
        "                num_return_sequences=1,\n",
        "                no_repeat_ngram_size=1,\n",
        "                remove_invalid_values=True,\n",
        "                max_length = len(input_ids[0]) + 20,\n",
        "                top_p=0.9, \n",
        "                temperature = 0.7,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            generated_cq = nltk_tokenizer.tokenize(generated_text[len(query):])[0]\n",
        "            generated_cq = process_generation(generated_cq)\n",
        "            generated_cq = ' '.join(word_tokenize(generated_cq))\n",
        "\n",
        "            \n",
        "            template_scores[generated_cq] = calculatePerplexity(sentence=query+generated_cq, model=model, tokenizer=tokenizer)\n",
        "        \n",
        "        generated_cq = sorted(template_scores.keys(), key = lambda x: template_scores[x])[0] \n",
        "\n",
        "        facet_test_data.at[iter, 'generated'] = generated_cq\n",
        "        tokenized_hyp = word_tokenize(facet_test_data.at[iter, 'generated'])\n",
        "        \n",
        "        if iter % 50 == 0: \n",
        "            print(iter, query, \"-\", facet, '-', generated_cq)\n",
        "            pprint.pprint(template_scores)\n",
        "        rs_list.append(rs.get_scores(generated_cq, ref)[0]['rouge-l']['f'])\n",
        "        \n",
        "        b1.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(1, 0, 0, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b2.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 1, 0, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b3.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 0, 1, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b4.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 0, 0, 1),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "\n",
        "        m.append(meteor_score([tokenized_ref], tokenized_hyp))\n",
        "\n",
        "    output_df = facet_test_data[['initial_request', 'facet_desc', 'question', 'generated']]\n",
        "    output_df.columns = ['query', 'facet', 'reference', 'candidate']\n",
        "    output_df.to_csv(model_output)\n",
        "\n",
        "print(\"b1\", np.mean(b1), \"b2\", np.mean(b2), \"b3\", np.mean(b3), \"b4\", np.mean(b4))\n",
        "print(\"rouge-L\", np.mean(rs_list))\n",
        "print(\"m\", np.mean(m))\n",
        "print(\"c\", np.mean(c))\n",
        "\n",
        "template_b1 = np.mean(b1)\n",
        "template_b2 = np.mean(b2)\n",
        "template_b3 = np.mean(b3)\n",
        "template_b4 = np.mean(b4)\n",
        "template_m = np.mean(m)\n",
        "template_r = np.mean(rs_list)\n",
        "template_c = np.mean(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 2.3. No prompt, Facet + neurologic decoding + ranked by perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 tell me about cass county missouri - list homes sale - county list of homes with sale records.\n",
            "50 tell me about memory - computer - in computer science,\" he said.\n",
            "100 Find information on ontario california airport. - directions location - Click here for directions and location.\n",
            "150 Find me map of USA - roads - and UK roads and highways.\n",
            "200 Where can I buy pressure washers? - washer - I don't know what they are.\n",
            "250 Tell me about defender - lyrics - lyrics and what they mean.\n",
            "300 Tell me more about Rocky Mountain News - recent events historical - and the recent historical events.\n",
            "350 I want to know about appraisals. - appraisal cost - The cost of an appraisal.\n",
            "400 Where should I order dog clean-up bags - specif bag type - and specifc type bag.\n",
            "b1 0.21009466369496277 b2 0.06442674864631723 b3 0.028553873315365456 b4 0.022501687356610053\n",
            "rouge-L 0.23507797559462323\n",
            "m 0.19491474938070089\n",
            "c 0.9027450980392155\n"
          ]
        }
      ],
      "source": [
        "facet_test_file = 'data/clariq_f/ClariQ-FKw-dev.tsv'\n",
        "facet_test_data = pd.read_csv(facet_test_file, sep='\\t')\n",
        "_, facet_test_data = process_clariq_f(facet_test_data)\n",
        "\n",
        "generated_file = 'neurologic_decoding/zero_shot/gpt2noprompt'\n",
        "\n",
        "rs = rouge.Rouge()\n",
        "rs_list = []\n",
        "b1, b2, b3, b4 = [], [], [], []\n",
        "m = []\n",
        "c = []\n",
        "ps = PorterStemmer()\n",
        "\n",
        "generated_cqs = open(generated_file, 'r').readlines()\n",
        "\n",
        "for iter, row in facet_test_data.iterrows():\n",
        "    query = model_output_data.at[iter, 'query']\n",
        "    facet = facet_test_data.at[iter, 'facet_desc']\n",
        "    ref = model_output_data.at[iter, 'reference']\n",
        "    tokenized_ref = word_tokenize(ref)\n",
        "    generated_cq = generated_cqs[iter][len(facet_test_data.at[iter, 'initial_request']):].strip()\n",
        "    facet_test_data.at[iter, 'generated'] = generated_cq\n",
        "    tokenized_hyp = word_tokenize(facet_test_data.at[iter, 'generated'])\n",
        "    \n",
        "    if iter % 50 == 0: \n",
        "        print(iter, query, \"-\", facet, '-', generated_cq)\n",
        "    \n",
        "    rs_list.append(rs.get_scores(generated_cq, ref)[0]['rouge-l']['f'])\n",
        "    \n",
        "    b1.append(sentence_bleu([tokenized_ref], \n",
        "                            tokenized_hyp, \n",
        "                            weights=(1, 0, 0, 0),\n",
        "                            smoothing_function = SmoothingFunction().method1\n",
        "                            ))\n",
        "    b2.append(sentence_bleu([tokenized_ref], \n",
        "                            tokenized_hyp, \n",
        "                            weights=(0, 1, 0, 0),\n",
        "                            smoothing_function = SmoothingFunction().method1\n",
        "                            ))\n",
        "    b3.append(sentence_bleu([tokenized_ref], \n",
        "                            tokenized_hyp, \n",
        "                            weights=(0, 0, 1, 0),\n",
        "                            smoothing_function = SmoothingFunction().method1\n",
        "                            ))\n",
        "    b4.append(sentence_bleu([tokenized_ref], \n",
        "                            tokenized_hyp, \n",
        "                            weights=(0, 0, 0, 1),\n",
        "                            smoothing_function = SmoothingFunction().method1\n",
        "                            ))\n",
        "\n",
        "    m.append(meteor_score([tokenized_ref], tokenized_hyp))\n",
        "\n",
        "    constraint_unsatisfied = 0\n",
        "    force_words = facet.split()\n",
        "    for constraint in force_words:\n",
        "        if ps.stem(constraint) not in set([ps.stem(w) for w in word_tokenize(generated_cq)]):\n",
        "            constraint_unsatisfied += 1\n",
        "    c.append(1 - constraint_unsatisfied/len(force_words))\n",
        "\n",
        "print(\"b1\", np.mean(b1), \"b2\", np.mean(b2), \"b3\", np.mean(b3), \"b4\", np.mean(b4))\n",
        "print(\"rouge-L\", np.mean(rs_list))\n",
        "print(\"m\", np.mean(m))\n",
        "print(\"c\", np.mean(c))\n",
        "\n",
        "output_df = facet_test_data[['initial_request', 'facet_desc', 'question', 'generated']]\n",
        "output_df.columns = ['query', 'facet', 'reference', 'candidate']\n",
        "output_df.to_csv('zeroshot_nd_noprompt_pp.csv')\n",
        "\n",
        "facet_nd_b1 = np.mean(b1)\n",
        "facet_nd_b2 = np.mean(b2)\n",
        "facet_nd_b3 = np.mean(b3)\n",
        "facet_nd_b4 = np.mean(b4)\n",
        "facet_nd_m = np.mean(m)\n",
        "facet_nd_r = np.mean(rs_list)\n",
        "facet_nd_c = np.mean(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 2.7. Template + Facet&subject + neuro + perplexity (Ours v2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 tell me about cass county missouri - list homes sale - do you want to know the list of sale homes\n",
            "50 tell me about memory - computer - are you interested in computer science\n",
            "100 Find information on ontario california airport. - directions location - do you want to know the location of the airport\n",
            "150 Find me map of USA - roads - do you want to know what roads are in the map\n",
            "200 Where can I buy pressure washers? - washer - do you want to know what pressure washer washers are\n",
            "250 Tell me about defender - lyrics - do you want to know more about defender lyrics\n",
            "300 Tell me more about Rocky Mountain News - recent events historical - would you like to know more about recent historical events\n",
            "350 I want to know about appraisals. - appraisal cost - do you want to know what appraisal cost appra apprais\n",
            "400 Where should I order dog clean-up bags - specif bag type - do you want to know what type specifical dog bag bags are available\n",
            "b1 0.44002332248763826 b2 0.1849682947950811 b3 0.10529881262916234 b4 0.06571861356147742\n",
            "rouge-L 0.4573270261078457\n",
            "m 0.43493450121992033\n",
            "c 0.9830980392156863\n"
          ]
        }
      ],
      "source": [
        "facet_test_file = 'data/clariq_f/ClariQ-FKw-dev.tsv'\n",
        "facet_test_data = pd.read_csv(facet_test_file, sep='\\t')\n",
        "_, facet_test_data = process_clariq_f(facet_test_data)\n",
        "\n",
        "\n",
        "rs = rouge.Rouge()\n",
        "rs_list = []\n",
        "b1, b2, b3, b4 = [], [], [], []\n",
        "m = []\n",
        "ps = PorterStemmer()\n",
        "c = []\n",
        "\n",
        "model_output = 'zeroshot_taggednoun_nd_pp.csv'\n",
        "\n",
        "if os.path.isfile(model_output):\n",
        "    model_output_data = pd.read_csv(model_output)\n",
        "\n",
        "    for iter, row in facet_test_data.iterrows():\n",
        "        query = model_output_data.at[iter, 'query']\n",
        "        facet = facet_test_data.at[iter, 'facet_desc']\n",
        "        ref = model_output_data.at[iter, 'reference']\n",
        "        tokenized_ref = word_tokenize(ref)\n",
        "        generated_cq = model_output_data.at[iter, 'candidate']\n",
        "        tokenized_hyp = word_tokenize(generated_cq)\n",
        "        \n",
        "        if iter % 50 == 0: \n",
        "            print(iter, query, \"-\", facet, '-', generated_cq)\n",
        "            \n",
        "        rs_list.append(rs.get_scores(generated_cq, ref)[0]['rouge-l']['f'])\n",
        "        \n",
        "        b1.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(1, 0, 0, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b2.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 1, 0, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b3.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 0, 1, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b4.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 0, 0, 1),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "\n",
        "        m.append(meteor_score([tokenized_ref], tokenized_hyp))\n",
        "\n",
        "        constraint_unsatisfied = 0\n",
        "        force_words = facet.split()\n",
        "        for constraint in force_words:\n",
        "            if ps.stem(constraint) not in set([ps.stem(w) for w in word_tokenize(generated_cq)]):\n",
        "                constraint_unsatisfied += 1\n",
        "        c.append(1 - constraint_unsatisfied/len(force_words))\n",
        "else:\n",
        "    generated_file = 'neurologic_decoding/zero_shot/gpt2facet_taggednoun'\n",
        "    generated_cq_all_templates = open(generated_file, 'r').readlines()\n",
        "    generated_cq_grouped = [[generated_cq_all_templates[len(starting_texts) * k + l] \n",
        "                                for l in range(len(starting_texts))] \n",
        "                                for k in range(int(len(generated_cq_all_templates)/8))]\n",
        "    for iter, row in facet_test_data.iterrows():\n",
        "        query = ''\n",
        "        facet = facet_test_data.at[iter, 'facet_desc']\n",
        "        ref = facet_test_data.at[iter, 'question']\n",
        "        tokenized_ref = word_tokenize(ref)\n",
        "        force_flexible = facet.split()\n",
        "\n",
        "        template_scores = {}\n",
        "        for full_sentence in generated_cq_grouped[iter]:\n",
        "            query = re.sub('\\[SEP\\]', '&', full_sentence).split('&')[0].strip()\n",
        "            generated_follow_up = re.sub('\\[SEP\\]', '&', full_sentence).split('&')[1].strip()\n",
        "            generated_cq = re.sub('[.?]', '&', generated_follow_up).split('&')[0].strip()\n",
        "            constraint_penalty = 1\n",
        "            for constraint in force_flexible:\n",
        "                if ps.stem(constraint) not in set([ps.stem(w) for w in word_tokenize(generated_cq)]):\n",
        "                    constraint_penalty *= 2\n",
        "            template_scores[generated_cq] = calculatePerplexity(sentence=full_sentence, model=model, tokenizer=tokenizer) * constraint_penalty\n",
        "        \n",
        "        generated_cq = sorted(template_scores.keys(), key = lambda x: template_scores[x])[0] \n",
        "\n",
        "        facet_test_data.at[iter, 'generated'] = generated_cq\n",
        "        tokenized_hyp = word_tokenize(facet_test_data.at[iter, 'generated'])\n",
        "        \n",
        "        if iter % 50 == 0: \n",
        "            print(iter, query, \"-\", facet, '-', generated_cq)\n",
        "            pprint.pprint(template_scores)\n",
        "        rs_list.append(rs.get_scores(generated_cq, ref)[0]['rouge-l']['f'])\n",
        "        \n",
        "        b1.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(1, 0, 0, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b2.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 1, 0, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b3.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 0, 1, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b4.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 0, 0, 1),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "\n",
        "        m.append(meteor_score([tokenized_ref], tokenized_hyp))\n",
        "    \n",
        "    output_df = facet_test_data[['initial_request', 'facet_desc', 'question', 'generated']]\n",
        "    output_df.columns = ['query', 'facet', 'reference', 'candidate']\n",
        "    output_df.to_csv(model_output)\n",
        "\n",
        "print(\"b1\", np.mean(b1), \"b2\", np.mean(b2), \"b3\", np.mean(b3), \"b4\", np.mean(b4))\n",
        "print(\"rouge-L\", np.mean(rs_list))\n",
        "print(\"m\", np.mean(m))\n",
        "print(\"c\", np.mean(c))\n",
        "\n",
        "zero_taggednoun_nd_b1 = np.mean(b1)\n",
        "zero_taggednoun_nd_b2 = np.mean(b2)\n",
        "zero_taggednoun_nd_b3 = np.mean(b3)\n",
        "zero_taggednoun_nd_b4 = np.mean(b4)\n",
        "zero_taggednoun_nd_m = np.mean(m)\n",
        "zero_taggednoun_nd_r = np.mean(rs_list)\n",
        "zero_taggednoun_nd_c = np.mean(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 2.8 Only \"Are you interested in\", no question body (boarderline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "b1 0.09052243592983396 b2 0.03240188263147355 b3 0.030044300631148498 b4 0.027229303729754527\n",
            "rouge-L 0.20390190521530863\n",
            "m 0.16745748922004236\n",
            "c 0.004705882352941176\n"
          ]
        }
      ],
      "source": [
        "facet_test_file = 'data/clariq_f/ClariQ-FKw-dev.tsv'\n",
        "facet_test_data = pd.read_csv(facet_test_file, sep='\\t')\n",
        "_, facet_test_data = process_clariq_f(facet_test_data)\n",
        "\n",
        "rs = rouge.Rouge()\n",
        "rs_list = []\n",
        "b1, b2, b3, b4 = [], [], [], []\n",
        "m = []\n",
        "ps = PorterStemmer()\n",
        "c = []\n",
        "\n",
        "generated_file = 'neurologic_decoding/zero_shot/gpt2facet'\n",
        "generated_cq_all_templates = open(generated_file, 'r').readlines()\n",
        "generated_cq_grouped = [[generated_cq_all_templates[len(starting_texts) * k + l] \n",
        "                            for l in range(len(starting_texts))] \n",
        "                            for k in range(int(len(generated_cq_all_templates)/8         \n",
        "                            ))]\n",
        "\n",
        "for iter, row in facet_test_data.iterrows():\n",
        "    query = ''\n",
        "    facet = facet_test_data.at[iter, 'facet_desc']\n",
        "    ref = facet_test_data.at[iter, 'question']\n",
        "    tokenized_ref = word_tokenize(ref)\n",
        "    force_flexible = facet.split()\n",
        "\n",
        "    generated_cq = 'Are you interested in'\n",
        "\n",
        "    facet_test_data.at[iter, 'generated'] = generated_cq\n",
        "    tokenized_hyp = word_tokenize(facet_test_data.at[iter, 'generated'])\n",
        "    \n",
        "    #print(query, \"-\", ' '.join(tokenized_hyp))\n",
        "    #pprint.pprint(template_scores)\n",
        "    rs_list.append(rs.get_scores(generated_cq, ref)[0]['rouge-l']['f'])\n",
        "    \n",
        "    b1.append(sentence_bleu([tokenized_ref], \n",
        "                            tokenized_hyp, \n",
        "                            weights=(1, 0, 0, 0),\n",
        "                            smoothing_function = SmoothingFunction().method1\n",
        "                            ))\n",
        "    b2.append(sentence_bleu([tokenized_ref], \n",
        "                            tokenized_hyp, \n",
        "                            weights=(0, 1, 0, 0),\n",
        "                            smoothing_function = SmoothingFunction().method1\n",
        "                            ))\n",
        "    b3.append(sentence_bleu([tokenized_ref], \n",
        "                            tokenized_hyp, \n",
        "                            weights=(0, 0, 1, 0),\n",
        "                            smoothing_function = SmoothingFunction().method1\n",
        "                            ))\n",
        "    b4.append(sentence_bleu([tokenized_ref], \n",
        "                            tokenized_hyp, \n",
        "                            weights=(0, 0, 0, 1),\n",
        "                            smoothing_function = SmoothingFunction().method1\n",
        "                            ))\n",
        "\n",
        "    m.append(meteor_score([tokenized_ref], tokenized_hyp))\n",
        "\n",
        "    constraint_unsatisfied = 0\n",
        "    force_words = facet.split()\n",
        "    for constraint in force_words:\n",
        "        if ps.stem(constraint) not in set([ps.stem(w) for w in word_tokenize(generated_cq)]):\n",
        "            constraint_unsatisfied += 1\n",
        "    c.append(1 - constraint_unsatisfied/len(force_words))\n",
        "\n",
        "print(\"b1\", np.mean(b1), \"b2\", np.mean(b2), \"b3\", np.mean(b3), \"b4\", np.mean(b4))\n",
        "print(\"rouge-L\", np.mean(rs_list))\n",
        "print(\"m\", np.mean(m))\n",
        "print(\"c\", np.mean(c))\n",
        "\n",
        "template_only_b1 = np.mean(b1)\n",
        "template_only_b2 = np.mean(b2)\n",
        "template_only_b3 = np.mean(b3)\n",
        "template_only_b4 = np.mean(b4)\n",
        "template_only_m = np.mean(m)\n",
        "template_only_r = np.mean(rs_list)\n",
        "template_only_c = np.mean(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 4.2.1. Template + Facet & tagged noun + neuro + WSDM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 tell me about cass county missouri - list homes sale - do you need information on the homes sale list\n",
            "50 tell me about memory - computer - are you interested in computer science\n",
            "100 Find information on ontario california airport. - directions location - are you looking for directions or location information\n",
            "150 Find me map of USA - roads - are you looking for a map of the roads\n",
            "200 Where can I buy pressure washers? - washer - are you looking for pressure washer washers\n",
            "250 Tell me about defender - lyrics - do you want to know more about defender lyrics\n",
            "300 Tell me more about Rocky Mountain News - recent events historical - are you interested in recent historical events\n",
            "350 I want to know about appraisals. - appraisal cost - are you looking for appraisal cost\n",
            "400 Where should I order dog clean-up bags - specif bag type - would you like to specifiy dog bag type bags\n",
            "b1 0.4242048794497764 b2 0.17426603364908366 b3 0.09484695215887695 b4 0.06482489616750807\n",
            "rouge-L 0.4457258369368304\n",
            "m 0.38910815824244727\n",
            "c 0.9748627450980394\n"
          ]
        }
      ],
      "source": [
        "from word_forms.word_forms import get_word_forms\n",
        "facet_test_file = 'data/clariq_f/ClariQ-FKw-dev.tsv'\n",
        "facet_test_data = pd.read_csv(facet_test_file, sep='\\t')\n",
        "_, facet_test_data = process_clariq_f(facet_test_data)\n",
        "\n",
        "\n",
        "def calculate_WSDM(query, doc_list):\n",
        "    lambda_t = 1\n",
        "    lambda_o = 1\n",
        "    lambda_u = 1\n",
        "    mu = 25\n",
        "    collection = ' '.join(doc_list)\n",
        "    collection_size = len(collection)\n",
        "\n",
        "    def tfq(word, doc):\n",
        "        many_forms = get_word_forms(word)\n",
        "        word_forms = [word for k in many_forms.keys() for word in many_forms[k]] \n",
        "        return sum( [sum([1 if w == wf else 0 for w in doc]) for wf in word_forms])\n",
        "\n",
        "    def tf1(qk, qk1, doc):    \n",
        "        many_formsk = get_word_forms(qk)\n",
        "        word_formsk = [word for k in many_formsk.keys() for word in many_formsk[k]] \n",
        "        many_formsk1 = get_word_forms(qk1)\n",
        "        word_formsk1 = [word for k in many_formsk1.keys() for word in many_formsk1[k]] \n",
        "        return sum( [sum([1 if qkf == doc[k] and qk1f == doc[k+1] else 0 for k in range(len(doc)-1)]) for qkf in word_formsk for qk1f in word_formsk1])\n",
        "    \n",
        "    def tfuw(qk, qj, doc):\n",
        "        wsz = 2\n",
        "        many_formsk = get_word_forms(qk)\n",
        "        word_formsk = [word for k in many_formsk.keys() for word in many_formsk[k]] \n",
        "        many_formsj = get_word_forms(qj)\n",
        "        word_formsj = [word for k in many_formsj.keys() for word in many_formsj[k]] \n",
        "        return sum( [sum([1 if qkf == doc[k] and qjf in doc[max(k-wsz,0):min(k+wsz,len(doc))] else 0 for k in range(len(doc))]) for qkf in word_formsk for qjf in word_formsj])\n",
        "\n",
        "    def f_t(query, doc, collection):\n",
        "        return sum([(tfq(word, doc.split()) + mu * tfq(word, collection.split())/collection_size) / (len(doc.split()) + mu) for word in query.split() ])\n",
        "    \n",
        "    def f_o(query, doc, collection):\n",
        "        query = query.split()\n",
        "        if len(query) < 2:\n",
        "            return 0\n",
        "        return sum([(tf1(query[k], query[k+1], doc.split()) + mu * tf1(query[k], query[k+1], collection.split())/collection_size) / (len(doc.split()) + mu)  for k in range(len(query)-1)])\n",
        "\n",
        "    def f_u(query, doc, collection):\n",
        "        query = list(set(query.split()))\n",
        "        l = len(query)\n",
        "        if l < 2:\n",
        "            return 0\n",
        "        return sum([(tfuw(query[k], query[j], doc.split()) + mu * tfuw(query[k], query[j], collection.split())/collection_size) / (len(doc.split()) + mu)  for k in range(l) for j in range(k+1, l)])\n",
        "\n",
        "    return {\n",
        "        doc:lambda_t * f_t(query, doc, collection) + \\\n",
        "            lambda_o * f_o(query, doc, collection) + \\\n",
        "            lambda_u * f_u(query, doc, collection) \n",
        "        for doc in doc_list\n",
        "    }\n",
        "\n",
        "rs_list = []\n",
        "b1, b2, b3, b4 = [], [], [], []\n",
        "m = []\n",
        "ps = PorterStemmer()\n",
        "c = []\n",
        "\n",
        "model_output = 'zeroshot_taggednoun_nd_wsdm.csv'\n",
        "\n",
        "if os.path.isfile(model_output):\n",
        "    model_output_data = pd.read_csv(model_output)\n",
        "\n",
        "    for iter, row in facet_test_data.iterrows():\n",
        "        query = model_output_data.at[iter, 'query']\n",
        "        facet = facet_test_data.at[iter, 'facet_desc']\n",
        "        ref = model_output_data.at[iter, 'reference']\n",
        "        tokenized_ref = word_tokenize(ref)\n",
        "        generated_cq = model_output_data.at[iter, 'candidate']\n",
        "        tokenized_hyp = word_tokenize(generated_cq)\n",
        "        \n",
        "        if iter % 50 == 0: \n",
        "            print(iter, query, \"-\", facet, '-', generated_cq)\n",
        "            \n",
        "        rs_list.append(rs.get_scores(generated_cq, ref)[0]['rouge-l']['f'])\n",
        "        \n",
        "        b1.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(1, 0, 0, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b2.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 1, 0, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b3.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 0, 1, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b4.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 0, 0, 1),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "\n",
        "        m.append(meteor_score([tokenized_ref], tokenized_hyp))\n",
        "\n",
        "        constraint_unsatisfied = 0\n",
        "        force_words = facet.split()\n",
        "        for constraint in force_words:\n",
        "            if ps.stem(constraint) not in set([ps.stem(w) for w in word_tokenize(generated_cq)]):\n",
        "                constraint_unsatisfied += 1\n",
        "        c.append(1 - constraint_unsatisfied/len(force_words))\n",
        "\n",
        "\n",
        "else:\n",
        "\n",
        "    generated_file = 'neurologic_decoding/zero_shot/gpt2facet_taggednoun'\n",
        "    generated_cq_all_templates = open(generated_file, 'r').readlines()\n",
        "    generated_cq_grouped = [[generated_cq_all_templates[len(starting_texts) * k + l] \n",
        "                                for l in range(len(starting_texts))] \n",
        "                                for k in range(int(len(generated_cq_all_templates)/8))]\n",
        "    for iter, row in facet_test_data.iterrows():\n",
        "        query = ''\n",
        "        facet = facet_test_data.at[iter, 'facet_desc']\n",
        "        ref = facet_test_data.at[iter, 'question']\n",
        "        tokenized_ref = word_tokenize(ref)\n",
        "        force_flexible = facet.split()\n",
        "\n",
        "        generated_cqs = []\n",
        "        for full_sentence in generated_cq_grouped[iter]:\n",
        "            query = re.sub('\\[SEP\\]', '&', full_sentence).split('&')[0].strip()\n",
        "            generated_follow_up = re.sub('\\[SEP\\]', '&', full_sentence).split('&')[1].strip()\n",
        "            generated_cq = re.sub('[.?]', '&', generated_follow_up).split('&')[0].strip()\n",
        "            generated_cqs.append(generated_cq)\n",
        "        \n",
        "        template_scores = calculate_WSDM(query=facet, doc_list=generated_cqs)\n",
        "        \n",
        "        sorted_template_scores = sorted(template_scores.items(), key = lambda x: x[1], reverse=True)\n",
        "\n",
        "        generated_cq = sorted(template_scores.keys(), key = lambda x: template_scores[x], reverse=True)[0] \n",
        "\n",
        "        facet_test_data.at[iter, 'generated'] = generated_cq\n",
        "        tokenized_hyp = word_tokenize(facet_test_data.at[iter, 'generated'])\n",
        "        \n",
        "        \n",
        "        if iter % 50 == 0: \n",
        "            print(iter, query, \"-\", facet, '-', generated_cq)\n",
        "            pprint.pprint(sorted_template_scores)\n",
        "        rs_list.append(rs.get_scores(generated_cq, ref)[0]['rouge-l']['f'])\n",
        "        \n",
        "        b1.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(1, 0, 0, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b2.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 1, 0, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b3.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 0, 1, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b4.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 0, 0, 1),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "\n",
        "        m.append(meteor_score([tokenized_ref], tokenized_hyp))\n",
        "\n",
        "    output_df = facet_test_data[['initial_request', 'facet_desc', 'question', 'generated']]\n",
        "    output_df.columns = ['query', 'facet', 'reference', 'candidate']\n",
        "    output_df.to_csv(model_output)\n",
        "\n",
        "print(\"b1\", np.mean(b1), \"b2\", np.mean(b2), \"b3\", np.mean(b3), \"b4\", np.mean(b4))\n",
        "print(\"rouge-L\", np.mean(rs_list))\n",
        "print(\"m\", np.mean(m))\n",
        "print(\"c\", np.mean(c))\n",
        "\n",
        "zero_taggednoun_nd_wsdm_b1 = np.mean(b1)\n",
        "zero_taggednoun_nd_wsdm_b2 = np.mean(b2)\n",
        "zero_taggednoun_nd_wsdm_b3 = np.mean(b3)\n",
        "zero_taggednoun_nd_wsdm_b4 = np.mean(b4)\n",
        "zero_taggednoun_nd_wsdm_m = np.mean(m)\n",
        "zero_taggednoun_nd_wsdm_r = np.mean(rs_list)\n",
        "zero_taggednoun_nd_wsdm_c = np.mean(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 5.  Neurologic Decoding result analysis by each question prompt (template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SEP] are you looking for\n",
            "b1 0.4174069226224852 b2 0.22187649477432006 b3 0.1533567781840256 b4 0.12163278454272482\n",
            "m 0.4121104559849342\n",
            "rouge-L 0.46478386214961215\n",
            "c 0.8964705882352941\n",
            "[SEP] do you want to know\n",
            "b1 0.4321942454843958 b2 0.19389510498075813 b3 0.11558840523981023 b4 0.07344376763353411\n",
            "m 0.4138996199559468\n",
            "rouge-L 0.45402395768220605\n",
            "c 0.8964705882352941\n",
            "[SEP] would you like to\n",
            "b1 0.41677213227730003 b2 0.17027090657557872 b3 0.10502780242708211 b4 0.06548335471949998\n",
            "m 0.3862641413515435\n",
            "rouge-L 0.44457932910308434\n",
            "c 0.8964705882352941\n",
            "[SEP] are you interested in\n",
            "b1 0.41052554687096887 b2 0.18729103322918872 b3 0.10072285376409679 b4 0.0685977893139586\n",
            "m 0.38309885935181376\n",
            "rouge-L 0.4396009010664169\n",
            "c 0.8964705882352941\n",
            "[SEP] do you need information\n",
            "b1 0.3624816392337853 b2 0.13168083028262945 b3 0.05690658424184476 b4 0.029389967541808372\n",
            "m 0.35076128904595166\n",
            "rouge-L 0.3819194215378325\n",
            "c 0.8964705882352941\n",
            "[SEP] do you want information\n",
            "b1 0.3801903034295857 b2 0.15418414436484815 b3 0.07817047534601188 b4 0.031580751317858875\n",
            "m 0.35493391071365926\n",
            "rouge-L 0.4048663263094512\n",
            "c 0.8964705882352941\n",
            "[SEP] do you need to\n",
            "b1 0.3970345101705281 b2 0.13145625013699075 b3 0.052575359256592784 b4 0.02877042438937145\n",
            "m 0.3831688525748109\n",
            "rouge-L 0.42716740678840326\n",
            "c 0.8964705882352941\n",
            "[SEP] do you want to\n",
            "b1 0.40332324425334837 b2 0.1575346315982401 b3 0.0920340805159259 b4 0.055617665056045436\n",
            "m 0.37137924753785456\n",
            "rouge-L 0.43854260321245353\n",
            "c 0.8964705882352941\n",
            "oracle\n",
            "b1 0.6156760102425696 b2 0.4366072731286429 b3 0.3321270562135301 b4 0.23915825200799912\n",
            "m 0.6542762943768199\n",
            "rouge-L 0.6886330813207897\n",
            "c 0.8964705882352941\n"
          ]
        }
      ],
      "source": [
        "facet_test_file = 'data/clariq_f/ClariQ-FKw-dev.tsv'\n",
        "facet_test_data = pd.read_csv(facet_test_file, sep='\\t')\n",
        "_, facet_test_data = process_clariq_f(facet_test_data)\n",
        "\n",
        "generated_file = 'neurologic_decoding/zero_shot/gpt2facet'\n",
        "\n",
        "rs = rouge.Rouge()\n",
        "rs_list = []\n",
        "b1, b2, b3, b4 = [], [], [], []\n",
        "m = []\n",
        "\n",
        "generated_cq_all_templates = open(generated_file, 'r').readlines()\n",
        "generated_cq_grouped = [[generated_cq_all_templates[len(starting_texts) * k + l] \n",
        "                            for l in range(len(starting_texts))] \n",
        "                            for k in range(int(len(generated_cq_all_templates)/8))]\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "for template_i in range(len(starting_texts)):\n",
        "    rs_list = []\n",
        "    b1, b2, b3, b4 = [], [], [], []\n",
        "    m = []\n",
        "    c = []\n",
        "    for iter, row in facet_test_data.iterrows():\n",
        "        query = ''\n",
        "        facet = facet_test_data.at[iter, 'facet_desc']\n",
        "        ref = facet_test_data.at[iter, 'question']\n",
        "        tokenized_ref = word_tokenize(ref)\n",
        "        force_flexible = facet.split()\n",
        "\n",
        "        generated_cqs = []\n",
        "        for full_sentence in generated_cq_grouped[iter]:\n",
        "            query = re.sub('\\[SEP\\]', '&', full_sentence).split('&')[0].strip()\n",
        "            generated_follow_up = re.sub('\\[SEP\\]', '&', full_sentence).split('&')[1].strip()\n",
        "            generated_cq = re.sub('[.?]', '&', generated_follow_up).split('&')[0].strip()\n",
        "            generated_cqs.append(generated_cq)\n",
        "\n",
        "        '''\n",
        "        rs_best = [(k, rs.get_scores(generated_cq, ref)[0]['rouge-l']['f'])\n",
        "            for k, generated_cq in enumerate(generated_cqs)\n",
        "        ]\n",
        "\n",
        "        rs_best = sorted(rs_best, key = lambda x:x[1], reverse = True)\n",
        "        '''\n",
        "        best_id = template_i\n",
        "\n",
        "        facet_test_data.at[iter, 'generated'] = generated_cqs[best_id]\n",
        "\n",
        "        rs_list.append(rs.get_scores(generated_cqs[best_id], ref)[0]['rouge-l']['f'])\n",
        "        \n",
        "        b1.append(\n",
        "            sentence_bleu([tokenized_ref],  word_tokenize(generated_cqs[best_id]), \n",
        "                                weights=(1, 0, 0, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "        ))\n",
        "        b2.append(\n",
        "            sentence_bleu([tokenized_ref],  word_tokenize(generated_cqs[best_id]), \n",
        "                                weights=(0, 1, 0, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "        ))\n",
        "        b3.append(\n",
        "            sentence_bleu([tokenized_ref],  word_tokenize(generated_cqs[best_id]), \n",
        "                                weights=(0, 0, 1, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "        ))\n",
        "        b4.append(\n",
        "            sentence_bleu([tokenized_ref],  word_tokenize(generated_cqs[best_id]), \n",
        "                                weights=(0, 0, 0, 1),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "        ))\n",
        "\n",
        "        m.append(meteor_score([tokenized_ref], word_tokenize(generated_cqs[best_id])))\n",
        "\n",
        "        constraint_unsatisfied = 0\n",
        "        force_words = facet.split()\n",
        "        for constraint in force_words:\n",
        "            if ps.stem(constraint) not in set([ps.stem(w) for w in word_tokenize(generated_cq)]):\n",
        "                constraint_unsatisfied += 1\n",
        "        c.append(1 - constraint_unsatisfied/len(force_words))\n",
        "        \n",
        "    \n",
        "    print(starting_texts[template_i])\n",
        "    print(\"b1\", np.mean(b1), \"b2\", np.mean(b2), \"b3\", np.mean(b3), \"b4\", np.mean(b4))\n",
        "    print(\"m\", np.mean(m))\n",
        "    print(\"rouge-L\", np.mean(rs_list))\n",
        "    print(\"c\", np.mean(c))\n",
        "\n",
        "## oracle\n",
        "\n",
        "rs_list = []\n",
        "b1, b2, b3, b4 = [], [], [], []\n",
        "m = []\n",
        "c = []\n",
        "\n",
        "for iter, row in facet_test_data.iterrows():\n",
        "    query = ''\n",
        "    facet = facet_test_data.at[iter, 'facet_desc']\n",
        "    ref = facet_test_data.at[iter, 'question']\n",
        "    tokenized_ref = word_tokenize(ref)\n",
        "    force_flexible = facet.split()\n",
        "\n",
        "    generated_cqs = []\n",
        "    for full_sentence in generated_cq_grouped[iter]:\n",
        "        query = re.sub('\\[SEP\\]', '&', full_sentence).split('&')[0].strip()\n",
        "        generated_follow_up = re.sub('\\[SEP\\]', '&', full_sentence).split('&')[1].strip()\n",
        "        generated_cq = re.sub('[.?]', '&', generated_follow_up).split('&')[0].strip()\n",
        "        generated_cqs.append(generated_cq)\n",
        "\n",
        "    \n",
        "    rs_best = [(k, rs.get_scores(generated_cq, ref)[0]['rouge-l']['f'])\n",
        "        for k, generated_cq in enumerate(generated_cqs)\n",
        "    ]\n",
        "\n",
        "    rs_best = sorted(rs_best, key = lambda x:x[1], reverse = True)\n",
        "    best_id = rs_best[0][0]\n",
        "\n",
        "    facet_test_data.at[iter, 'generated'] = generated_cqs[best_id]\n",
        "\n",
        "    rs_list.append(rs.get_scores(generated_cqs[best_id], ref)[0]['rouge-l']['f'])\n",
        "    \n",
        "    b1.append(\n",
        "        sentence_bleu([tokenized_ref],  word_tokenize(generated_cqs[best_id]), \n",
        "                            weights=(1, 0, 0, 0),\n",
        "                            smoothing_function = SmoothingFunction().method1\n",
        "    ))\n",
        "    b2.append(\n",
        "        sentence_bleu([tokenized_ref],  word_tokenize(generated_cqs[best_id]), \n",
        "                            weights=(0, 1, 0, 0),\n",
        "                            smoothing_function = SmoothingFunction().method1\n",
        "    ))\n",
        "    b3.append(\n",
        "        sentence_bleu([tokenized_ref],  word_tokenize(generated_cqs[best_id]), \n",
        "                            weights=(0, 0, 1, 0),\n",
        "                            smoothing_function = SmoothingFunction().method1\n",
        "    ))\n",
        "    b4.append(\n",
        "        sentence_bleu([tokenized_ref],  word_tokenize(generated_cqs[best_id]), \n",
        "                            weights=(0, 0, 0, 1),\n",
        "                            smoothing_function = SmoothingFunction().method1\n",
        "    ))\n",
        "\n",
        "    m.append(meteor_score([tokenized_ref], word_tokenize(generated_cqs[best_id])))\n",
        "\n",
        "    constraint_unsatisfied = 0\n",
        "    force_words = facet.split()\n",
        "    for constraint in force_words:\n",
        "        if ps.stem(constraint) not in set([ps.stem(w) for w in word_tokenize(generated_cq)]):\n",
        "            constraint_unsatisfied += 1\n",
        "    c.append(1 - constraint_unsatisfied/len(force_words))\n",
        "\n",
        "\n",
        "print('oracle')\n",
        "print(\"b1\", np.mean(b1), \"b2\", np.mean(b2), \"b3\", np.mean(b3), \"b4\", np.mean(b4))\n",
        "print(\"m\", np.mean(m))\n",
        "print(\"rouge-L\", np.mean(rs_list))\n",
        "print(\"c\", np.mean(c))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 5.1. Neurologic Decoding result analysis by facet word count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Facet 1 count 176 % 0.41411764705882353\n",
            "b1 0.37066004628009935 b2 0.16648085923807768 b3 0.10720332666666116 b4 0.07229187120720551\n",
            "rouge-L 0.4143029459167021\n",
            "m 0.36525013976195686\n",
            "reranking 0.23295454545454544\n",
            "c 0.9545454545454546\n",
            "Facet 2 count 167 % 0.39294117647058824\n",
            "b1 0.44930889741235686 b2 0.1926663900118912 b3 0.11052348497415793 b4 0.07530655005025497\n",
            "rouge-L 0.4613084686140523\n",
            "m 0.41954701182229287\n",
            "reranking 0.16766467065868262\n",
            "c 0.9940119760479041\n",
            "Facet 3 count 57 % 0.13411764705882354\n",
            "b1 0.49581447382812605 b2 0.23869937283842516 b3 0.14656948195102065 b4 0.09623476150229873\n",
            "rouge-L 0.5178166345544951\n",
            "m 0.4764083837759933\n",
            "reranking 0.15789473684210525\n",
            "c 1.0\n",
            "Facet 4+ count 25 % 0.058823529411764705\n",
            "b1 0.5307288955582292 b2 0.22797396076788806 b3 0.11222597116530317 b4 0.05376991257708576\n",
            "rouge-L 0.49804562112769185\n",
            "m 0.5067972981060292\n",
            "reranking 0.12\n",
            "c 0.99\n"
          ]
        }
      ],
      "source": [
        "facet_test_file = 'data/clariq_f/ClariQ-FKw-dev.tsv'\n",
        "facet_test_data = pd.read_csv(facet_test_file, sep='\\t')\n",
        "_, facet_test_data = process_clariq_f(facet_test_data)\n",
        "\n",
        "generated_file = 'neurologic_decoding/zero_shot/gpt2facet'\n",
        "\n",
        "rs = rouge.Rouge()\n",
        "b1_1, b2_1, b3_1, b4_1 = [], [], [], []\n",
        "m_1 = []\n",
        "rs_list_1 = []\n",
        "ranking_acc_1 = []\n",
        "c_1 = []\n",
        "\n",
        "b1_2, b2_2, b3_2, b4_2 = [], [], [], []\n",
        "m_2 = []\n",
        "rs_list_2 = []\n",
        "ranking_acc_2 = []\n",
        "c_2 = []\n",
        "\n",
        "b1_3, b2_3, b3_3, b4_3 = [], [], [], []\n",
        "m_3 = []\n",
        "rs_list_3 = []\n",
        "ranking_acc_3 = []\n",
        "c_3 = []\n",
        "\n",
        "b1_4, b2_4, b3_4, b4_4 = [], [], [], []\n",
        "m_4 = []\n",
        "rs_list_4 = []\n",
        "ranking_acc_4 = []\n",
        "c_4 = []\n",
        "\n",
        "generated_cq_all_templates = open(generated_file, 'r').readlines()\n",
        "generated_cq_grouped = [[generated_cq_all_templates[len(starting_texts) * k + l] \n",
        "                            for l in range(len(starting_texts))] \n",
        "                            for k in range(int(len(generated_cq_all_templates)/8))]\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "for iter, row in facet_test_data.iterrows():\n",
        "    query = ''\n",
        "    facet = facet_test_data.at[iter, 'facet_desc']\n",
        "    ref = facet_test_data.at[iter, 'question']\n",
        "    tokenized_ref = word_tokenize(ref)\n",
        "    force_flexible = facet.split()        \n",
        "\n",
        "    template_scores = {}\n",
        "    generated_cqs = []\n",
        "    for full_sentence in generated_cq_grouped[iter]:\n",
        "        query = re.sub('\\[SEP\\]', '&', full_sentence).split('&')[0].strip()\n",
        "        generated_follow_up = re.sub('\\[SEP\\]', '&', full_sentence).split('&')[1].strip()\n",
        "        generated_cq = re.sub('[.?]', '&', generated_follow_up).split('&')[0].strip()\n",
        "        generated_cqs.append(generated_cq)\n",
        "        constraint_penalty = 1\n",
        "        for constraint in force_flexible:\n",
        "            if ps.stem(constraint) not in set([ps.stem(w) for w in word_tokenize(generated_cq)]):\n",
        "                constraint_penalty *= 2\n",
        "        template_scores[generated_cq] = calculatePerplexity(sentence=full_sentence, model=model, tokenizer=tokenizer) * constraint_penalty\n",
        "    \n",
        "    rs_best = [(generated_cq, rs.get_scores(generated_cq, ref)[0]['rouge-l']['f'])\n",
        "        for k, generated_cq in enumerate(generated_cqs)\n",
        "    ]\n",
        "\n",
        "    rs_best = sorted(rs_best, key = lambda x:x[1], reverse = True)\n",
        "    best_cq = rs_best[0][0]\n",
        "\n",
        "    generated_cq = sorted(template_scores.keys(), key = lambda x: template_scores[x])[0] \n",
        "\n",
        "    facet_test_data.at[iter, 'generated'] = generated_cq\n",
        "    tokenized_hyp = word_tokenize(facet_test_data.at[iter, 'generated'])\n",
        "    \n",
        "    if len(force_flexible) == 1:\n",
        "\n",
        "        rs_list_1.append(rs.get_scores(generated_cq, ref)[0]['rouge-l']['f'])\n",
        "        \n",
        "        b1_1.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(1, 0, 0, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b2_1.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 1, 0, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b3_1.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 0, 1, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b4_1.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 0, 0, 1),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "\n",
        "        m_1.append(meteor_score([tokenized_ref], tokenized_hyp))\n",
        "        ranking_acc_1.append(1 if best_cq == generated_cq else 0)\n",
        "\n",
        "        constraint_unsatisfied = 0\n",
        "        force_words = facet.split()\n",
        "        for constraint in force_words:\n",
        "            if ps.stem(constraint) not in set([ps.stem(w) for w in word_tokenize(generated_cq)]):\n",
        "                constraint_unsatisfied += 1\n",
        "        c_1.append(1 - constraint_unsatisfied/len(force_words))\n",
        "\n",
        "\n",
        "    elif len(force_flexible) == 2:\n",
        "        rs_list_2.append(rs.get_scores(generated_cq, ref)[0]['rouge-l']['f'])\n",
        "        \n",
        "        b1_2.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(1, 0, 0, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b2_2.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 1, 0, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b3_2.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 0, 1, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b4_2.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 0, 0, 1),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "\n",
        "        m_2.append(meteor_score([tokenized_ref], tokenized_hyp))\n",
        "        ranking_acc_2.append(1 if best_cq == generated_cq else 0)\n",
        "\n",
        "        constraint_unsatisfied = 0\n",
        "        force_words = facet.split()\n",
        "        for constraint in force_words:\n",
        "            if ps.stem(constraint) not in set([ps.stem(w) for w in word_tokenize(generated_cq)]):\n",
        "                constraint_unsatisfied += 1\n",
        "        c_2.append(1 - constraint_unsatisfied/len(force_words))\n",
        "\n",
        "    elif len(force_flexible) == 3:\n",
        "        rs_list_3.append(rs.get_scores(generated_cq, ref)[0]['rouge-l']['f'])\n",
        "        \n",
        "        b1_3.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(1, 0, 0, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b2_3.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 1, 0, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b3_3.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 0, 1, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b4_3.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 0, 0, 1),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "\n",
        "        m_3.append(meteor_score([tokenized_ref], tokenized_hyp))\n",
        "        ranking_acc_3.append(1 if best_cq == generated_cq else 0)\n",
        "\n",
        "        constraint_unsatisfied = 0\n",
        "        force_words = facet.split()\n",
        "        for constraint in force_words:\n",
        "            if ps.stem(constraint) not in set([ps.stem(w) for w in word_tokenize(generated_cq)]):\n",
        "                constraint_unsatisfied += 1\n",
        "        c_3.append(1 - constraint_unsatisfied/len(force_words))\n",
        "\n",
        "\n",
        "    else:\n",
        "        rs_list_4.append(rs.get_scores(generated_cq, ref)[0]['rouge-l']['f'])\n",
        "        \n",
        "        b1_4.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(1, 0, 0, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b2_4.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 1, 0, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b3_4.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 0, 1, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b4_4.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 0, 0, 1),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "\n",
        "        m_4.append(meteor_score([tokenized_ref], tokenized_hyp))\n",
        "        ranking_acc_4.append(1 if best_cq == generated_cq else 0)\n",
        "\n",
        "        constraint_unsatisfied = 0\n",
        "        force_words = facet.split()\n",
        "        for constraint in force_words:\n",
        "            if ps.stem(constraint) not in set([ps.stem(w) for w in word_tokenize(generated_cq)]):\n",
        "                constraint_unsatisfied += 1\n",
        "        c_4.append(1 - constraint_unsatisfied/len(force_words))\n",
        "\n",
        "\n",
        "print(\"Facet 1\", \"count\", len(m_1), \"%\", len(m_1)/(len(m_1+m_2+m_3+m_4)))\n",
        "print(\"b1\", np.mean(b1_1), \"b2\", np.mean(b2_1), \"b3\", np.mean(b3_1), \"b4\", np.mean(b4_1))\n",
        "print(\"rouge-L\", np.mean(rs_list_1))\n",
        "print(\"m\", np.mean(m_1))\n",
        "print(\"reranking\", np.mean(ranking_acc_1))\n",
        "print(\"c\", np.mean(c_1))\n",
        "\n",
        "print(\"Facet 2\", \"count\", len(m_2), \"%\", len(m_2)/(len(m_1+m_2+m_3+m_4)))\n",
        "print(\"b1\", np.mean(b1_2), \"b2\", np.mean(b2_2), \"b3\", np.mean(b3_2), \"b4\", np.mean(b4_2))\n",
        "print(\"rouge-L\", np.mean(rs_list_2))\n",
        "print(\"m\", np.mean(m_2))\n",
        "print(\"reranking\", np.mean(ranking_acc_2))\n",
        "print(\"c\", np.mean(c_2))\n",
        "\n",
        "print(\"Facet 3\", \"count\", len(m_3), \"%\", len(m_3)/(len(m_1+m_2+m_3+m_4)))\n",
        "print(\"b1\", np.mean(b1_3), \"b2\", np.mean(b2_3), \"b3\", np.mean(b3_3), \"b4\", np.mean(b4_3))\n",
        "print(\"rouge-L\", np.mean(rs_list_3))\n",
        "print(\"m\", np.mean(m_3))\n",
        "print(\"reranking\", np.mean(ranking_acc_3))\n",
        "print(\"c\", np.mean(c_3))\n",
        "\n",
        "print(\"Facet 4+\", \"count\", len(m_4), \"%\", len(m_4)/(len(m_1+m_2+m_3+m_4)))\n",
        "print(\"b1\", np.mean(b1_4), \"b2\", np.mean(b2_4), \"b3\", np.mean(b3_4), \"b4\", np.mean(b4_4))\n",
        "print(\"rouge-L\", np.mean(rs_list_4))\n",
        "print(\"m\", np.mean(m_4))\n",
        "print(\"reranking\", np.mean(ranking_acc_4))\n",
        "print(\"c\", np.mean(c_4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 5.2. Analysis by facet word count on HF decoding results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Facet 1 count 176 % 0.41411764705882353\n",
            "b1 0.27550844287382986 b2 0.08444366092780525 b3 0.0482652234780481 b4 0.033841097393735003\n",
            "rouge-L 0.2965379003453253\n",
            "m 0.27554149450784154\n",
            "Facet 2 count 167 % 0.39294117647058824\n",
            "b1 0.3211927016220878 b2 0.11521572272004516 b3 0.07002232708974651 b4 0.04742007288917467\n",
            "rouge-L 0.3436276368035422\n",
            "m 0.32994245947243234\n",
            "Facet 3 count 57 % 0.13411764705882354\n",
            "b1 0.37100093680480056 b2 0.11233053592905683 b3 0.0516392139102477 b4 0.03381056724647838\n",
            "rouge-L 0.3856013355210162\n",
            "m 0.3516601800854668\n",
            "Facet 4+ count 25 % 0.058823529411764705\n",
            "b1 0.42528586585242734 b2 0.13284774192629373 b3 0.07730630169513093 b4 0.0515464719139813\n",
            "rouge-L 0.3994982516907758\n",
            "m 0.35643684579964274\n"
          ]
        }
      ],
      "source": [
        "facet_test_file = 'data/clariq_f/ClariQ-FKw-dev.tsv'\n",
        "facet_test_data = pd.read_csv(facet_test_file, sep='\\t')\n",
        "_, facet_test_data = process_clariq_f(facet_test_data)\n",
        "\n",
        "generated_df = pd.read_csv('zeroshot_hf.csv')\n",
        "\n",
        "rs = rouge.Rouge()\n",
        "rs_list_1 = []\n",
        "b1_1, b2_1, b3_1, b4_1 = [], [], [], []\n",
        "m_1 = []\n",
        "rs_list_1 = []\n",
        "\n",
        "b1_2, b2_2, b3_2, b4_2 = [], [], [], []\n",
        "m_2 = []\n",
        "rs_list_2 = []\n",
        "\n",
        "b1_3, b2_3, b3_3, b4_3 = [], [], [], []\n",
        "m_3 = []\n",
        "rs_list_3 = []\n",
        "\n",
        "b1_4, b2_4, b3_4, b4_4 = [], [], [], []\n",
        "m_4 = []\n",
        "rs_list_4 = []\n",
        "\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "for iter, row in facet_test_data.iterrows():\n",
        "    query = ''\n",
        "    facet = facet_test_data.at[iter, 'facet_desc']\n",
        "    ref = facet_test_data.at[iter, 'question']\n",
        "    tokenized_ref = word_tokenize(ref)\n",
        "    force_flexible = facet.split()\n",
        "\n",
        "    generated_cq = generated_df.at[iter, 'candidate']\n",
        "    facet_test_data.at[iter, 'generated'] = generated_df.at[iter, 'candidate']\n",
        "    tokenized_hyp = word_tokenize(facet_test_data.at[iter, 'generated'])\n",
        "    \n",
        "    if len(force_flexible) == 1:\n",
        "\n",
        "        rs_list_1.append(rs.get_scores(generated_cq, ref)[0]['rouge-l']['f'])\n",
        "        \n",
        "        b1_1.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(1, 0, 0, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b2_1.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 1, 0, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b3_1.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 0, 1, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b4_1.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 0, 0, 1),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "\n",
        "        m_1.append(meteor_score([tokenized_ref], tokenized_hyp))\n",
        "\n",
        "    elif len(force_flexible) == 2:\n",
        "        rs_list_2.append(rs.get_scores(generated_cq, ref)[0]['rouge-l']['f'])\n",
        "        \n",
        "        b1_2.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(1, 0, 0, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b2_2.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 1, 0, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b3_2.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 0, 1, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b4_2.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 0, 0, 1),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "\n",
        "        m_2.append(meteor_score([tokenized_ref], tokenized_hyp))\n",
        "\n",
        "    elif len(force_flexible) == 3:\n",
        "        rs_list_3.append(rs.get_scores(generated_cq, ref)[0]['rouge-l']['f'])\n",
        "        \n",
        "        b1_3.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(1, 0, 0, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b2_3.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 1, 0, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b3_3.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 0, 1, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b4_3.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 0, 0, 1),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "\n",
        "        m_3.append(meteor_score([tokenized_ref], tokenized_hyp))\n",
        "\n",
        "    else:\n",
        "        rs_list_4.append(rs.get_scores(generated_cq, ref)[0]['rouge-l']['f'])\n",
        "        \n",
        "        b1_4.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(1, 0, 0, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b2_4.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 1, 0, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b3_4.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 0, 1, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b4_4.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 0, 0, 1),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "\n",
        "        m_4.append(meteor_score([tokenized_ref], tokenized_hyp))\n",
        "\n",
        "\n",
        "print(\"Facet 1\", \"count\", len(m_1), \"%\", len(m_1)/(len(m_1+m_2+m_3+m_4)))\n",
        "print(\"b1\", np.mean(b1_1), \"b2\", np.mean(b2_1), \"b3\", np.mean(b3_1), \"b4\", np.mean(b4_1))\n",
        "print(\"rouge-L\", np.mean(rs_list_1))\n",
        "print(\"m\", np.mean(m_1))\n",
        "\n",
        "print(\"Facet 2\", \"count\", len(m_2), \"%\", len(m_2)/(len(m_1+m_2+m_3+m_4)))\n",
        "print(\"b1\", np.mean(b1_2), \"b2\", np.mean(b2_2), \"b3\", np.mean(b3_2), \"b4\", np.mean(b4_2))\n",
        "print(\"rouge-L\", np.mean(rs_list_2))\n",
        "print(\"m\", np.mean(m_2))\n",
        "\n",
        "print(\"Facet 3\", \"count\", len(m_3), \"%\", len(m_3)/(len(m_1+m_2+m_3+m_4)))\n",
        "print(\"b1\", np.mean(b1_3), \"b2\", np.mean(b2_3), \"b3\", np.mean(b3_3), \"b4\", np.mean(b4_3))\n",
        "print(\"rouge-L\", np.mean(rs_list_3))\n",
        "print(\"m\", np.mean(m_3))\n",
        "\n",
        "print(\"Facet 4+\", \"count\", len(m_4), \"%\", len(m_4)/(len(m_1+m_2+m_3+m_4)))\n",
        "print(\"b1\", np.mean(b1_4), \"b2\", np.mean(b2_4), \"b3\", np.mean(b3_4), \"b4\", np.mean(b4_4))\n",
        "print(\"rouge-L\", np.mean(rs_list_4))\n",
        "print(\"m\", np.mean(m_4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 5.3. Analysis by facet word count on finetuning baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Facet 1 count 176 % 0.41411764705882353\n",
            "b1 0.27280088325905355 b2 0.10163992421528394 b3 0.053693751447859574 b4 0.030843726070661395\n",
            "rouge-L 0.31157616449628356\n",
            "m 0.28959097021452185\n",
            "Facet 2 count 167 % 0.39294117647058824\n",
            "b1 0.2670542077791494 b2 0.1063942612909681 b3 0.06144455024652616 b4 0.04133252298327092\n",
            "rouge-L 0.30837631521180603\n",
            "m 0.27363438989728\n",
            "Facet 3 count 57 % 0.13411764705882354\n",
            "b1 0.30781525643213287 b2 0.1252111938715249 b3 0.07710040862054839 b4 0.05537387034736785\n",
            "rouge-L 0.3456929112139785\n",
            "m 0.3024132761408417\n",
            "Facet 4+ count 25 % 0.058823529411764705\n",
            "b1 0.3105070547995927 b2 0.12173359803414291 b3 0.06166130708769793 b4 0.03171710450988532\n",
            "rouge-L 0.34828246920422423\n",
            "m 0.2989814730411402\n"
          ]
        }
      ],
      "source": [
        "facet_test_file = 'data/clariq_f/ClariQ-FKw-dev.tsv'\n",
        "facet_test_data = pd.read_csv(facet_test_file, sep='\\t')\n",
        "_, facet_test_data = process_clariq_f(facet_test_data)\n",
        "\n",
        "generated_df = pd.read_csv('sekulic.csv')\n",
        "\n",
        "rs = rouge.Rouge()\n",
        "rs_list_1 = []\n",
        "b1_1, b2_1, b3_1, b4_1 = [], [], [], []\n",
        "m_1 = []\n",
        "rs_list_1 = []\n",
        "\n",
        "b1_2, b2_2, b3_2, b4_2 = [], [], [], []\n",
        "m_2 = []\n",
        "rs_list_2 = []\n",
        "\n",
        "b1_3, b2_3, b3_3, b4_3 = [], [], [], []\n",
        "m_3 = []\n",
        "rs_list_3 = []\n",
        "\n",
        "b1_4, b2_4, b3_4, b4_4 = [], [], [], []\n",
        "m_4 = []\n",
        "rs_list_4 = []\n",
        "\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "for iter, row in facet_test_data.iterrows():\n",
        "    query = ''\n",
        "    facet = facet_test_data.at[iter, 'facet_desc']\n",
        "    ref = facet_test_data.at[iter, 'question']\n",
        "    tokenized_ref = word_tokenize(ref)\n",
        "    force_flexible = facet.split()\n",
        "\n",
        "    generated_cq = generated_df.at[iter, 'candidate']\n",
        "    facet_test_data.at[iter, 'generated'] = generated_df.at[iter, 'candidate']\n",
        "    tokenized_hyp = word_tokenize(facet_test_data.at[iter, 'generated'])\n",
        "    \n",
        "    if len(force_flexible) == 1:\n",
        "\n",
        "        rs_list_1.append(rs.get_scores(generated_cq, ref)[0]['rouge-l']['f'])\n",
        "        \n",
        "        b1_1.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(1, 0, 0, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b2_1.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 1, 0, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b3_1.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 0, 1, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b4_1.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 0, 0, 1),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "\n",
        "        m_1.append(meteor_score([tokenized_ref], tokenized_hyp))\n",
        "\n",
        "    elif len(force_flexible) == 2:\n",
        "        rs_list_2.append(rs.get_scores(generated_cq, ref)[0]['rouge-l']['f'])\n",
        "        \n",
        "        b1_2.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(1, 0, 0, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b2_2.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 1, 0, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b3_2.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 0, 1, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b4_2.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 0, 0, 1),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "\n",
        "        m_2.append(meteor_score([tokenized_ref], tokenized_hyp))\n",
        "\n",
        "    elif len(force_flexible) == 3:\n",
        "        rs_list_3.append(rs.get_scores(generated_cq, ref)[0]['rouge-l']['f'])\n",
        "        \n",
        "        b1_3.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(1, 0, 0, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b2_3.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 1, 0, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b3_3.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 0, 1, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b4_3.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 0, 0, 1),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "\n",
        "        m_3.append(meteor_score([tokenized_ref], tokenized_hyp))\n",
        "\n",
        "    else:\n",
        "        rs_list_4.append(rs.get_scores(generated_cq, ref)[0]['rouge-l']['f'])\n",
        "        \n",
        "        b1_4.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(1, 0, 0, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b2_4.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 1, 0, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b3_4.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 0, 1, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b4_4.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 0, 0, 1),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "\n",
        "        m_4.append(meteor_score([tokenized_ref], tokenized_hyp))\n",
        "\n",
        "\n",
        "print(\"Facet 1\", \"count\", len(m_1), \"%\", len(m_1)/(len(m_1+m_2+m_3+m_4)))\n",
        "print(\"b1\", np.mean(b1_1), \"b2\", np.mean(b2_1), \"b3\", np.mean(b3_1), \"b4\", np.mean(b4_1))\n",
        "print(\"rouge-L\", np.mean(rs_list_1))\n",
        "print(\"m\", np.mean(m_1))\n",
        "\n",
        "print(\"Facet 2\", \"count\", len(m_2), \"%\", len(m_2)/(len(m_1+m_2+m_3+m_4)))\n",
        "print(\"b1\", np.mean(b1_2), \"b2\", np.mean(b2_2), \"b3\", np.mean(b3_2), \"b4\", np.mean(b4_2))\n",
        "print(\"rouge-L\", np.mean(rs_list_2))\n",
        "print(\"m\", np.mean(m_2))\n",
        "\n",
        "print(\"Facet 3\", \"count\", len(m_3), \"%\", len(m_3)/(len(m_1+m_2+m_3+m_4)))\n",
        "print(\"b1\", np.mean(b1_3), \"b2\", np.mean(b2_3), \"b3\", np.mean(b3_3), \"b4\", np.mean(b4_3))\n",
        "print(\"rouge-L\", np.mean(rs_list_3))\n",
        "print(\"m\", np.mean(m_3))\n",
        "\n",
        "print(\"Facet 4+\", \"count\", len(m_4), \"%\", len(m_4)/(len(m_1+m_2+m_3+m_4)))\n",
        "print(\"b1\", np.mean(b1_4), \"b2\", np.mean(b2_4), \"b3\", np.mean(b3_4), \"b4\", np.mean(b4_4))\n",
        "print(\"rouge-L\", np.mean(rs_list_4))\n",
        "print(\"m\", np.mean(m_4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 6.9. Huggingface decoding question body"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "b1 0.20177504099202298 b2 0.04947821965166067 b3 0.022721678213357348 b4 0.01882946356277396\n",
            "rouge-L 0.22264293607342042\n",
            "m 0.20600085382212535\n",
            "c 0.7207843137254902\n"
          ]
        }
      ],
      "source": [
        "facet_test_file = 'data/clariq_f/ClariQ-FKw-dev.tsv'\n",
        "facet_test_data = pd.read_csv(facet_test_file, sep='\\t')\n",
        "_, facet_test_data = process_clariq_f(facet_test_data)\n",
        "\n",
        "model_output = 'zeroshot_hf.csv'\n",
        "model_output_data = pd.read_csv(model_output)\n",
        "\n",
        "rs = rouge.Rouge()\n",
        "rs_list = []\n",
        "b1, b2, b3, b4 = [], [], [], []\n",
        "m = []\n",
        "c = []\n",
        "\n",
        "generated_cq_all_templates = open(generated_file, 'r').readlines()\n",
        "generated_cq_grouped = [[generated_cq_all_templates[len(starting_texts) * k + l] \n",
        "                            for l in range(len(starting_texts))] \n",
        "                            for k in range(int(len(generated_cq_all_templates)/8))]\n",
        "\n",
        "template_len = 4\n",
        "ps = PorterStemmer()\n",
        "\n",
        "for iter, row in facet_test_data.iterrows():\n",
        "    facet = facet_test_data.at[iter,'facet_desc']\n",
        "    ref = ' '.join(facet_test_data.at[iter, 'question'].split()[template_len:])\n",
        "    generated_cq = ' '.join(model_output_data.at[iter, 'candidate'].split()[template_len:])\n",
        "\n",
        "    tokenized_ref = word_tokenize(ref)\n",
        "    tokenized_hyp = word_tokenize(generated_cq)\n",
        "\n",
        "    if generated_cq == '':\n",
        "        rs_list.append(0)\n",
        "    else:\n",
        "        rs_list.append(rs.get_scores(generated_cq, ref)[0]['rouge-l']['f'])\n",
        "    \n",
        "    b1.append(sentence_bleu([tokenized_ref], \n",
        "                            tokenized_hyp, \n",
        "                            weights=(1, 0, 0, 0),\n",
        "                            smoothing_function = SmoothingFunction().method1\n",
        "                            ))\n",
        "    b2.append(sentence_bleu([tokenized_ref], \n",
        "                            tokenized_hyp, \n",
        "                            weights=(0, 1, 0, 0),\n",
        "                            smoothing_function = SmoothingFunction().method1\n",
        "                            ))\n",
        "    b3.append(sentence_bleu([tokenized_ref], \n",
        "                            tokenized_hyp, \n",
        "                            weights=(0, 0, 1, 0),\n",
        "                            smoothing_function = SmoothingFunction().method1\n",
        "                            ))\n",
        "    b4.append(sentence_bleu([tokenized_ref], \n",
        "                            tokenized_hyp, \n",
        "                            weights=(0, 0, 0, 1),\n",
        "                            smoothing_function = SmoothingFunction().method1\n",
        "                            ))\n",
        "\n",
        "    m.append(meteor_score([tokenized_ref], tokenized_hyp))\n",
        "\n",
        "    constraint_unsatisfied = 0\n",
        "    force_words = facet.split()\n",
        "    for constraint in force_words:\n",
        "        if ps.stem(constraint) not in set([ps.stem(w) for w in word_tokenize(generated_cq)]):\n",
        "            constraint_unsatisfied += 1\n",
        "    c.append(1 - constraint_unsatisfied/len(force_words))\n",
        "\n",
        "print(\"b1\", np.mean(b1), \"b2\", np.mean(b2), \"b3\", np.mean(b3), \"b4\", np.mean(b4))\n",
        "print(\"rouge-L\", np.mean(rs_list))\n",
        "print(\"m\", np.mean(m))\n",
        "print(\"c\", np.mean(c))\n",
        "\n",
        "hf_truncate_b1 = np.mean(b1)\n",
        "hf_truncate_b2 = np.mean(b2)\n",
        "hf_truncate_b3 = np.mean(b3)\n",
        "hf_truncate_b4 = np.mean(b4)\n",
        "hf_truncate_m = np.mean(m)\n",
        "hf_truncate_r = np.mean(rs_list)\n",
        "hf_truncate_c = np.mean(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 6.10. No prompt, facet only + neuro.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "b1 0.21009466369496277 b2 0.06442674864631723 b3 0.028553873315365456 b4 0.022501687356610053\n",
            "rouge-L 0.23507797559462323\n",
            "m 0.19491474938070089\n",
            "c 0.9027450980392155\n"
          ]
        }
      ],
      "source": [
        "facet_test_file = 'data/clariq_f/ClariQ-FKw-dev.tsv'\n",
        "facet_test_data = pd.read_csv(facet_test_file, sep='\\t')\n",
        "_, facet_test_data = process_clariq_f(facet_test_data)\n",
        "\n",
        "model_output = 'zeroshot_nd_noprompt_pp.csv'\n",
        "model_output_data = pd.read_csv(model_output)\n",
        "\n",
        "rs = rouge.Rouge()\n",
        "rs_list = []\n",
        "b1, b2, b3, b4 = [], [], [], []\n",
        "m = []\n",
        "c = []\n",
        "\n",
        "generated_cq_all_templates = open(generated_file, 'r').readlines()\n",
        "generated_cq_grouped = [[generated_cq_all_templates[len(starting_texts) * k + l] \n",
        "                            for l in range(len(starting_texts))] \n",
        "                            for k in range(int(len(generated_cq_all_templates)/8))]\n",
        "\n",
        "template_len = 0\n",
        "ps = PorterStemmer()\n",
        "\n",
        "for iter, row in facet_test_data.iterrows():\n",
        "    facet = facet_test_data.at[iter,'facet_desc']\n",
        "    ref = ' '.join(facet_test_data.at[iter, 'question'].split()[template_len:])\n",
        "    generated_cq = ' '.join(model_output_data.at[iter, 'candidate'].split()[template_len:])\n",
        "\n",
        "    tokenized_ref = word_tokenize(ref)\n",
        "    tokenized_hyp = word_tokenize(generated_cq)\n",
        "\n",
        "    if generated_cq == '':\n",
        "        rs_list.append(0)\n",
        "    else:\n",
        "        rs_list.append(rs.get_scores(generated_cq, ref)[0]['rouge-l']['f'])\n",
        "    \n",
        "    b1.append(sentence_bleu([tokenized_ref], \n",
        "                            tokenized_hyp, \n",
        "                            weights=(1, 0, 0, 0),\n",
        "                            smoothing_function = SmoothingFunction().method1\n",
        "                            ))\n",
        "    b2.append(sentence_bleu([tokenized_ref], \n",
        "                            tokenized_hyp, \n",
        "                            weights=(0, 1, 0, 0),\n",
        "                            smoothing_function = SmoothingFunction().method1\n",
        "                            ))\n",
        "    b3.append(sentence_bleu([tokenized_ref], \n",
        "                            tokenized_hyp, \n",
        "                            weights=(0, 0, 1, 0),\n",
        "                            smoothing_function = SmoothingFunction().method1\n",
        "                            ))\n",
        "    b4.append(sentence_bleu([tokenized_ref], \n",
        "                            tokenized_hyp, \n",
        "                            weights=(0, 0, 0, 1),\n",
        "                            smoothing_function = SmoothingFunction().method1\n",
        "                            ))\n",
        "\n",
        "    m.append(meteor_score([tokenized_ref], tokenized_hyp))\n",
        "\n",
        "    constraint_unsatisfied = 0\n",
        "    force_words = facet.split()\n",
        "    for constraint in force_words:\n",
        "        if ps.stem(constraint) not in set([ps.stem(w) for w in word_tokenize(generated_cq)]):\n",
        "            constraint_unsatisfied += 1\n",
        "    c.append(1 - constraint_unsatisfied/len(force_words))\n",
        "\n",
        "print(\"b1\", np.mean(b1), \"b2\", np.mean(b2), \"b3\", np.mean(b3), \"b4\", np.mean(b4))\n",
        "print(\"rouge-L\", np.mean(rs_list))\n",
        "print(\"m\", np.mean(m))\n",
        "print(\"c\", np.mean(c))\n",
        "\n",
        "facet_nd_truncate_b1 = np.mean(b1)\n",
        "facet_nd_truncate_b2 = np.mean(b2)\n",
        "facet_nd_truncate_b3 = np.mean(b3)\n",
        "facet_nd_truncate_b4 = np.mean(b4)\n",
        "facet_nd_truncate_m = np.mean(m)\n",
        "facet_nd_truncate_r = np.mean(rs_list)\n",
        "facet_nd_truncate_c = np.mean(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Full question evaluation adding GPT3 results and finetune GPT2 with prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Full question evaluation\n",
            "MODEL                    BLEU1    BLEU2    BLEU3    BLEU4    METEOR   ROUGE    COVERAGE\n",
            "Template-only BDL        9.05     3.24     3.0      2.72     16.75    20.39    0.47    \n",
            "Prompt                   19.92    7.06     4.68     3.08     21.35    22.95    2.22    \n",
            "Facet only (ND)          21.01    6.44     2.86     2.25     19.49    23.51    90.27   \n",
            "Prompt append Facet      37.93    13.89    9.65     7.51     33.74    45.21    100.0   \n",
            "Finetuned GPT2 (Base)    27.75    10.79    6.03     3.83     28.56    31.71    20.85   \n",
            "Finetuned GPT2 w/ prompt 32.79    14.58    8.57     5.63     37.55    40.81    72.55   \n",
            "Zero-shot HF decoding    31.51    10.31    5.9      4.02     31.19    33.3     72.55   \n",
            "Zero-shot ND decoding    42.78    19.01    11.41    7.56     40.98    45.16    97.82   \n",
            "0-shot GPT3              43.11    21.0     13.13    8.65     47.54    46.92    86.35   \n",
            "1-shot GPT3              45.27    23.0     14.25    9.32     49.06    48.13    93.84   \n",
            "2-shot GPT3              40.65    20.62    12.33    8.32     51.58    43.04    87.75   \n",
            "3-shot GPT3              39.46    19.62    11.73    7.6      53.09    43.77    93.38   \n"
          ]
        }
      ],
      "source": [
        "def round_metric(num):\n",
        "    return round(num * 100, 2)\n",
        "\n",
        "print (\"Full question evaluation\")\n",
        "print (\"{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}\".format('MODEL', 'BLEU1','BLEU2','BLEU3','BLEU4','METEOR','ROUGE', 'COVERAGE'))\n",
        "print (\"{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}\".format('Template-only BDL', \n",
        "                                                            round_metric(template_only_b1), \n",
        "                                                            round_metric(template_only_b2), \n",
        "                                                            round_metric(template_only_b3), \n",
        "                                                            round_metric(template_only_b4), \n",
        "                                                            round_metric(template_only_m), \n",
        "                                                            round_metric(template_only_r), \n",
        "                                                            round_metric(template_only_c)))\n",
        "print (\"{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}\".format('Prompt', \n",
        "                                                            round_metric(template_b1), \n",
        "                                                            round_metric(template_b2), \n",
        "                                                            round_metric(template_b3), \n",
        "                                                            round_metric(template_b4), \n",
        "                                                            round_metric(template_m), \n",
        "                                                            round_metric(template_r), \n",
        "                                                            round_metric(template_c)))\n",
        "print (\"{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}\".format('Facet only (ND)', \n",
        "                                                            round_metric(facet_nd_b1), \n",
        "                                                            round_metric(facet_nd_b2), \n",
        "                                                            round_metric(facet_nd_b3), \n",
        "                                                            round_metric(facet_nd_b4), \n",
        "                                                            round_metric(facet_nd_m), \n",
        "                                                            round_metric(facet_nd_r), \n",
        "                                                            round_metric(facet_nd_c)))\n",
        "print (\"{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}\".format('Prompt append Facet', \n",
        "                                                            round_metric(template_facet_b1), \n",
        "                                                            round_metric(template_facet_b2), \n",
        "                                                            round_metric(template_facet_b3), \n",
        "                                                            round_metric(template_facet_b4), \n",
        "                                                            round_metric(template_facet_m), \n",
        "                                                            round_metric(template_facet_r), \n",
        "                                                            round_metric(template_facet_c)))\n",
        "print (\"{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}\".format('Finetuned GPT2 (Base)', \n",
        "                                                            round_metric(sekulic_b1), \n",
        "                                                            round_metric(sekulic_b2), \n",
        "                                                            round_metric(sekulic_b3), \n",
        "                                                            round_metric(sekulic_b4), \n",
        "                                                            round_metric(sekulic_m), \n",
        "                                                            round_metric(sekulic_r), \n",
        "                                                            round_metric(sekulic_c)))\n",
        "print (\"{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}\".format('Finetuned GPT2 w/ prompt', \n",
        "                                                            round_metric(ftgpt2_prompt_b1), \n",
        "                                                            round_metric(ftgpt2_prompt_b2), \n",
        "                                                            round_metric(ftgpt2_prompt_b3), \n",
        "                                                            round_metric(ftgpt2_prompt_b4), \n",
        "                                                            round_metric(ftgpt2_prompt_m), \n",
        "                                                            round_metric(ftgpt2_prompt_r), \n",
        "                                                            round_metric(ftgpt2_prompt_c)))\n",
        "print (\"{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}\".format('Zero-shot HF decoding', \n",
        "                                                            round_metric(zero_hf_b1), \n",
        "                                                            round_metric(zero_hf_b2), \n",
        "                                                            round_metric(zero_hf_b3), \n",
        "                                                            round_metric(zero_hf_b4), \n",
        "                                                            round_metric(zero_hf_m), \n",
        "                                                            round_metric(zero_hf_r), \n",
        "                                                            round_metric(zero_hf_c)))\n",
        "print (\"{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}\".format('Zero-shot ND decoding', \n",
        "                                                            round_metric(zero_nd_b1), \n",
        "                                                            round_metric(zero_nd_b2), \n",
        "                                                            round_metric(zero_nd_b3), \n",
        "                                                            round_metric(zero_nd_b4), \n",
        "                                                            round_metric(zero_nd_m), \n",
        "                                                            round_metric(zero_nd_r), \n",
        "                                                            round_metric(zero_nd_c)))\n",
        "print (\"{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}\".format('0-shot GPT3', \n",
        "                                                            round_metric(gpt3_0_b1), \n",
        "                                                            round_metric(gpt3_0_b2), \n",
        "                                                            round_metric(gpt3_0_b3), \n",
        "                                                            round_metric(gpt3_0_b4), \n",
        "                                                            round_metric(gpt3_0_m), \n",
        "                                                            round_metric(gpt3_0_r), \n",
        "                                                            round_metric(gpt3_0_c)))\n",
        "print (\"{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}\".format('1-shot GPT3', \n",
        "                                                            round_metric(gpt3_1_b1), \n",
        "                                                            round_metric(gpt3_1_b2), \n",
        "                                                            round_metric(gpt3_1_b3), \n",
        "                                                            round_metric(gpt3_1_b4), \n",
        "                                                            round_metric(gpt3_1_m), \n",
        "                                                            round_metric(gpt3_1_r), \n",
        "                                                            round_metric(gpt3_1_c)))\n",
        "print (\"{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}\".format('2-shot GPT3', \n",
        "                                                            round_metric(gpt3_2_b1), \n",
        "                                                            round_metric(gpt3_2_b2), \n",
        "                                                            round_metric(gpt3_2_b3), \n",
        "                                                            round_metric(gpt3_2_b4), \n",
        "                                                            round_metric(gpt3_2_m), \n",
        "                                                            round_metric(gpt3_2_r), \n",
        "                                                            round_metric(gpt3_2_c)))\n",
        "print (\"{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}\".format('3-shot GPT3', \n",
        "                                                            round_metric(gpt3_3_b1), \n",
        "                                                            round_metric(gpt3_3_b2), \n",
        "                                                            round_metric(gpt3_3_b3), \n",
        "                                                            round_metric(gpt3_3_b4), \n",
        "                                                            round_metric(gpt3_3_m), \n",
        "                                                            round_metric(gpt3_3_r), \n",
        "                                                            round_metric(gpt3_3_c)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Generating trial evaluation data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## zero-shot gpt2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from word_forms.word_forms import get_word_forms\n",
        "facet_test_file = 'data/clariq_f/ClariQ-FKw-trial.tsv'\n",
        "facet_test_data = pd.read_csv(facet_test_file, sep='\\t')\n",
        "\n",
        "write_to_file = 'neurologic_decoding/dataset/clean/constraint/test.constraint.json'\n",
        "prompt_write_to_file = 'neurologic_decoding/dataset/clean/init/commongen.test.init.txt'\n",
        "no_prompt_write_to_file = 'neurologic_decoding/dataset/clean/init/commongen_no_prompt.test.init.txt'\n",
        "\n",
        "pos_tagger = spacy.load('en_core_web_sm')\n",
        "\n",
        "all_queries= []\n",
        "all_constraints = []\n",
        "for iter, row in facet_test_data.iterrows():\n",
        "    facet = facet_test_data.at[iter, 'facet_desc']\n",
        "    query = facet_test_data.at[iter, 'initial_request']\n",
        "    noun_in_query = [token.text for token in pos_tagger(query) if token.pos_ == 'NOUN']\n",
        "    propn_in_query = [token.text for token in pos_tagger(query) if token.pos_ == 'PROPN']\n",
        "    all_queries.append(query)\n",
        "\n",
        "    constraints = [[term] for term in facet.split()+noun_in_query]\n",
        "    #constraints += [[' '.join(propn_in_query)]]\n",
        "    #for facet_word in facet.split():\n",
        "    #for facet_word in facet.split():\n",
        "    #    many_forms = get_word_forms(facet_word)\n",
        "    #    constraints.append(list(set([word for k in many_forms.keys() for word in many_forms[k] ]+[facet_word])))\n",
        "    all_constraints.append(constraints)\n",
        "\n",
        "\n",
        "with open(write_to_file, 'w') as output:\n",
        "    for constraints in all_constraints:\n",
        "        for k, prompt in enumerate(starting_texts):\n",
        "            json_str = json.dumps(constraints)\n",
        "            output.write(json_str)\n",
        "            output.write('\\n')\n",
        "\n",
        "with open(prompt_write_to_file, 'w') as output:\n",
        "    for query in all_queries:\n",
        "        for k, prompt in enumerate(starting_texts):\n",
        "            output.write(query + prompt)\n",
        "            output.write('\\n')\n",
        "\n",
        "with open(no_prompt_write_to_file, 'w') as output:\n",
        "    for query in all_queries:\n",
        "        output.write(query)\n",
        "        output.write('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## using WSDM ranker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 tell me about uss yorktown charleston SC - aircrafts - are you looking for aircrafts\n",
            "50 What are specific dangers of asbestos? - exposure asnestos - would you like to discuss asbestos exposure dangers\n",
            "b1 0.4141665089717724 b2 0.1253300292517271 b3 0.056597530124790185 b4 0.0315843977495502\n",
            "rouge-L 0.4001556893146436\n",
            "m 0.3492758853895794\n",
            "c 0.9595959595959596\n"
          ]
        }
      ],
      "source": [
        "facet_test_file = 'data/clariq_f/ClariQ-FKw-trial.tsv'\n",
        "facet_test_data = pd.read_csv(facet_test_file, sep='\\t')\n",
        "_, facet_test_data = process_clariq_f(facet_test_data)\n",
        "\n",
        "rs = rouge.Rouge()\n",
        "rs_list = []\n",
        "b1, b2, b3, b4 = [], [], [], []\n",
        "m = []\n",
        "ps = PorterStemmer()\n",
        "c = []\n",
        "\n",
        "model_output = 'zeroshot_trial.csv'\n",
        "\n",
        "if os.path.isfile(model_output):\n",
        "    model_output_data = pd.read_csv(model_output)\n",
        "\n",
        "    for iter, row in facet_test_data.iterrows():\n",
        "        query = model_output_data.at[iter, 'query']\n",
        "        facet = facet_test_data.at[iter, 'facet_desc']\n",
        "        ref = model_output_data.at[iter, 'reference']\n",
        "        tokenized_ref = word_tokenize(ref)\n",
        "        generated_cq = model_output_data.at[iter, 'candidate']\n",
        "        tokenized_hyp = word_tokenize(generated_cq)\n",
        "        \n",
        "        if iter % 50 == 0: \n",
        "            print(iter, query, \"-\", facet, '-', generated_cq)\n",
        "            \n",
        "        rs_list.append(rs.get_scores(generated_cq, ref)[0]['rouge-l']['f'])\n",
        "        \n",
        "        b1.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(1, 0, 0, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b2.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 1, 0, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b3.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 0, 1, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b4.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 0, 0, 1),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "\n",
        "        m.append(meteor_score([tokenized_ref], tokenized_hyp))\n",
        "\n",
        "        constraint_unsatisfied = 0\n",
        "        force_words = facet.split()\n",
        "        for constraint in force_words:\n",
        "            if ps.stem(constraint) not in set([ps.stem(w) for w in word_tokenize(generated_cq)]):\n",
        "                constraint_unsatisfied += 1\n",
        "        c.append(1 - constraint_unsatisfied/len(force_words))\n",
        "else:\n",
        "    pos_tagger = spacy.load('en_core_web_sm')\n",
        "    generated_file = 'neurologic_decoding/zero_shot/gpt2trial'\n",
        "    generated_cq_all_templates = open(generated_file, 'r').readlines()\n",
        "    generated_cq_grouped = [[generated_cq_all_templates[len(starting_texts) * k + l] \n",
        "                                for l in range(len(starting_texts))] \n",
        "                                for k in range(int(len(generated_cq_all_templates)/8))]\n",
        "    for iter, row in facet_test_data.iterrows():\n",
        "        query = ''\n",
        "        facet = facet_test_data.at[iter, 'facet_desc']\n",
        "        ref = facet_test_data.at[iter, 'question']\n",
        "        tokenized_ref = word_tokenize(ref)\n",
        "        force_flexible = facet.split()\n",
        "            \n",
        "        generated_cqs = []\n",
        "        for full_sentence in generated_cq_grouped[iter]:\n",
        "            query = re.sub('\\[SEP\\]', '&', full_sentence).split('&')[0].strip()\n",
        "            generated_follow_up = re.sub('\\[SEP\\]', '&', full_sentence).split('&')[1].strip()\n",
        "            generated_cq = re.sub('[.?]', '&', generated_follow_up).split('&')[0].strip()\n",
        "            generated_cqs.append(generated_cq)\n",
        "        \n",
        "        center_words = ' '.join([token.text for token in pos_tagger(query) if token.pos_ == 'NOUN' or  token.pos_ == 'PROPN'])\n",
        "\n",
        "        template_scores = calculate_WSDM(query=facet + ' ' + center_words, doc_list=generated_cqs)\n",
        "        \n",
        "        sorted_template_scores = sorted(template_scores.items(), key = lambda x: x[1], reverse=True)\n",
        "\n",
        "        generated_cq = sorted(template_scores.keys(), key = lambda x: template_scores[x], reverse=True)[0] \n",
        "\n",
        "        facet_test_data.at[iter, 'generated'] = generated_cq\n",
        "        tokenized_hyp = word_tokenize(facet_test_data.at[iter, 'generated'])\n",
        "        \n",
        "        if iter % 1 == 0: \n",
        "            print(iter, query, \"-\", facet, '-', generated_cq)\n",
        "            pprint.pprint(template_scores)\n",
        "        rs_list.append(rs.get_scores(generated_cq, ref)[0]['rouge-l']['f'])\n",
        "        \n",
        "        b1.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(1, 0, 0, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b2.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 1, 0, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b3.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 0, 1, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b4.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 0, 0, 1),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "\n",
        "        m.append(meteor_score([tokenized_ref], tokenized_hyp))\n",
        "    \n",
        "    output_df = facet_test_data[['initial_request', 'facet_desc', 'question', 'generated']]\n",
        "    output_df.columns = ['query', 'facet', 'reference', 'candidate']\n",
        "    output_df.to_csv(model_output)\n",
        "\n",
        "print(\"b1\", np.mean(b1), \"b2\", np.mean(b2), \"b3\", np.mean(b3), \"b4\", np.mean(b4))\n",
        "print(\"rouge-L\", np.mean(rs_list))\n",
        "print(\"m\", np.mean(m))\n",
        "print(\"c\", np.mean(c))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## zero-shot gpt3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 tell me about uss yorktown charleston SC - aircrafts - are you interested in aircrafts\n",
            "50 What are specific dangers of asbestos? - exposure asnestos - are you interested in the dangers of asbestos exposure\n",
            "b1 0.4055506205267506 b2 0.21345373382999158 b3 0.1277100675827996 b4 0.08804459598751083\n",
            "rouge-L 0.4553104620841699\n",
            "m 0.4382431443929386\n",
            "c 0.791077441077441\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "facet_test_file = 'data/clariq_f/ClariQ-FKw-trial.tsv'\n",
        "facet_test_data = pd.read_csv(facet_test_file, sep='\\t')\n",
        "_, facet_test_data = process_clariq_f(facet_test_data)\n",
        "\n",
        "rs_list = []\n",
        "b1, b2, b3, b4 = [], [], [], []\n",
        "m = []\n",
        "c = []\n",
        "\n",
        "temperature = 0\n",
        "use_examples = 0\n",
        "model_output = 'zeroshotgpt3_trial.csv'\n",
        "\n",
        "if os.path.isfile(model_output):\n",
        "    model_output_data = pd.read_csv(model_output)\n",
        "\n",
        "    for iter, row in facet_test_data.iterrows():\n",
        "        query = model_output_data.at[iter, 'query']\n",
        "        facet = facet_test_data.at[iter, 'facet_desc']\n",
        "        ref = model_output_data.at[iter, 'reference']\n",
        "        tokenized_ref = word_tokenize(ref)\n",
        "        generated_cq = model_output_data.at[iter, 'candidate']\n",
        "        tokenized_hyp = word_tokenize(generated_cq)\n",
        "        \n",
        "        if iter % 50 == 0: \n",
        "            print(iter, query, \"-\", facet, '-', generated_cq)\n",
        "        \n",
        "        rs_list.append(rs.get_scores(generated_cq, ref)[0]['rouge-l']['f'])\n",
        "        \n",
        "        b1.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(1, 0, 0, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b2.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 1, 0, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b3.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 0, 1, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b4.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 0, 0, 1),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "\n",
        "        m.append(meteor_score([tokenized_ref], tokenized_hyp))\n",
        "\n",
        "        constraint_unsatisfied = 0\n",
        "        force_words = facet.split()\n",
        "        for constraint in force_words:\n",
        "            if ps.stem(constraint) not in set([ps.stem(w) for w in word_tokenize(generated_cq)]):\n",
        "                constraint_unsatisfied += 1\n",
        "        c.append(1 - constraint_unsatisfied/len(force_words))\n",
        "else:\n",
        "    for iter, row in facet_test_data.iterrows():\n",
        "        query = facet_test_data.at[iter, 'initial_request']\n",
        "        facet = facet_test_data.at[iter, 'facet_desc']\n",
        "        ref = facet_test_data.at[iter, 'question']\n",
        "        tokenized_ref = word_tokenize(ref)\n",
        "        force_flexible = facet.split()\n",
        "\n",
        "        template_scores = {}\n",
        "        for s_t in starting_texts:\n",
        "            s_t = re.sub('\\[SEP\\]', ' ', s_t).strip()\n",
        "            prompt = ' '.join(gpt3_examples[:use_examples]) + ' ' + query + ' ' + \"Ask a question that contains words in the list\" + ' ' + \"[\" + \", \".join([\"'\"+f+\"'\" for f in facet.split()])  + '].' + s_t\n",
        "            response = openai.Completion.create(\n",
        "                model=\"text-davinci-002\",\n",
        "                prompt= prompt,\n",
        "                temperature=temperature,\n",
        "                max_tokens=32,\n",
        "                top_p=1,\n",
        "                frequency_penalty=0.0,\n",
        "                presence_penalty=0.0,\n",
        "                stop=[\"\\n\"]\n",
        "            )\n",
        "            \n",
        "            generated_cq = s_t + response['choices'][0]['text']\n",
        "            generated_cq = re.sub('\\[SEP\\]', ' ', generated_cq).strip()\n",
        "            generated_cq = re.sub('[.?]', '&', generated_cq).split('&')[0].strip()\n",
        "\n",
        "            constraint_penalty = 1\n",
        "            for constraint in force_flexible:\n",
        "                if ps.stem(constraint) not in set([ps.stem(w) for w in word_tokenize(generated_cq)]):\n",
        "                    constraint_penalty *= 2\n",
        "            template_scores[generated_cq] = calculatePerplexity(sentence=query + generated_cq, model=model, tokenizer=tokenizer) * constraint_penalty\n",
        "        \n",
        "        generated_cq = sorted(template_scores.keys(), key = lambda x: template_scores[x])[0] \n",
        "        facet_test_data.at[iter, 'generated'] = generated_cq\n",
        "        tokenized_hyp = word_tokenize(facet_test_data.at[iter, 'generated'])\n",
        "        \n",
        "\n",
        "        if iter % 50 == 0: \n",
        "            print(iter, query, \"-\", force_flexible, '-', ' '.join(tokenized_hyp))\n",
        "            pprint.pprint(template_scores)\n",
        "    \n",
        "        rs_list.append(rs.get_scores(generated_cq, ref)[0]['rouge-l']['f'])\n",
        "        \n",
        "        b1.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(1, 0, 0, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b2.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 1, 0, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b3.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 0, 1, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b4.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 0, 0, 1),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "\n",
        "        m.append(meteor_score([tokenized_ref], tokenized_hyp))\n",
        "\n",
        "    output_df = facet_test_data[['initial_request', 'facet_desc', 'question', 'generated']]\n",
        "    output_df.columns = ['query', 'facet', 'reference', 'candidate']\n",
        "    output_df.to_csv(model_output)\n",
        "\n",
        "\n",
        "print(\"b1\", np.mean(b1), \"b2\", np.mean(b2), \"b3\", np.mean(b3), \"b4\", np.mean(b4))\n",
        "print(\"rouge-L\", np.mean(rs_list))\n",
        "print(\"m\", np.mean(m))\n",
        "print(\"c\", np.mean(c))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## finetuning gpt2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output file not found, generating output.\n",
            "0 tell me about uss yorktown charleston SC Ask a question that contains words in the list ['aircrafts']. - do you want to know about aircrafts\n",
            "50 What are specific dangers of asbestos? Ask a question that contains words in the list ['exposure', 'asnestos']. - are you interested in the sun exposure the sun exposure information the sun exposure\n",
            "b1 0.28625975382135704 b2 0.10700706063394176 b3 0.050937711173140464 b4 0.02941782018847191\n",
            "rouge-L 0.37583581111308995\n",
            "m 0.35637739326231804\n",
            "c nan\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "sample_every = 100\n",
        "epochs = 8\n",
        "learning_rate = 5e-5\n",
        "warmup_steps = 1e2\n",
        "epsilon = 1e-8\n",
        "max_length = 128\n",
        "prompt_instruction = ''\n",
        "\n",
        "facet_test_file = 'data/clariq_f/ClariQ-FKw-trial.tsv'\n",
        "facet_test_data = pd.read_csv(facet_test_file, sep='\\t')\n",
        "_, facet_test_data = process_clariq_f(facet_test_data)\n",
        "\n",
        "rs_list = []\n",
        "b1, b2, b3, b4 = [], [], [], []\n",
        "m = []\n",
        "c = []\n",
        "\n",
        "temperature = 0.1\n",
        "model_output = 'ftgpt2_trial.csv'\n",
        "\n",
        "if os.path.isfile(model_output):\n",
        "    model_output_data = pd.read_csv(model_output)\n",
        "\n",
        "    for iter, row in facet_test_data.iterrows():\n",
        "        query = model_output_data.at[iter, 'query']\n",
        "        facet = facet_test_data.at[iter, 'facet_desc']\n",
        "        ref = model_output_data.at[iter, 'reference']\n",
        "        tokenized_ref = word_tokenize(ref)\n",
        "        generated_cq = model_output_data.at[iter, 'candidate']\n",
        "        tokenized_hyp = word_tokenize(generated_cq)\n",
        "        \n",
        "        if iter % 50 == 0: \n",
        "            print(iter, query, \"-\", facet, '-', generated_cq)\n",
        "        \n",
        "        rs_list.append(rs.get_scores(generated_cq, ref)[0]['rouge-l']['f'])\n",
        "        \n",
        "        b1.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(1, 0, 0, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b2.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 1, 0, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b3.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 0, 1, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b4.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 0, 0, 1),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "\n",
        "        m.append(meteor_score([tokenized_ref], tokenized_hyp))\n",
        "\n",
        "        constraint_unsatisfied = 0\n",
        "        force_words = facet.split()\n",
        "        for constraint in force_words:\n",
        "            if ps.stem(constraint) not in set([ps.stem(w) for w in word_tokenize(generated_cq)]):\n",
        "                constraint_unsatisfied += 1\n",
        "        c.append(1 - constraint_unsatisfied/len(force_words))\n",
        "\n",
        "else:  \n",
        "    print(\"Output file not found, generating output.\") \n",
        "    model_dir = './model_save/'+str(epochs)+'/'\n",
        "    if os.path.exists(model_dir):\n",
        "        tokenizer = GPT2Tokenizer.from_pretrained(model_dir, bos_token=BOS, eos_token=EOS, pad_token=PAD) \n",
        "        configuration = GPT2Config.from_pretrained(model_dir, output_hidden_states=False)\n",
        "        model = GPT2LMHeadModel.from_pretrained(model_dir, config=configuration)\n",
        "        model.resize_token_embeddings(len(tokenizer))\n",
        "        model.cuda()\n",
        "    else:\n",
        "        print(\"Model checkpoint not found, finetuning.\")\n",
        "        clariq_f_train_file = 'data/clariq_f/ClariQ-FKw-train_no_trial.tsv'\n",
        "        clariq_f_train_data = pd.read_csv(clariq_f_train_file, sep='\\t') \n",
        "        clariq_f_train_dict, clariq_f_train_data = process_clariq_f(clariq_f_train_data)\n",
        "        clariq_f_train_text_list = clariq_f_train_data['instructional_q_f_cq']\n",
        "\n",
        "        class GPT2Dataset(Dataset):\n",
        "            def __init__(self, txt_list, tokenizer, gpt2_type=\"gpt2\", max_length=768):\n",
        "                self.tokenizer = tokenizer\n",
        "                self.input_ids = []\n",
        "                self.attn_masks = []\n",
        "            \n",
        "                print(\"training text example\", txt_list[0])\n",
        "                for txt in txt_list:\n",
        "                    encodings_dict = tokenizer(txt, truncation=True, max_length=max_length, padding=\"max_length\")\n",
        "                    self.input_ids.append(T.tensor(encodings_dict['input_ids']))\n",
        "                    self.attn_masks.append(T.tensor(encodings_dict['attention_mask']))\n",
        "            \n",
        "            def __len__(self):\n",
        "                return len(self.input_ids)\n",
        "            \n",
        "            def __getitem__(self, idx):\n",
        "                return self.input_ids[idx], self.attn_masks[idx] \n",
        "            \n",
        "        dataset = GPT2Dataset(clariq_f_train_text_list, tokenizer, max_length=max_length)\n",
        "\n",
        "        train_size = int(0.99 * len(dataset))\n",
        "        val_size = len(dataset) - train_size\n",
        "\n",
        "        train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "        print('{:>5,} train /{:>5,} val'.format(train_size, val_size))\n",
        "\n",
        "        train_dataloader = DataLoader(\n",
        "        train_dataset,\n",
        "            sampler = RandomSampler(train_dataset),\n",
        "            batch_size = batch_size\n",
        "        )\n",
        "\n",
        "        validation_dataloader = DataLoader(\n",
        "        val_dataset,\n",
        "            sampler = SequentialSampler(val_dataset),\n",
        "            batch_size = batch_size\n",
        "        )\n",
        "\n",
        "        optimizer = AdamW(model.parameters(),\n",
        "            lr = learning_rate,\n",
        "            eps = epsilon\n",
        "        )\n",
        "\n",
        "        total_steps = len(train_dataloader) * epochs\n",
        "        scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "            num_warmup_steps = warmup_steps, \n",
        "            num_training_steps = total_steps\n",
        "        )\n",
        "\n",
        "        training_stats = []\n",
        "\n",
        "        for epoch_i in range(0, epochs):\n",
        "            # ========================================\n",
        "            #               Training\n",
        "            # ========================================\n",
        "            print(\"\")\n",
        "            print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "            print('Training...')\n",
        "\n",
        "            total_train_loss = 0\n",
        "\n",
        "            model.train()\n",
        "\n",
        "            for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "                b_input_ids = batch[0].to(device)\n",
        "                b_labels = batch[0].to(device)\n",
        "                b_masks = batch[1].to(device)\n",
        "\n",
        "                model.zero_grad()        \n",
        "\n",
        "                outputs = model(  b_input_ids,\n",
        "                                labels=b_labels, \n",
        "                                attention_mask = b_masks,\n",
        "                                token_type_ids=None\n",
        "                                )\n",
        "                loss = outputs[0]  \n",
        "\n",
        "                batch_loss = loss.item()\n",
        "                total_train_loss += batch_loss\n",
        "\n",
        "                # Get sample every x batches.\n",
        "                if step % sample_every == 0 and not step == 0:\n",
        "                    print('  Batch {:>5,}  of  {:>5,}. Loss: {:>5,}.'.format(step, len(train_dataloader), batch_loss))\n",
        "                    model.eval()\n",
        "                    model.train()\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "\n",
        "            # Calculate the average loss over all of the batches.\n",
        "            avg_train_loss = total_train_loss / len(train_dataloader)       \n",
        "            \n",
        "            # Measure how long this epoch took.\n",
        "\n",
        "            print(\"\")\n",
        "            print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))       \n",
        "            # ========================================\n",
        "            #               Validation\n",
        "            # ========================================\n",
        "            print(\"\")\n",
        "            print(\"Running Validation...\")\n",
        "            model.eval()\n",
        "            total_eval_loss = 0\n",
        "            nb_eval_steps = 0\n",
        "\n",
        "            # Evaluate data for one epoch\n",
        "            for batch in validation_dataloader:       \n",
        "                b_input_ids = batch[0].to(device)\n",
        "                b_labels = batch[0].to(device)\n",
        "                b_masks = batch[1].to(device)\n",
        "                \n",
        "                with T.no_grad():        \n",
        "                    outputs  = model(b_input_ids, \n",
        "                                    attention_mask = b_masks,\n",
        "                                    labels=b_labels)         \n",
        "                    loss = outputs[0]  \n",
        "                    \n",
        "                batch_loss = loss.item()\n",
        "                total_eval_loss += batch_loss        \n",
        "\n",
        "            avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "\n",
        "\n",
        "            print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "\n",
        "            # Record all statistics from this epoch.\n",
        "            training_stats.append(\n",
        "                {\n",
        "                    'epoch': epoch_i + 1,\n",
        "                    'Training Loss': avg_train_loss,\n",
        "                    'Valid. Loss': avg_val_loss,\n",
        "                }\n",
        "            )\n",
        "\n",
        "        print(\"\")\n",
        "        print(\"Training complete!\")\n",
        "\n",
        "        output_dir = model_dir\n",
        "        if not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "\n",
        "        print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "        # They can then be reloaded using `from_pretrained()`\n",
        "        model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "        model_to_save.save_pretrained(output_dir)\n",
        "        tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "    clariq_f_test_file = 'data/clariq_f/ClariQ-FKw-trial.tsv'\n",
        "    clariq_f_test_data = pd.read_csv(clariq_f_test_file, sep='\\t') \n",
        "    clariq_f_test_dict, clariq_f_test_data = process_clariq_f(clariq_f_test_data)\n",
        "\n",
        "    rs = rouge.Rouge()\n",
        "    rs_list = []\n",
        "    b1, b2, b3, b4 = [], [], [], []\n",
        "    m = []\n",
        "    \n",
        "    for iter, row in clariq_f_test_data.iterrows():\n",
        "        query = clariq_f_test_data.at[iter, 'instructional_q_f']\n",
        "        ref = clariq_f_test_data.at[iter, 'question']\n",
        "        tokenized_input = T.tensor(tokenizer.encode(query)).unsqueeze(0).to(device)\n",
        "        generated_text = ''\n",
        "        generated_cq = ''\n",
        "        \n",
        "        sample_outputs = model.generate(\n",
        "                tokenized_input,\n",
        "                do_sample=True,   \n",
        "                top_k=20, \n",
        "                max_length = len(tokenized_input[0]) + 32,\n",
        "                top_p=0.9, \n",
        "                temperature = temperature,\n",
        "                num_return_sequences=1,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        generated_text = tokenizer.decode(sample_outputs[0], skip_special_tokens=True)\n",
        "        generated_cq = generated_text[len(query):].strip()\n",
        "        generated_cq = re.sub('\\[SEP\\]', ' ', generated_cq).strip()\n",
        "        generated_cq = re.sub('[.?]', '&', generated_cq).split('&')[0].strip()\n",
        "\n",
        "        tokenized_ref = word_tokenize(ref)\n",
        "        clariq_f_test_data.at[iter, 'generated'] = generated_cq\n",
        "        tokenized_hyp = word_tokenize(clariq_f_test_data.at[iter, 'generated'])\n",
        "\n",
        "        if iter % 50 == 0: \n",
        "            print(iter, query, '-', generated_cq)\n",
        "\n",
        "        rs_list.append(rs.get_scores(generated_cq, ref)[0]['rouge-l']['f'])\n",
        "        \n",
        "        b1.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(1, 0, 0, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b2.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 1, 0, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b3.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 0, 1, 0),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "        b4.append(sentence_bleu([tokenized_ref], \n",
        "                                tokenized_hyp, \n",
        "                                weights=(0, 0, 0, 1),\n",
        "                                smoothing_function = SmoothingFunction().method1\n",
        "                                ))\n",
        "\n",
        "        m.append(meteor_score([tokenized_ref], tokenized_hyp))\n",
        "\n",
        "    output_df = clariq_f_test_data[['initial_request', 'facet_desc', 'question', 'generated']]\n",
        "    output_df.columns = ['query', 'facet', 'reference', 'candidate']\n",
        "    output_df.to_csv(model_output)\n",
        "    \n",
        "print(\"b1\", np.mean(b1), \"b2\", np.mean(b2), \"b3\", np.mean(b3), \"b4\", np.mean(b4))\n",
        "print(\"rouge-L\", np.mean(rs_list))\n",
        "print(\"m\", np.mean(m))\n",
        "print(\"c\", np.mean(c))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Concatenate them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_24914/3194762248.py:35: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  trial_data.at[k, 'generation'+str(j)] = index_to_df[perm[j]][k]\n"
          ]
        }
      ],
      "source": [
        "zerogpt2_output = 'zeroshot_trial.csv'\n",
        "gpt3_output = 'zeroshotgpt3_trial.csv'\n",
        "ftgpt2_output = 'ftgpt2_trial.csv'\n",
        "\n",
        "zerogpt2_output_data = pd.read_csv(zerogpt2_output)\n",
        "gpt3_output_data = pd.read_csv(gpt3_output)\n",
        "ftgpt2_output_data = pd.read_csv(ftgpt2_output)\n",
        "\n",
        "assert len(gpt3_output_data) == len(zerogpt2_output_data) and len(gpt3_output_data) == len(ftgpt2_output_data)\n",
        "\n",
        "trial_data = gpt3_output_data[['query', 'facet', 'reference']]\n",
        "zerogpt2_candidates = zerogpt2_output_data['candidate'].tolist()\n",
        "gpt3_candidates = gpt3_output_data['candidate'].tolist()\n",
        "ftgpt2_candidates = ftgpt2_output_data['candidate'].tolist()\n",
        "\n",
        "index_to_model = {\n",
        "    0: \"zeroshotgpt2\",\n",
        "    1: \"oneshotgpt3\",\n",
        "    2: \"finetunegpt2\"\n",
        "}\n",
        "\n",
        "index_to_df = {\n",
        "    0: zerogpt2_candidates,\n",
        "    1: gpt3_candidates,\n",
        "    2: ftgpt2_candidates\n",
        "}\n",
        "\n",
        "n_models = len(index_to_model.keys())\n",
        "indices = []\n",
        "\n",
        "for k in range(len(zerogpt2_candidates)):\n",
        "    perm = np.random.permutation(n_models)\n",
        "    indices.append(perm.tolist())\n",
        "    for j in range(n_models):\n",
        "        trial_data.at[k, 'generation'+str(j)] = index_to_df[perm[j]][k]\n",
        "\n",
        "np.savetxt('trial_indices', np.array(indices), fmt='%d', delimiter=' ')\n",
        "trial_data.to_csv(\"trial_data.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Comparing our generation with SIGIR'22 paper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## processing their data and generate files for neurologic decoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "original_output = pd.read_csv('srqg-gen_results.txt', sep='\\t', header=None)\n",
        "original_output.columns = ['initial_request', 'facet_desc', 'srqg-gen']\n",
        "for iter, row in original_output.iterrows():\n",
        "    processed_facets = re.sub('[\\[\\]\\']','', original_output.at[iter, 'facet_desc'] )\n",
        "    processed_facets = re.sub('<unknown>', '', processed_facets) # removing empty facet\n",
        "    original_output.at[iter, 'facet_desc'] = ','.join([facet.strip() for facet in processed_facets.split(',') if facet.strip() != '']).strip()\n",
        "original_output.to_csv('data/srqg/test.tsv', sep = '\\t')\n",
        "\n",
        "######################################################\n",
        "\n",
        "from word_forms.word_forms import get_word_forms\n",
        "facet_test_file = 'data/srqg/test.tsv'\n",
        "facet_test_data = pd.read_csv(facet_test_file, sep='\\t')\n",
        "\n",
        "write_to_file = 'neurologic_decoding/dataset/clean/constraint/test.constraint.json'\n",
        "prompt_write_to_file = 'neurologic_decoding/dataset/clean/init/commongen.test.init.txt'\n",
        "no_prompt_write_to_file = 'neurologic_decoding/dataset/clean/init/commongen_no_prompt.test.init.txt'\n",
        "\n",
        "pos_tagger = spacy.load('en_core_web_sm')\n",
        "\n",
        "all_queries= []\n",
        "all_constraints = []\n",
        "for iter, row in facet_test_data.iterrows():\n",
        "    facet = facet_test_data.at[iter, 'facet_desc']\n",
        "    query = facet_test_data.at[iter, 'initial_request']\n",
        "    noun_in_query = [token.text for token in pos_tagger(query) if token.pos_ == 'NOUN']\n",
        "    propn_in_query = [token.text for token in pos_tagger(query) if token.pos_ == 'PROPN']\n",
        "\n",
        "    for term in facet.split(','):\n",
        "        if term.strip() != '':\n",
        "            all_queries.append(query)\n",
        "            all_constraints.append([[term.strip()]])\n",
        "\n",
        "\n",
        "with open(write_to_file, 'w') as output:\n",
        "    for constraints in all_constraints:\n",
        "        for k, prompt in enumerate(starting_texts):\n",
        "            json_str = json.dumps(constraints)\n",
        "            output.write(json_str)\n",
        "            output.write('\\n')\n",
        "\n",
        "with open(prompt_write_to_file, 'w') as output:\n",
        "    for query in all_queries:\n",
        "        for k, prompt in enumerate(starting_texts):\n",
        "            output.write(query + prompt)\n",
        "            output.write('\\n')\n",
        "\n",
        "with open(no_prompt_write_to_file, 'w') as output:\n",
        "    for query in all_queries:\n",
        "        output.write(query)\n",
        "        output.write('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 google chrome exe - 64 bit,32 bit - are you looking for 64 bit or 32 bit\n",
            "[('are you looking for 64 bit or 32 bit', 0.1678720668595307),\n",
            " ('are you interested in the 64 bit version', 0.14265606888557708),\n",
            " ('would you like to see the 64 bit version', 0.13846030215364835),\n",
            " ('do you need information about the 64 bit version', 0.13846030215364835),\n",
            " ('do you want information about the 64 bit version', 0.13846030215364835),\n",
            " ('do you need to install 64 bit on your computer', 0.13450429352068696),\n",
            " ('do you want to know the 64 bit version of Chrome', 0.13076806314511233),\n",
            " ('do you want to install 64 bit Chrome on your computer', 0.13076806314511233)]\n",
            "50 romans 9 - female,male - are you looking for\n",
            "[('are you looking for', 0.0),\n",
            " ('do you want to know what nlt is', 0.0),\n",
            " ('would you like to see the nltv', 0.0),\n",
            " ('are you interested in joining nltv', 0.0),\n",
            " ('do you need information about the nltv', 0.0),\n",
            " ('do you want information about the nltv', 0.0),\n",
            " ('do you need to use nltools to do this', 0.0),\n",
            " ('do you want to join nlt', 0.0)]\n"
          ]
        }
      ],
      "source": [
        "from word_forms.word_forms import get_word_forms\n",
        "facet_test_file = 'data/srqg/test.tsv'\n",
        "facet_test_data = pd.read_csv(facet_test_file, sep='\\t')\n",
        "\n",
        "model_output = 'zeroshot_srqg_new.csv'\n",
        "\n",
        "generated_file = 'neurologic_decoding/zero_shot/gpt2newsrqg'\n",
        "generated_cq_all_templates = open(generated_file, 'r').readlines()\n",
        "generated_cq_grouped = [[generated_cq_all_templates[len(starting_texts) * k + l] \n",
        "                            for l in range(len(starting_texts))] \n",
        "                            for k in range(int(len(generated_cq_all_templates)/8))]\n",
        "for iter, row in facet_test_data.iterrows():\n",
        "    facet = facet_test_data.at[iter, 'facet_desc']\n",
        "    force_flexible = facet.split()\n",
        "\n",
        "    generated_cqs = []\n",
        "    for full_sentence in generated_cq_grouped[iter]:\n",
        "        query = re.sub('\\[SEP\\]', '&', full_sentence).split('&')[0].strip()\n",
        "        generated_follow_up = re.sub('\\[SEP\\]', '&', full_sentence).split('&')[1].strip()\n",
        "        generated_cq = re.sub('[.?]', '&', generated_follow_up).split('&')[0].strip()\n",
        "        generated_cqs.append(generated_cq)\n",
        "    \n",
        "    template_scores = calculate_WSDM(query=facet, doc_list=generated_cqs)\n",
        "    \n",
        "    sorted_template_scores = sorted(template_scores.items(), key = lambda x: x[1], reverse=True)\n",
        "\n",
        "    generated_cq = sorted(template_scores.keys(), key = lambda x: template_scores[x], reverse=True)[0] \n",
        "\n",
        "    facet_test_data.at[iter, 'generated'] = generated_cq\n",
        "    tokenized_hyp = word_tokenize(facet_test_data.at[iter, 'generated'])\n",
        "    \n",
        "    \n",
        "    if iter % 50 == 0: \n",
        "        print(iter, query, \"-\", facet, '-', generated_cq)\n",
        "        pprint.pprint(sorted_template_scores)\n",
        "   \n",
        "\n",
        "output_df = facet_test_data[['initial_request', 'facet_desc', 'generated']]\n",
        "output_df.columns = ['query', 'facet', 'candidate']\n",
        "output_df.to_csv(model_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Compute trial set agreement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Zerogpt2\n",
            "By major vote\n",
            "Naturalness Counter({'fair': 43, 'bad': 34, 'good': 22})\n",
            "Usefulness Counter({'good': 55, 'bad': 25, 'fair': 19})\n",
            "By all vote\n",
            "Naturalness Counter({'bad': 106, 'fair': 105, 'good': 86})\n",
            "Usefulness Counter({'good': 154, 'bad': 78, 'fair': 65})\n",
            "Agreements\n",
            "Naturalness\n",
            "mi_yc 0.40839754678408546\n",
            "mi_zd 0.16238681905892838\n",
            "zd_yc 0.1643218908272368\n",
            "mi_mv 0.7795801526717557\n",
            "zd_mv 0.3622471910112359\n",
            "yc_mv 0.5996266915538964\n",
            "Usefulness\n",
            "mi_yc 0.10483187341056799\n",
            "mi_zd 0.3437317215831547\n",
            "zd_yc 0.2172767203513909\n",
            "mi_mv 0.4609101516919487\n",
            "zd_mv 0.8117219917012448\n",
            "yc_mv 0.37598944591029027\n",
            "gpt3\n",
            "By major vote\n",
            "Naturalness Counter({'good': 90, 'bad': 6, 'fair': 3})\n",
            "Usefulness Counter({'good': 76, 'bad': 22, 'fair': 1})\n",
            "By all vote\n",
            "Naturalness Counter({'good': 260, 'bad': 21, 'fair': 16})\n",
            "Usefulness Counter({'good': 218, 'bad': 65, 'fair': 14})\n",
            "Agreements\n",
            "Naturalness\n",
            "mi_yc 0.4698795180722891\n",
            "mi_zd 0.6333333333333333\n",
            "zd_yc 0.425189816882537\n",
            "mi_mv 0.8450704225352113\n",
            "zd_mv 0.7409733124018838\n",
            "yc_mv 0.5547226386806596\n",
            "Usefulness\n",
            "mi_yc 0.5953678474114441\n",
            "mi_zd 0.6004036326942482\n",
            "zd_yc 0.5655989469065379\n",
            "Finetune gpt2\n",
            "By major vote\n",
            "Naturalness Counter({'bad': 69, 'good': 21, 'fair': 9})\n",
            "Usefulness Counter({'bad': 59, 'good': 26, 'fair': 14})\n",
            "By all vote\n",
            "Naturalness Counter({'bad': 187, 'good': 62, 'fair': 48})\n",
            "Usefulness Counter({'bad': 166, 'good': 85, 'fair': 46})\n",
            "Agreements\n",
            "Naturalness\n",
            "mi_yc 0.4298642533936652\n",
            "mi_zd 0.526141384388807\n",
            "zd_yc 0.35129231889333823\n",
            "Usefulness\n",
            "mi_yc 0.2801826175177544\n",
            "mi_zd 0.5197628458498025\n",
            "zd_yc 0.3547169811320754\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "from collections import Counter\n",
        "\n",
        "zd_df = pd.read_csv(\"trial_data_zd.csv\")\n",
        "yc_df = pd.read_csv(\"trial_data_yc.csv\")\n",
        "mi_df = pd.read_csv(\"trial_data_mi.csv\")\n",
        "\n",
        "zerogpt2_df = pd.read_csv(\"zeroshot_trial.csv\")\n",
        "gpt3_df = pd.read_csv(\"zeroshotgpt3_trial.csv\")\n",
        "ftgpt2_df = pd.read_csv(\"ftgpt2_trial.csv\")\n",
        "\n",
        "labeler_df = mi_df\n",
        "for iter, row in labeler_df.iterrows():\n",
        "    row = [0,1,2]\n",
        "    for k in range(3):\n",
        "        if labeler_df.at[iter, 'generation'+str(k)] == zerogpt2_df.at[iter,'candidate']:\n",
        "            zerogpt2_df.at[iter, 'mi_naturalness'] = labeler_df.at[iter, 'Naturalness'+str(k)].strip()\n",
        "            zerogpt2_df.at[iter, 'mi_usefulness'] = labeler_df.at[iter, 'Usefulness'+str(k)].strip()\n",
        "            row.remove(k)\n",
        "        if labeler_df.at[iter, 'generation'+str(k)] == gpt3_df.at[iter,'candidate']:\n",
        "            gpt3_df.at[iter, 'mi_naturalness'] = labeler_df.at[iter, 'Naturalness'+str(k)].strip()\n",
        "            gpt3_df.at[iter, 'mi_usefulness'] = labeler_df.at[iter, 'Usefulness'+str(k)].strip()\n",
        "            row.remove(k)\n",
        "        if labeler_df.at[iter, 'generation'+str(k)] == ftgpt2_df.at[iter,'candidate']:\n",
        "            ftgpt2_df.at[iter, 'mi_naturalness'] = labeler_df.at[iter, 'Naturalness'+str(k)].strip()\n",
        "            ftgpt2_df.at[iter, 'mi_usefulness'] = labeler_df.at[iter, 'Usefulness'+str(k)].strip()\n",
        "            row.remove(k)\n",
        "    \n",
        "    assert len(row) == 0\n",
        "\n",
        "labeler_df = yc_df\n",
        "for iter, row in labeler_df.iterrows():\n",
        "    row = [0,1,2]\n",
        "    for k in range(3):\n",
        "        if labeler_df.at[iter, 'generation'+str(k)] == zerogpt2_df.at[iter,'candidate']:\n",
        "            zerogpt2_df.at[iter, 'yc_naturalness'] = labeler_df.at[iter, 'Naturalness'+str(k)].strip()\n",
        "            zerogpt2_df.at[iter, 'yc_usefulness'] = labeler_df.at[iter, 'Usefulness'+str(k)].strip()\n",
        "            row.remove(k)\n",
        "        if labeler_df.at[iter, 'generation'+str(k)] == gpt3_df.at[iter,'candidate']:\n",
        "            gpt3_df.at[iter, 'yc_naturalness'] = labeler_df.at[iter, 'Naturalness'+str(k)].strip()\n",
        "            gpt3_df.at[iter, 'yc_usefulness'] = labeler_df.at[iter, 'Usefulness'+str(k)].strip()\n",
        "            row.remove(k)\n",
        "        if labeler_df.at[iter, 'generation'+str(k)] == ftgpt2_df.at[iter,'candidate']:\n",
        "            ftgpt2_df.at[iter, 'yc_naturalness'] = labeler_df.at[iter, 'Naturalness'+str(k)].strip()\n",
        "            ftgpt2_df.at[iter, 'yc_usefulness'] = labeler_df.at[iter, 'Usefulness'+str(k)].strip()\n",
        "            row.remove(k)\n",
        "    \n",
        "    assert len(row) == 0\n",
        "\n",
        "labeler_df = zd_df\n",
        "for iter, row in labeler_df.iterrows():\n",
        "    row = [0,1,2]\n",
        "    for k in range(3):\n",
        "        if labeler_df.at[iter, 'generation'+str(k)] == zerogpt2_df.at[iter,'candidate']:\n",
        "            zerogpt2_df.at[iter, 'zd_naturalness'] = labeler_df.at[iter, 'Naturalness'+str(k)].strip()\n",
        "            zerogpt2_df.at[iter, 'zd_usefulness'] = labeler_df.at[iter, 'Usefulness'+str(k)].strip()\n",
        "            row.remove(k)\n",
        "        if labeler_df.at[iter, 'generation'+str(k)] == gpt3_df.at[iter,'candidate']:\n",
        "            gpt3_df.at[iter, 'zd_naturalness'] = labeler_df.at[iter, 'Naturalness'+str(k)].strip()\n",
        "            gpt3_df.at[iter, 'zd_usefulness'] = labeler_df.at[iter, 'Usefulness'+str(k)].strip()\n",
        "            row.remove(k)\n",
        "        if labeler_df.at[iter, 'generation'+str(k)] == ftgpt2_df.at[iter,'candidate']:\n",
        "            ftgpt2_df.at[iter, 'zd_naturalness'] = labeler_df.at[iter, 'Naturalness'+str(k)].strip()\n",
        "            ftgpt2_df.at[iter, 'zd_usefulness'] = labeler_df.at[iter, 'Usefulness'+str(k)].strip()\n",
        "            row.remove(k)\n",
        "    \n",
        "    assert len(row) == 0\n",
        "\n",
        "zerogpt2_df.to_csv(\"zeroshot_trial_labeled.csv\")\n",
        "gpt3_df.to_csv(\"fewshotgpt3_trial_labeled.csv\")\n",
        "ftgpt2_df.to_csv(\"ftgpt2_trial_labeled.csv\")\n",
        "\n",
        "zerogpt2_zipped_naturalness = zip(zerogpt2_df['mi_naturalness'].tolist(), zerogpt2_df['zd_naturalness'].tolist(), zerogpt2_df['yc_naturalness'].tolist())\n",
        "zerogpt2_zipped_usefulness = zip(zerogpt2_df['mi_usefulness'].tolist(), zerogpt2_df['zd_usefulness'].tolist(), zerogpt2_df['yc_usefulness'].tolist())\n",
        "zerogpt2_naturalness_major_vote = [Counter(list(labels)).most_common()[0][0] if Counter(list(labels)).most_common()[0][1] > 1 else 'fair' for labels in zerogpt2_zipped_naturalness]\n",
        "zerogpt2_usefulness_major_vote = [Counter(list(labels)).most_common()[0][0] if Counter(list(labels)).most_common()[0][1] > 1 else 'fair' for labels in zerogpt2_zipped_usefulness]\n",
        "\n",
        "print(\"Zerogpt2\")\n",
        "print(\"By major vote\")\n",
        "print(\"Naturalness\", Counter(zerogpt2_naturalness_major_vote))\n",
        "print(\"Usefulness\", Counter(zerogpt2_usefulness_major_vote))\n",
        "print(\"By all vote\")\n",
        "print(\"Naturalness\", Counter(zerogpt2_df['mi_naturalness'].tolist() + zerogpt2_df['zd_naturalness'].tolist() + zerogpt2_df['yc_naturalness'].tolist()))\n",
        "print(\"Usefulness\", Counter(zerogpt2_df['mi_usefulness'].tolist() + zerogpt2_df['zd_usefulness'].tolist() + zerogpt2_df['yc_usefulness'].tolist()))\n",
        "print(\"Agreements\")\n",
        "print(\"Naturalness\")\n",
        "print(\"mi_yc\", cohen_kappa_score(zerogpt2_df['mi_naturalness'].tolist(), zerogpt2_df['yc_naturalness'].tolist()))\n",
        "print(\"mi_zd\", cohen_kappa_score(zerogpt2_df['mi_naturalness'].tolist(), zerogpt2_df['zd_naturalness'].tolist()))\n",
        "print(\"zd_yc\", cohen_kappa_score(zerogpt2_df['zd_naturalness'].tolist(), zerogpt2_df['yc_naturalness'].tolist()))\n",
        "print(\"mi_mv\", cohen_kappa_score(zerogpt2_df['mi_naturalness'].tolist(), zerogpt2_naturalness_major_vote))\n",
        "print(\"zd_mv\", cohen_kappa_score(zerogpt2_df['zd_naturalness'].tolist(), zerogpt2_naturalness_major_vote))\n",
        "print(\"yc_mv\", cohen_kappa_score(zerogpt2_df['yc_naturalness'].tolist(), zerogpt2_naturalness_major_vote))\n",
        "\n",
        "print(\"Usefulness\")\n",
        "print(\"mi_yc\", cohen_kappa_score(zerogpt2_df['mi_usefulness'].tolist(), zerogpt2_df['yc_usefulness'].tolist()))\n",
        "print(\"mi_zd\", cohen_kappa_score(zerogpt2_df['mi_usefulness'].tolist(), zerogpt2_df['zd_usefulness'].tolist()))\n",
        "print(\"zd_yc\", cohen_kappa_score(zerogpt2_df['zd_usefulness'].tolist(), zerogpt2_df['yc_usefulness'].tolist()))\n",
        "print(\"mi_mv\", cohen_kappa_score(zerogpt2_df['mi_usefulness'].tolist(), zerogpt2_usefulness_major_vote))\n",
        "print(\"zd_mv\", cohen_kappa_score(zerogpt2_df['zd_usefulness'].tolist(), zerogpt2_usefulness_major_vote))\n",
        "print(\"yc_mv\", cohen_kappa_score(zerogpt2_df['yc_usefulness'].tolist(), zerogpt2_usefulness_major_vote))\n",
        "\n",
        "\n",
        "gpt3_zipped_naturalness = zip(gpt3_df['mi_naturalness'].tolist(), gpt3_df['zd_naturalness'].tolist(), gpt3_df['yc_naturalness'].tolist())\n",
        "gpt3_zipped_usefulness = zip(gpt3_df['mi_usefulness'].tolist(), gpt3_df['zd_usefulness'].tolist(), gpt3_df['yc_usefulness'].tolist())\n",
        "gpt3_naturalness_major_vote = [Counter(list(labels)).most_common()[0][0] if Counter(list(labels)).most_common()[0][1] > 1 else 'fair' for labels in gpt3_zipped_naturalness]\n",
        "gpt3_usefulness_major_vote = [Counter(list(labels)).most_common()[0][0] if Counter(list(labels)).most_common()[0][1] > 1 else 'fair' for labels in gpt3_zipped_usefulness]\n",
        "\n",
        "print(\"gpt3\")\n",
        "print(\"By major vote\")\n",
        "print(\"Naturalness\", Counter(gpt3_naturalness_major_vote))\n",
        "print(\"Usefulness\", Counter(gpt3_usefulness_major_vote))\n",
        "print(\"By all vote\")\n",
        "print(\"Naturalness\", Counter(gpt3_df['mi_naturalness'].tolist() + gpt3_df['zd_naturalness'].tolist() + gpt3_df['yc_naturalness'].tolist()))\n",
        "print(\"Usefulness\", Counter(gpt3_df['mi_usefulness'].tolist() + gpt3_df['zd_usefulness'].tolist() + gpt3_df['yc_usefulness'].tolist()))\n",
        "print(\"Agreements\")\n",
        "print(\"Naturalness\")\n",
        "print(\"mi_yc\", cohen_kappa_score(gpt3_df['mi_naturalness'].tolist(), gpt3_df['yc_naturalness'].tolist()))\n",
        "print(\"mi_zd\", cohen_kappa_score(gpt3_df['mi_naturalness'].tolist(), gpt3_df['zd_naturalness'].tolist()))\n",
        "print(\"zd_yc\", cohen_kappa_score(gpt3_df['zd_naturalness'].tolist(), gpt3_df['yc_naturalness'].tolist()))\n",
        "print(\"mi_mv\", cohen_kappa_score(gpt3_df['mi_naturalness'].tolist(), gpt3_naturalness_major_vote))\n",
        "print(\"zd_mv\", cohen_kappa_score(gpt3_df['zd_naturalness'].tolist(), gpt3_naturalness_major_vote))\n",
        "print(\"yc_mv\", cohen_kappa_score(gpt3_df['yc_naturalness'].tolist(), gpt3_naturalness_major_vote))\n",
        "\n",
        "print(\"Usefulness\")\n",
        "print(\"mi_yc\", cohen_kappa_score(gpt3_df['mi_usefulness'].tolist(), gpt3_df['yc_usefulness'].tolist()))\n",
        "print(\"mi_zd\", cohen_kappa_score(gpt3_df['mi_usefulness'].tolist(), gpt3_df['zd_usefulness'].tolist()))\n",
        "print(\"zd_yc\", cohen_kappa_score(gpt3_df['zd_usefulness'].tolist(), gpt3_df['yc_usefulness'].tolist()))\n",
        "\n",
        "ftgpt2_zipped_naturalness = zip(ftgpt2_df['mi_naturalness'].tolist(), ftgpt2_df['zd_naturalness'].tolist(), ftgpt2_df['yc_naturalness'].tolist())\n",
        "ftgpt2_zipped_usefulness = zip(ftgpt2_df['mi_usefulness'].tolist(), ftgpt2_df['zd_usefulness'].tolist(), ftgpt2_df['yc_usefulness'].tolist())\n",
        "ftgpt2_naturalness_major_vote = [Counter(list(labels)).most_common()[0][0] if Counter(list(labels)).most_common()[0][1] > 1 else 'fair' for labels in ftgpt2_zipped_naturalness]\n",
        "ftgpt2_usefulness_major_vote = [Counter(list(labels)).most_common()[0][0] if Counter(list(labels)).most_common()[0][1] > 1 else 'fair' for labels in ftgpt2_zipped_usefulness]\n",
        "\n",
        "print(\"Finetune gpt2\")\n",
        "print(\"By major vote\")\n",
        "print(\"Naturalness\", Counter(ftgpt2_naturalness_major_vote))\n",
        "print(\"Usefulness\", Counter(ftgpt2_usefulness_major_vote))\n",
        "print(\"By all vote\")\n",
        "print(\"Naturalness\", Counter(ftgpt2_df['mi_naturalness'].tolist() + ftgpt2_df['zd_naturalness'].tolist() + ftgpt2_df['yc_naturalness'].tolist()))\n",
        "print(\"Usefulness\", Counter(ftgpt2_df['mi_usefulness'].tolist() + ftgpt2_df['zd_usefulness'].tolist() + ftgpt2_df['yc_usefulness'].tolist()))\n",
        "print(\"Agreements\")\n",
        "print(\"Naturalness\")\n",
        "print(\"mi_yc\", cohen_kappa_score(ftgpt2_df['mi_naturalness'].tolist(), ftgpt2_df['yc_naturalness'].tolist()))\n",
        "print(\"mi_zd\", cohen_kappa_score(ftgpt2_df['mi_naturalness'].tolist(), ftgpt2_df['zd_naturalness'].tolist()))\n",
        "print(\"zd_yc\", cohen_kappa_score(ftgpt2_df['zd_naturalness'].tolist(), ftgpt2_df['yc_naturalness'].tolist()))\n",
        "\n",
        "print(\"Usefulness\")\n",
        "print(\"mi_yc\", cohen_kappa_score(ftgpt2_df['mi_usefulness'].tolist(), ftgpt2_df['yc_usefulness'].tolist()))\n",
        "print(\"mi_zd\", cohen_kappa_score(ftgpt2_df['mi_usefulness'].tolist(), ftgpt2_df['zd_usefulness'].tolist()))\n",
        "print(\"zd_yc\", cohen_kappa_score(ftgpt2_df['zd_usefulness'].tolist(), ftgpt2_df['yc_usefulness'].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Generate human annotation file for RQ2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_5806/998677384.py:35: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  trial_data.at[k, 'generation'+str(j)] = index_to_df[perm[j]][k]\n"
          ]
        }
      ],
      "source": [
        "taf_output_data = pd.read_csv('template_facet.csv')\n",
        "sekulic_output_data = pd.read_csv('sekulic.csv')\n",
        "ftgpt2_output_data = pd.read_csv('ftgpt2_prompt_temp0.1.csv')\n",
        "zerogpt2_output_data = pd.read_csv('zeroshot_nd_wsdm.csv')\n",
        "\n",
        "assert len(taf_output_data) == len(sekulic_output_data) and len(sekulic_output_data) == len(ftgpt2_output_data) and len(ftgpt2_output_data) == len(zerogpt2_output_data)\n",
        "\n",
        "trial_data = zerogpt2_output_data[['query', 'facet', 'reference']]\n",
        "taf_candidates = taf_output_data['candidate'].tolist()\n",
        "sekulic_candidates = sekulic_output_data['candidate'].tolist()\n",
        "ftgpt2_candidates = ftgpt2_output_data['candidate'].tolist()\n",
        "zerogpt2_candidates = zerogpt2_output_data['candidate'].tolist()\n",
        "\n",
        "index_to_model = {\n",
        "    0: \"taf\",\n",
        "    1: \"sekulic\",\n",
        "    2: \"prompt\",\n",
        "    3: \"zeroshot\"\n",
        "}\n",
        "\n",
        "index_to_df = {\n",
        "    0: taf_candidates,\n",
        "    1: sekulic_candidates,\n",
        "    2: ftgpt2_candidates,\n",
        "    3: zerogpt2_candidates\n",
        "}\n",
        "\n",
        "n_models = len(index_to_model.keys())\n",
        "indices = []\n",
        "\n",
        "for k in range(len(zerogpt2_candidates)):\n",
        "    perm = np.random.permutation(n_models)\n",
        "    indices.append(perm.tolist())\n",
        "    for j in range(n_models):\n",
        "        trial_data.at[k, 'generation'+str(j)] = index_to_df[perm[j]][k]\n",
        "\n",
        "np.savetxt('trial_indices', np.array(indices), fmt='%d', delimiter=' ')\n",
        "trial_data.to_csv(\"human.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================\n",
            "Full reference evaluation\n",
            "================================================================\n",
            "b1 0.04311297961020846 b2 0.006631606205278626 b3 0.004148754940587422 b4 0.00432941193005007\n",
            "rouge-L 0.052052737118538465\n",
            "m 0.055976241870738526\n",
            "c 0.013333333333333334\n",
            "================================================================\n",
            "Question body evaluation\n",
            "================================================================\n",
            "b1 0.023189283411068365 b2 0.0040683780339581446 b3 0.0034799485175119427 b4 0.00405636676922024\n",
            "rouge-L 0.03308281607841434\n",
            "m 0.03714642349666363\n",
            "c 0.009215686274509804\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline, set_seed\n",
        "generator = pipeline('text-generation', model='gpt2')\n",
        "set_seed(42)\n",
        "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=1)\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "sample_every = 100\n",
        "epochs = 8\n",
        "learning_rate = 5e-5\n",
        "warmup_steps = 1e2\n",
        "epsilon = 1e-8\n",
        "max_length = 128\n",
        "prompt_instruction = ''\n",
        "\n",
        "facet_test_file = 'data/clariq_f/ClariQ-FKw-dev.tsv'\n",
        "facet_test_data = pd.read_csv(facet_test_file, sep='\\t')\n",
        "_, facet_test_data = process_clariq_f(facet_test_data)\n",
        "\n",
        "b1, b2, b3, b4 = [], [], [], []\n",
        "m = []\n",
        "r = []\n",
        "c = []\n",
        "\n",
        "t_b1, t_b2, t_b3, t_b4 = [], [], [], []\n",
        "t_m = []\n",
        "t_r = []\n",
        "t_c = []\n",
        "\n",
        "model_output = 'gpt2.csv'\n",
        "\n",
        "if os.path.isfile(model_output):\n",
        "    b1, b2, b3, b4, m, r, c, t_b1, t_b2, t_b3, t_b4, t_m, t_r, t_c = evaluate_from_output(model_output)\n",
        "\n",
        "else:\n",
        "    clariq_f_train_file = 'data/clariq_f/ClariQ-FKw-train.tsv'\n",
        "    clariq_f_train_data = pd.read_csv(clariq_f_train_file, sep='\\t') \n",
        "    clariq_f_train_dict, clariq_f_train_data = process_clariq_f(clariq_f_train_data)\n",
        "    clariq_f_train_text_list = clariq_f_train_data['f_q_cq']\n",
        "\n",
        "    clariq_f_test_file = 'data/clariq_f/ClariQ-FKw-dev.tsv'\n",
        "    clariq_f_test_data = pd.read_csv(clariq_f_test_file, sep='\\t') \n",
        "    clariq_f_test_dict, clariq_f_test_data = process_clariq_f(clariq_f_test_data)\n",
        "\n",
        "    rs = rouge.Rouge()\n",
        "    rs_list = []\n",
        "    b1, b2, b3, b4 = [], [], [], []\n",
        "    m = []\n",
        "    \n",
        "    for iter, row in clariq_f_test_data.iterrows():\n",
        "        query = clariq_f_test_data.at[iter, 'initial_request'] + BOS\n",
        "        ref = clariq_f_test_data.at[iter, 'question']\n",
        "        facet = clariq_f_test_data.at[iter, 'facet_desc']\n",
        "\n",
        "        sample_outputs = generator(query, max_length=len(query) + 30, num_return_sequences=1)\n",
        "        \n",
        "        generated_text = sample_outputs[0]['generated_text']\n",
        "        generated_cq = generated_text[len(query):]\n",
        "        generated_cq = re.sub('\\[SEP\\]', ' ', generated_cq).strip()\n",
        "        generated_cq = re.sub('[.?\\n]', '&', generated_cq).split('&')[0].strip()\n",
        "\n",
        "        clariq_f_test_data.at[iter, 'generated'] = process_generation(generated_cq)\n",
        "\n",
        "\n",
        "        # full reference evaluation\n",
        "        hyp_b1, hyp_b2, hyp_b3, hyp_b4, hyp_m, hyp_r, hyp_c = auto_evaluation(ref, generated_cq, facet)\n",
        "\n",
        "        b1.append(hyp_b1)\n",
        "        b2.append(hyp_b2)\n",
        "        b3.append(hyp_b3)\n",
        "        b4.append(hyp_b4)\n",
        "        m.append(hyp_m)\n",
        "        r.append(hyp_r)\n",
        "        c.append(hyp_c)\n",
        "\n",
        "        # question body evaluation\n",
        "        truncate_ref = ' '.join(ref.split()[template_len:])\n",
        "        truncate_generated_cq = ' '.join(generated_cq.split()[template_len:])\n",
        "        \n",
        "        t_hyp_b1, t_hyp_b2, t_hyp_b3, t_hyp_b4, t_hyp_m, t_hyp_r, t_hyp_c = auto_evaluation(truncate_ref, truncate_generated_cq, facet)\n",
        "\n",
        "        t_b1.append(t_hyp_b1)\n",
        "        t_b2.append(t_hyp_b2)\n",
        "        t_b3.append(t_hyp_b3)\n",
        "        t_b4.append(t_hyp_b4)\n",
        "        t_m.append(t_hyp_m)\n",
        "        t_r.append(t_hyp_r)\n",
        "        t_c.append(t_hyp_c)\n",
        "\n",
        "    output_df = clariq_f_test_data[['initial_request', 'facet_desc', 'question', 'generated']]\n",
        "    output_df.columns = ['query', 'facet', 'reference', 'candidate']\n",
        "    output_df.to_csv(model_output)\n",
        "\n",
        "# full reference results\n",
        "print(\"================================================================\")\n",
        "print(\"Full reference evaluation\")\n",
        "print(\"================================================================\")\n",
        "print(\"b1\", np.mean(b1), \"b2\", np.mean(b2), \"b3\", np.mean(b3), \"b4\", np.mean(b4))\n",
        "print(\"rouge-L\", np.mean(r))\n",
        "print(\"m\", np.mean(m))\n",
        "print(\"c\", np.mean(c))\n",
        "\n",
        "sekulic0_b1 = np.mean(b1)\n",
        "sekulic0_b2 = np.mean(b2)\n",
        "sekulic0_b3 = np.mean(b3)\n",
        "sekulic0_b4 = np.mean(b4)\n",
        "sekulic0_m = np.mean(m)\n",
        "sekulic0_r = np.mean(r)\n",
        "sekulic0_c = np.mean(c)\n",
        "\n",
        "# question body results\n",
        "print(\"================================================================\")\n",
        "print(\"Question body evaluation\")\n",
        "print(\"================================================================\")\n",
        "print(\"b1\", np.mean(t_b1), \"b2\", np.mean(t_b2), \"b3\", np.mean(t_b3), \"b4\", np.mean(t_b4))\n",
        "print(\"rouge-L\", np.mean(t_r))\n",
        "print(\"m\", np.mean(t_m))\n",
        "print(\"c\", np.mean(t_c))\n",
        "\n",
        "t_sekulic0_b1 = np.mean(t_b1)\n",
        "t_sekulic0_b2 = np.mean(t_b2)\n",
        "t_sekulic0_b3 = np.mean(t_b3)\n",
        "t_sekulic0_b4 = np.mean(t_b4)\n",
        "t_sekulic0_m = np.mean(t_m)\n",
        "t_sekulic0_r = np.mean(t_r)\n",
        "t_sekulic0_c = np.mean(t_c)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multiple template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================\n",
            "Full reference evaluation\n",
            "================================================================\n",
            "b1 0.21265806579183102 b2 0.09247087817195276 b3 0.05440921170863705 b4 0.03858218109211079\n",
            "rouge-L 0.2758348540127664\n",
            "m 0.24035101874009449\n",
            "c 0.06858823529411766\n",
            "================================================================\n",
            "Question body evaluation\n",
            "================================================================\n",
            "b1 0.07888041389072298 b2 0.025424614734841443 b3 0.012360211398353017 b4 0.011564095956348113\n",
            "rouge-L 0.10652104329201147\n",
            "m 0.11132045940816086\n",
            "c 0.06270588235294117\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline, set_seed\n",
        "generator = pipeline('text-generation', model='gpt2')\n",
        "set_seed(42)\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "sample_every = 100\n",
        "epochs = 8\n",
        "learning_rate = 5e-5\n",
        "warmup_steps = 1e2\n",
        "epsilon = 1e-8\n",
        "max_length = 128\n",
        "prompt_instruction = ''\n",
        "\n",
        "facet_test_file = 'data/clariq_f/ClariQ-FKw-dev.tsv'\n",
        "facet_test_data = pd.read_csv(facet_test_file, sep='\\t')\n",
        "_, facet_test_data = process_clariq_f(facet_test_data)\n",
        "\n",
        "b1, b2, b3, b4 = [], [], [], []\n",
        "m = []\n",
        "r = []\n",
        "c = []\n",
        "\n",
        "t_b1, t_b2, t_b3, t_b4 = [], [], [], []\n",
        "t_m = []\n",
        "t_r = []\n",
        "t_c = []\n",
        "\n",
        "model_output = 'template0.csv'\n",
        "\n",
        "if os.path.isfile(model_output):\n",
        "    b1, b2, b3, b4, m, r, c, t_b1, t_b2, t_b3, t_b4, t_m, t_r, t_c = evaluate_from_output(model_output)\n",
        "\n",
        "else:\n",
        "    clariq_f_train_file = 'data/clariq_f/ClariQ-FKw-train.tsv'\n",
        "    clariq_f_train_data = pd.read_csv(clariq_f_train_file, sep='\\t') \n",
        "    clariq_f_train_dict, clariq_f_train_data = process_clariq_f(clariq_f_train_data)\n",
        "    clariq_f_train_text_list = clariq_f_train_data['f_q_cq']\n",
        "\n",
        "    clariq_f_test_file = 'data/clariq_f/ClariQ-FKw-dev.tsv'\n",
        "    clariq_f_test_data = pd.read_csv(clariq_f_test_file, sep='\\t') \n",
        "    clariq_f_test_dict, clariq_f_test_data = process_clariq_f(clariq_f_test_data)\n",
        "\n",
        "    rs = rouge.Rouge()\n",
        "    rs_list = []\n",
        "    b1, b2, b3, b4 = [], [], [], []\n",
        "    m = []\n",
        "    \n",
        "    for iter, row in clariq_f_test_data.iterrows():\n",
        "        query = clariq_f_test_data.at[iter, 'initial_request']\n",
        "        ref = clariq_f_test_data.at[iter, 'question']\n",
        "        facet = clariq_f_test_data.at[iter, 'facet_desc']\n",
        "    \n",
        "        noun_in_query = [token.text for token in pos_tagger(query) if token.pos_ == 'NOUN']\n",
        "        propn_in_query = [token.text.lower() for token in pos_tagger(query) if token.pos_ == 'PROPN']\n",
        "\n",
        "        generated_cqs = []\n",
        "        for st in starting_texts:\n",
        "            sample_outputs = generator(query + ' ' + st, max_length=64, num_return_sequences=1)\n",
        "        \n",
        "            generated_text = sample_outputs[0]['generated_text']\n",
        "            generated_cq = generated_text[len(query):]\n",
        "            generated_cq = re.sub('\\[SEP\\]', ' ', generated_cq).strip()\n",
        "            generated_cq = re.sub('[.?\\n]', '&', generated_cq).split('&')[0].strip()\n",
        "            generated_cqs.append(generated_cq)\n",
        "\n",
        "        template_scores = calculate_WSDM(query=' '.join(noun_in_query+propn_in_query), doc_list=generated_cqs)\n",
        "        generated_cq = sorted(template_scores.keys(), key = lambda x: template_scores[x], reverse=True)[0] \n",
        "        #print(generated_cq)\n",
        "        clariq_f_test_data.at[iter, 'generated'] = process_generation(generated_cq)\n",
        "\n",
        "\n",
        "        # full reference evaluation\n",
        "        hyp_b1, hyp_b2, hyp_b3, hyp_b4, hyp_m, hyp_r, hyp_c = auto_evaluation(ref, generated_cq, facet)\n",
        "\n",
        "        b1.append(hyp_b1)\n",
        "        b2.append(hyp_b2)\n",
        "        b3.append(hyp_b3)\n",
        "        b4.append(hyp_b4)\n",
        "        m.append(hyp_m)\n",
        "        r.append(hyp_r)\n",
        "        c.append(hyp_c)\n",
        "\n",
        "        # question body evaluation\n",
        "        truncate_ref = ' '.join(ref.split()[template_len:])\n",
        "        truncate_generated_cq = ' '.join(generated_cq.split()[template_len:])\n",
        "        \n",
        "        t_hyp_b1, t_hyp_b2, t_hyp_b3, t_hyp_b4, t_hyp_m, t_hyp_r, t_hyp_c = auto_evaluation(truncate_ref, truncate_generated_cq, facet)\n",
        "\n",
        "        t_b1.append(t_hyp_b1)\n",
        "        t_b2.append(t_hyp_b2)\n",
        "        t_b3.append(t_hyp_b3)\n",
        "        t_b4.append(t_hyp_b4)\n",
        "        t_m.append(t_hyp_m)\n",
        "        t_r.append(t_hyp_r)\n",
        "        t_c.append(t_hyp_c)\n",
        "\n",
        "    output_df = clariq_f_test_data[['initial_request', 'facet_desc', 'question', 'generated']]\n",
        "    output_df.columns = ['query', 'facet', 'reference', 'candidate']\n",
        "    output_df.to_csv(model_output)\n",
        "\n",
        "# full reference results\n",
        "print(\"================================================================\")\n",
        "print(\"Full reference evaluation\")\n",
        "print(\"================================================================\")\n",
        "print(\"b1\", np.mean(b1), \"b2\", np.mean(b2), \"b3\", np.mean(b3), \"b4\", np.mean(b4))\n",
        "print(\"rouge-L\", np.mean(r))\n",
        "print(\"m\", np.mean(m))\n",
        "print(\"c\", np.mean(c))\n",
        "\n",
        "sekulic0_b1 = np.mean(b1)\n",
        "sekulic0_b2 = np.mean(b2)\n",
        "sekulic0_b3 = np.mean(b3)\n",
        "sekulic0_b4 = np.mean(b4)\n",
        "sekulic0_m = np.mean(m)\n",
        "sekulic0_r = np.mean(r)\n",
        "sekulic0_c = np.mean(c)\n",
        "\n",
        "# question body results\n",
        "print(\"================================================================\")\n",
        "print(\"Question body evaluation\")\n",
        "print(\"================================================================\")\n",
        "print(\"b1\", np.mean(t_b1), \"b2\", np.mean(t_b2), \"b3\", np.mean(t_b3), \"b4\", np.mean(t_b4))\n",
        "print(\"rouge-L\", np.mean(t_r))\n",
        "print(\"m\", np.mean(t_m))\n",
        "print(\"c\", np.mean(t_c))\n",
        "\n",
        "t_sekulic0_b1 = np.mean(t_b1)\n",
        "t_sekulic0_b2 = np.mean(t_b2)\n",
        "t_sekulic0_b3 = np.mean(t_b3)\n",
        "t_sekulic0_b4 = np.mean(t_b4)\n",
        "t_sekulic0_m = np.mean(t_m)\n",
        "t_sekulic0_r = np.mean(t_r)\n",
        "t_sekulic0_c = np.mean(t_c)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Zerogpt2\n",
            "By major vote\n",
            "Naturalness Counter({'good': 351, 'fair': 59, 'bad': 15})\n",
            "Usefulness Counter({'good': 293, 'fair': 93, 'bad': 39})\n",
            "By all vote\n",
            "Naturalness Counter({'good': 1222, 'fair': 358, 'bad': 120})\n",
            "Usefulness Counter({'good': 1000, 'fair': 469, 'bad': 231})\n",
            "Rewriting.\n",
            "By major vote\n",
            "Naturalness Counter({'good': 244, 'fair': 140, 'bad': 41})\n",
            "Usefulness Counter({'good': 216, 'fair': 153, 'bad': 56})\n",
            "By all vote\n",
            "Naturalness Counter({'good': 858, 'fair': 632, 'bad': 210})\n",
            "Usefulness Counter({'good': 753, 'fair': 678, 'bad': 269})\n",
            "Sekulic.\n",
            "By major vote\n",
            "Naturalness Counter({'good': 253, 'bad': 112, 'fair': 60})\n",
            "Usefulness Counter({'bad': 302, 'fair': 74, 'good': 49})\n",
            "By all vote\n",
            "Naturalness Counter({'good': 894, 'bad': 431, 'fair': 374, nan: 1})\n",
            "Usefulness Counter({'bad': 1048, 'fair': 377, 'good': 274, nan: 1})\n",
            "prompt finetuning.\n",
            "By major vote\n",
            "Naturalness Counter({'bad': 192, 'good': 186, 'fair': 47})\n",
            "Usefulness Counter({'bad': 205, 'good': 125, 'fair': 95})\n",
            "By all vote\n",
            "Naturalness Counter({'good': 730, 'bad': 691, 'fair': 279})\n",
            "Usefulness Counter({'bad': 777, 'good': 513, 'fair': 410})\n",
            "Agreements\n",
            "Naturalness\n",
            "1_2 0.303389944585287\n",
            "1_3 0.17753766033189233\n",
            "1_4 0.13127269459816882\n",
            "1_5 0.2777012843374037\n",
            "2_3 0.6095191840699369\n",
            "2_4 0.37860533121167583\n",
            "2_5 0.8946676276823085\n",
            "3_4 0.7128407543756772\n",
            "3_5 0.5672598974678856\n",
            "4_5 0.36427759914725255\n",
            "Usefulness\n",
            "1_2 0.28226889402159305\n",
            "1_3 0.20149676715089437\n",
            "1_4 0.12182263417793238\n",
            "1_5 0.2552937084221054\n",
            "2_3 0.647228240558037\n",
            "2_4 0.3073977638229438\n",
            "2_5 0.8945930849429001\n",
            "3_4 0.5727988614849716\n",
            "3_5 0.6107556966734257\n",
            "4_5 0.2938596869847464\n",
            "Agreements all\n",
            "Naturalness\n",
            "0.44170719778074885\n",
            "Usefulness\n",
            "0.418751533823955\n",
            "Agreements 1,2,5\n",
            "Naturalness\n",
            "0.4919196188683331\n",
            "Usefulness\n",
            "0.4773852291288662\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "\n",
        "human1 = pd.read_csv(\"csv/Deshmukh.csv\", encoding = \"utf-8\")\n",
        "human2 = pd.read_csv(\"csv/Gloria.csv\", encoding = \"utf-8\")\n",
        "human3 = pd.read_csv(\"csv/Jyotsna.csv\", encoding = \"utf-8\")\n",
        "human4 = pd.read_csv(\"csv/Kranthi.csv\", encoding = \"utf-8\")\n",
        "human5 = pd.read_csv(\"csv/Mithila.csv\", encoding = \"utf-8\")\n",
        "\n",
        "zerogpt2_df = pd.read_csv(\"csv/zeroshot_nd_wsdm.csv\")\n",
        "rewriting_df = pd.read_csv(\"csv/template_facet.csv\")\n",
        "sekulic_df = pd.read_csv(\"csv/sekulic.csv\")\n",
        "ftgpt2_df = pd.read_csv(\"csv/ftgpt2_prompt_temp0.1.csv\")\n",
        "\n",
        "labeler_df = human1\n",
        "labeler_name = 'human1'\n",
        "for iter, irow in labeler_df.iterrows():\n",
        "    row = [0,1,2,3]\n",
        "    for k in range(4):\n",
        "        if labeler_df.at[iter, 'generation'+str(k)] == zerogpt2_df.at[iter,'candidate']:\n",
        "            zerogpt2_df.at[iter, labeler_name + '_naturalness'] = labeler_df.at[iter, 'Naturalness'+str(k)].strip().lower()\n",
        "            zerogpt2_df.at[iter, labeler_name + '_usefulness'] = labeler_df.at[iter, 'Usefulness'+str(k)].strip().lower()\n",
        "            try:\n",
        "                row.remove(k)\n",
        "            except:\n",
        "                pass\n",
        "        if labeler_df.at[iter, 'generation'+str(k)] == rewriting_df.at[iter,'candidate']:\n",
        "            rewriting_df.at[iter, labeler_name + '_naturalness'] = labeler_df.at[iter, 'Naturalness'+str(k)].strip().lower()\n",
        "            rewriting_df.at[iter, labeler_name + '_usefulness'] = labeler_df.at[iter, 'Usefulness'+str(k)].strip().lower()\n",
        "            try:\n",
        "                row.remove(k)\n",
        "            except:\n",
        "                pass\n",
        "        if labeler_df.at[iter, 'generation'+str(k)] == ftgpt2_df.at[iter,'candidate']:\n",
        "            ftgpt2_df.at[iter, labeler_name + '_naturalness'] = labeler_df.at[iter, 'Naturalness'+str(k)].strip().lower()\n",
        "            ftgpt2_df.at[iter, labeler_name + '_usefulness'] = labeler_df.at[iter, 'Usefulness'+str(k)].strip().lower()\n",
        "            try:\n",
        "                row.remove(k)\n",
        "            except:\n",
        "                pass\n",
        "        if labeler_df.at[iter, 'generation'+str(k)] == sekulic_df.at[iter,'candidate']:\n",
        "            sekulic_df.at[iter, labeler_name + '_naturalness'] = labeler_df.at[iter, 'Naturalness'+str(k)].strip().lower()\n",
        "            sekulic_df.at[iter, labeler_name + '_usefulness'] = labeler_df.at[iter, 'Usefulness'+str(k)].strip().lower()\n",
        "            try:\n",
        "                row.remove(k)\n",
        "            except:\n",
        "                pass\n",
        "    \n",
        "    try:\n",
        "        assert len(row) == 0\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "labeler_df = human2\n",
        "labeler_name = 'human2'\n",
        "for iter, irow in labeler_df.iterrows():\n",
        "    row = [0,1,2,3]\n",
        "    for k in range(4):\n",
        "        if labeler_df.at[iter, 'generation'+str(k)] == zerogpt2_df.at[iter,'candidate']:\n",
        "            zerogpt2_df.at[iter, labeler_name + '_naturalness'] = labeler_df.at[iter, 'Naturalness'+str(k)].strip().lower()\n",
        "            zerogpt2_df.at[iter, labeler_name + '_usefulness'] = labeler_df.at[iter, 'Usefulness'+str(k)].strip().lower()\n",
        "            try:\n",
        "                row.remove(k)\n",
        "            except:\n",
        "                pass\n",
        "        if labeler_df.at[iter, 'generation'+str(k)] == rewriting_df.at[iter,'candidate']:\n",
        "            rewriting_df.at[iter, labeler_name + '_naturalness'] = labeler_df.at[iter, 'Naturalness'+str(k)].strip().lower()\n",
        "            rewriting_df.at[iter, labeler_name + '_usefulness'] = labeler_df.at[iter, 'Usefulness'+str(k)].strip().lower()\n",
        "            try:\n",
        "                row.remove(k)\n",
        "            except:\n",
        "                pass\n",
        "        if labeler_df.at[iter, 'generation'+str(k)] == ftgpt2_df.at[iter,'candidate']:\n",
        "            ftgpt2_df.at[iter, labeler_name + '_naturalness'] = labeler_df.at[iter, 'Naturalness'+str(k)].strip().lower()\n",
        "            ftgpt2_df.at[iter, labeler_name + '_usefulness'] = labeler_df.at[iter, 'Usefulness'+str(k)].strip().lower()\n",
        "            try:\n",
        "                row.remove(k)\n",
        "            except:\n",
        "                pass\n",
        "        if labeler_df.at[iter, 'generation'+str(k)] == sekulic_df.at[iter,'candidate']:\n",
        "            sekulic_df.at[iter, labeler_name + '_naturalness'] = labeler_df.at[iter, 'Naturalness'+str(k)].strip().lower()\n",
        "            sekulic_df.at[iter, labeler_name + '_usefulness'] = labeler_df.at[iter, 'Usefulness'+str(k)].strip().lower()\n",
        "            try:\n",
        "                row.remove(k)\n",
        "            except:\n",
        "                pass\n",
        "    \n",
        "    try:\n",
        "        assert len(row) == 0\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "\n",
        "labeler_df = human3\n",
        "labeler_name = 'human3'\n",
        "for iter, irow in labeler_df.iterrows():\n",
        "    row = [0,1,2,3]\n",
        "    for k in range(4):\n",
        "        if labeler_df.at[iter, 'generation'+str(k)] == zerogpt2_df.at[iter,'candidate']:\n",
        "            zerogpt2_df.at[iter, labeler_name + '_naturalness'] = labeler_df.at[iter, 'Naturalness'+str(k)].strip().lower()\n",
        "            zerogpt2_df.at[iter, labeler_name + '_usefulness'] = labeler_df.at[iter, 'Usefulness'+str(k)].strip().lower()\n",
        "            try:\n",
        "                row.remove(k)\n",
        "            except:\n",
        "                pass\n",
        "        if labeler_df.at[iter, 'generation'+str(k)] == rewriting_df.at[iter,'candidate']:\n",
        "            rewriting_df.at[iter, labeler_name + '_naturalness'] = labeler_df.at[iter, 'Naturalness'+str(k)].strip().lower()\n",
        "            rewriting_df.at[iter, labeler_name + '_usefulness'] = labeler_df.at[iter, 'Usefulness'+str(k)].strip().lower()\n",
        "            try:\n",
        "                row.remove(k)\n",
        "            except:\n",
        "                pass\n",
        "        if labeler_df.at[iter, 'generation'+str(k)] == ftgpt2_df.at[iter,'candidate']:\n",
        "            ftgpt2_df.at[iter, labeler_name + '_naturalness'] = labeler_df.at[iter, 'Naturalness'+str(k)].strip().lower()\n",
        "            ftgpt2_df.at[iter, labeler_name + '_usefulness'] = labeler_df.at[iter, 'Usefulness'+str(k)].strip().lower()\n",
        "            try:\n",
        "                row.remove(k)\n",
        "            except:\n",
        "                pass\n",
        "        if labeler_df.at[iter, 'generation'+str(k)] == sekulic_df.at[iter,'candidate']:\n",
        "            sekulic_df.at[iter, labeler_name + '_naturalness'] = labeler_df.at[iter, 'Naturalness'+str(k)].strip().lower()\n",
        "            sekulic_df.at[iter, labeler_name + '_usefulness'] = labeler_df.at[iter, 'Usefulness'+str(k)].strip().lower()\n",
        "            try:\n",
        "                row.remove(k)\n",
        "            except:\n",
        "                pass\n",
        "    \n",
        "    try:\n",
        "        assert len(row) == 0\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "\n",
        "labeler_df = human4\n",
        "labeler_name = 'human4'\n",
        "for iter, irow in labeler_df.iterrows():\n",
        "    row = [0,1,2,3]\n",
        "    for k in range(4):\n",
        "        if labeler_df.at[iter, 'generation'+str(k)] == zerogpt2_df.at[iter,'candidate']:\n",
        "            zerogpt2_df.at[iter, labeler_name + '_naturalness'] = labeler_df.at[iter, 'Naturalness'+str(k)].strip().lower()\n",
        "            zerogpt2_df.at[iter, labeler_name + '_usefulness'] = labeler_df.at[iter, 'Usefulness'+str(k)].strip().lower()\n",
        "            try:\n",
        "                row.remove(k)\n",
        "            except:\n",
        "                pass\n",
        "        if labeler_df.at[iter, 'generation'+str(k)] == rewriting_df.at[iter,'candidate']:\n",
        "            rewriting_df.at[iter, labeler_name + '_naturalness'] = labeler_df.at[iter, 'Naturalness'+str(k)].strip().lower()\n",
        "            rewriting_df.at[iter, labeler_name + '_usefulness'] = labeler_df.at[iter, 'Usefulness'+str(k)].strip().lower()\n",
        "            try:\n",
        "                row.remove(k)\n",
        "            except:\n",
        "                pass\n",
        "        if labeler_df.at[iter, 'generation'+str(k)] == ftgpt2_df.at[iter,'candidate']:\n",
        "            ftgpt2_df.at[iter, labeler_name + '_naturalness'] = labeler_df.at[iter, 'Naturalness'+str(k)].strip().lower()\n",
        "            ftgpt2_df.at[iter, labeler_name + '_usefulness'] = labeler_df.at[iter, 'Usefulness'+str(k)].strip().lower()\n",
        "            try:\n",
        "                row.remove(k)\n",
        "            except:\n",
        "                pass\n",
        "        if labeler_df.at[iter, 'generation'+str(k)] == sekulic_df.at[iter,'candidate']:\n",
        "            sekulic_df.at[iter, labeler_name + '_naturalness'] = labeler_df.at[iter, 'Naturalness'+str(k)].strip().lower()\n",
        "            sekulic_df.at[iter, labeler_name + '_usefulness'] = labeler_df.at[iter, 'Usefulness'+str(k)].strip().lower()\n",
        "            try:\n",
        "                row.remove(k)\n",
        "            except:\n",
        "                pass\n",
        "    \n",
        "    try:\n",
        "        assert len(row) == 0\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "\n",
        "labeler_df = human5\n",
        "labeler_name = 'human5'\n",
        "for iter, irow in labeler_df.iterrows():\n",
        "    row = [0,1,2,3]\n",
        "    for k in range(4):\n",
        "        if labeler_df.at[iter, 'generation'+str(k)] == zerogpt2_df.at[iter,'candidate']:\n",
        "            zerogpt2_df.at[iter, labeler_name + '_naturalness'] = labeler_df.at[iter, 'Naturalness'+str(k)].strip().lower()\n",
        "            zerogpt2_df.at[iter, labeler_name + '_usefulness'] = labeler_df.at[iter, 'Usefulness'+str(k)].strip().lower()\n",
        "            try:\n",
        "                row.remove(k)\n",
        "            except:\n",
        "                pass\n",
        "        if labeler_df.at[iter, 'generation'+str(k)] == rewriting_df.at[iter,'candidate']:\n",
        "            rewriting_df.at[iter, labeler_name + '_naturalness'] = labeler_df.at[iter, 'Naturalness'+str(k)].strip().lower()\n",
        "            rewriting_df.at[iter, labeler_name + '_usefulness'] = labeler_df.at[iter, 'Usefulness'+str(k)].strip().lower()\n",
        "            try:\n",
        "                row.remove(k)\n",
        "            except:\n",
        "                pass\n",
        "        if labeler_df.at[iter, 'generation'+str(k)] == ftgpt2_df.at[iter,'candidate']:\n",
        "            ftgpt2_df.at[iter, labeler_name + '_naturalness'] = labeler_df.at[iter, 'Naturalness'+str(k)].strip().lower()\n",
        "            ftgpt2_df.at[iter, labeler_name + '_usefulness'] = labeler_df.at[iter, 'Usefulness'+str(k)].strip().lower()\n",
        "            try:\n",
        "                row.remove(k)\n",
        "            except:\n",
        "                pass\n",
        "        if labeler_df.at[iter, 'generation'+str(k)] == sekulic_df.at[iter,'candidate']:\n",
        "            sekulic_df.at[iter, labeler_name + '_naturalness'] = labeler_df.at[iter, 'Naturalness'+str(k)].strip().lower()\n",
        "            sekulic_df.at[iter, labeler_name + '_usefulness'] = labeler_df.at[iter, 'Usefulness'+str(k)].strip().lower()\n",
        "            try:\n",
        "                row.remove(k)\n",
        "            except:\n",
        "                pass    \n",
        "    \n",
        "    try:\n",
        "        assert len(row) == 0\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "\n",
        "zerogpt2_df.to_csv(\"csv/zeroshot_labeled.csv\")\n",
        "rewriting_df.to_csv(\"csv/template_facet_labeled.csv\")\n",
        "sekulic_df.to_csv(\"csv/sekulic_labeled.csv\")\n",
        "ftgpt2_df.to_csv(\"csv/ftgpt2_prompt_labeled.csv\")\n",
        "\n",
        "zerogpt2_zipped_naturalness = zip(zerogpt2_df['human1_naturalness'].tolist(), zerogpt2_df['human2_naturalness'].tolist(), zerogpt2_df['human3_naturalness'].tolist(), zerogpt2_df['human4_naturalness'].tolist())\n",
        "zerogpt2_zipped_usefulness = zip(zerogpt2_df['human1_usefulness'].tolist(), zerogpt2_df['human2_usefulness'].tolist(), zerogpt2_df['human3_usefulness'].tolist(), zerogpt2_df['human4_usefulness'].tolist())\n",
        "zerogpt2_naturalness_major_vote = [Counter(list(labels)).most_common()[0][0] if Counter(list(labels)).most_common()[0][1] > 1 else 'fair' for labels in zerogpt2_zipped_naturalness]\n",
        "zerogpt2_usefulness_major_vote = [Counter(list(labels)).most_common()[0][0] if Counter(list(labels)).most_common()[0][1] > 1 else 'fair' for labels in zerogpt2_zipped_usefulness]\n",
        "\n",
        "print(\"Zerogpt2\")\n",
        "print(\"By major vote\")\n",
        "print(\"Naturalness\", Counter(zerogpt2_naturalness_major_vote))\n",
        "print(\"Usefulness\", Counter(zerogpt2_usefulness_major_vote))\n",
        "print(\"By all vote\")\n",
        "print(\"Naturalness\", Counter(zerogpt2_df['human1_naturalness'].tolist() + zerogpt2_df['human2_naturalness'].tolist() + zerogpt2_df['human3_naturalness'].tolist() + zerogpt2_df['human4_naturalness'].tolist()))\n",
        "print(\"Usefulness\", Counter(zerogpt2_df['human1_usefulness'].tolist() + zerogpt2_df['human2_usefulness'].tolist() + zerogpt2_df['human3_usefulness'].tolist() + zerogpt2_df['human4_usefulness'].tolist()))\n",
        "\n",
        "human1_naturalness = zerogpt2_df['human1_naturalness'].tolist() + rewriting_df['human1_naturalness'].tolist() + ftgpt2_df['human1_naturalness'].tolist() + sekulic_df['human1_naturalness'].tolist()\n",
        "human2_naturalness = zerogpt2_df['human2_naturalness'].tolist() + rewriting_df['human2_naturalness'].tolist() + ftgpt2_df['human2_naturalness'].tolist() + sekulic_df['human2_naturalness'].tolist()\n",
        "human3_naturalness = zerogpt2_df['human3_naturalness'].tolist() + rewriting_df['human3_naturalness'].tolist() + ftgpt2_df['human3_naturalness'].tolist() + sekulic_df['human3_naturalness'].tolist()\n",
        "human4_naturalness = zerogpt2_df['human4_naturalness'].tolist() + rewriting_df['human4_naturalness'].tolist() + ftgpt2_df['human4_naturalness'].tolist() + sekulic_df['human4_naturalness'].tolist()\n",
        "human5_naturalness = zerogpt2_df['human5_naturalness'].tolist() + rewriting_df['human5_naturalness'].tolist() + ftgpt2_df['human5_naturalness'].tolist() + sekulic_df['human5_naturalness'].tolist()\n",
        "\n",
        "human1_usefulness = zerogpt2_df['human1_usefulness'].tolist() + rewriting_df['human1_usefulness'].tolist() + ftgpt2_df['human1_usefulness'].tolist() + sekulic_df['human1_usefulness'].tolist()\n",
        "human2_usefulness = zerogpt2_df['human2_usefulness'].tolist() + rewriting_df['human2_usefulness'].tolist() + ftgpt2_df['human2_usefulness'].tolist() + sekulic_df['human2_usefulness'].tolist()\n",
        "human3_usefulness = zerogpt2_df['human3_usefulness'].tolist() + rewriting_df['human3_usefulness'].tolist() + ftgpt2_df['human3_usefulness'].tolist() + sekulic_df['human3_usefulness'].tolist()\n",
        "human4_usefulness = zerogpt2_df['human4_usefulness'].tolist() + rewriting_df['human4_usefulness'].tolist() + ftgpt2_df['human4_usefulness'].tolist() + sekulic_df['human4_usefulness'].tolist()\n",
        "human5_usefulness = zerogpt2_df['human5_usefulness'].tolist() + rewriting_df['human5_usefulness'].tolist() + ftgpt2_df['human5_usefulness'].tolist() + sekulic_df['human5_usefulness'].tolist()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "rewriting_df_zipped_naturalness = zip(rewriting_df['human1_naturalness'].tolist(), rewriting_df['human2_naturalness'].tolist(), rewriting_df['human3_naturalness'].tolist(), rewriting_df['human4_naturalness'].tolist())\n",
        "rewriting_df_zipped_usefulness = zip(rewriting_df['human1_usefulness'].tolist(), rewriting_df['human2_usefulness'].tolist(), rewriting_df['human3_usefulness'].tolist(), rewriting_df['human4_usefulness'].tolist())\n",
        "rewriting_df_naturalness_major_vote = [Counter(list(labels)).most_common()[0][0] if Counter(list(labels)).most_common()[0][1] > 1 else 'fair' for labels in rewriting_df_zipped_naturalness]\n",
        "rewriting_df_usefulness_major_vote = [Counter(list(labels)).most_common()[0][0] if Counter(list(labels)).most_common()[0][1] > 1 else 'fair' for labels in rewriting_df_zipped_usefulness]\n",
        "\n",
        "print(\"Rewriting.\")\n",
        "print(\"By major vote\")\n",
        "print(\"Naturalness\", Counter(rewriting_df_naturalness_major_vote))\n",
        "print(\"Usefulness\", Counter(rewriting_df_usefulness_major_vote))\n",
        "print(\"By all vote\")\n",
        "print(\"Naturalness\", Counter(rewriting_df['human1_naturalness'].tolist() + rewriting_df['human2_naturalness'].tolist() + rewriting_df['human3_naturalness'].tolist() + rewriting_df['human4_naturalness'].tolist()))\n",
        "print(\"Usefulness\", Counter(rewriting_df['human1_usefulness'].tolist() + rewriting_df['human2_usefulness'].tolist() + rewriting_df['human3_usefulness'].tolist() + rewriting_df['human4_usefulness'].tolist()))\n",
        "\n",
        "\n",
        "sekulic_df_zipped_naturalness = zip(sekulic_df['human1_naturalness'].tolist(), sekulic_df['human2_naturalness'].tolist(), sekulic_df['human3_naturalness'].tolist(), sekulic_df['human4_naturalness'].tolist())\n",
        "sekulic_df_zipped_usefulness = zip(sekulic_df['human1_usefulness'].tolist(), sekulic_df['human2_usefulness'].tolist(), sekulic_df['human3_usefulness'].tolist(), sekulic_df['human4_usefulness'].tolist())\n",
        "sekulic_df_naturalness_major_vote = [Counter(list(labels)).most_common()[0][0] if Counter(list(labels)).most_common()[0][1] > 1 else 'fair' for labels in sekulic_df_zipped_naturalness]\n",
        "sekulic_df_usefulness_major_vote = [Counter(list(labels)).most_common()[0][0] if Counter(list(labels)).most_common()[0][1] > 1 else 'fair' for labels in sekulic_df_zipped_usefulness]\n",
        "\n",
        "print(\"Sekulic.\")\n",
        "print(\"By major vote\")\n",
        "print(\"Naturalness\", Counter(sekulic_df_naturalness_major_vote))\n",
        "print(\"Usefulness\", Counter(sekulic_df_usefulness_major_vote))\n",
        "print(\"By all vote\")\n",
        "print(\"Naturalness\", Counter(sekulic_df['human1_naturalness'].tolist() + sekulic_df['human2_naturalness'].tolist() + sekulic_df['human3_naturalness'].tolist() + sekulic_df['human4_naturalness'].tolist()))\n",
        "print(\"Usefulness\", Counter(sekulic_df['human1_usefulness'].tolist() + sekulic_df['human2_usefulness'].tolist() + sekulic_df['human3_usefulness'].tolist() + sekulic_df['human4_usefulness'].tolist()))\n",
        "\n",
        "\n",
        "ftgpt2_df_zipped_naturalness = zip(ftgpt2_df['human1_naturalness'].tolist(), ftgpt2_df['human2_naturalness'].tolist(), ftgpt2_df['human3_naturalness'].tolist(), ftgpt2_df['human4_naturalness'].tolist())\n",
        "ftgpt2_df_zipped_usefulness = zip(ftgpt2_df['human1_usefulness'].tolist(), ftgpt2_df['human2_usefulness'].tolist(), ftgpt2_df['human3_usefulness'].tolist(), ftgpt2_df['human4_usefulness'].tolist())\n",
        "ftgpt2_df_naturalness_major_vote = [Counter(list(labels)).most_common()[0][0] if Counter(list(labels)).most_common()[0][1] > 1 else 'fair' for labels in ftgpt2_df_zipped_naturalness]\n",
        "ftgpt2_df_usefulness_major_vote = [Counter(list(labels)).most_common()[0][0] if Counter(list(labels)).most_common()[0][1] > 1 else 'fair' for labels in ftgpt2_df_zipped_usefulness]\n",
        "\n",
        "print(\"prompt finetuning.\")\n",
        "print(\"By major vote\")\n",
        "print(\"Naturalness\", Counter(ftgpt2_df_naturalness_major_vote))\n",
        "print(\"Usefulness\", Counter(ftgpt2_df_usefulness_major_vote))\n",
        "print(\"By all vote\")\n",
        "print(\"Naturalness\", Counter(ftgpt2_df['human1_naturalness'].tolist() + ftgpt2_df['human2_naturalness'].tolist() + ftgpt2_df['human3_naturalness'].tolist() + ftgpt2_df['human4_naturalness'].tolist()))\n",
        "print(\"Usefulness\", Counter(ftgpt2_df['human1_usefulness'].tolist() + ftgpt2_df['human2_usefulness'].tolist() + ftgpt2_df['human3_usefulness'].tolist() + ftgpt2_df['human4_usefulness'].tolist()))\n",
        "\n",
        "\n",
        "print(\"Agreements\")\n",
        "print(\"Naturalness\")\n",
        "print(\"1_2\", cohen_kappa_score(human1_naturalness, human2_naturalness))\n",
        "print(\"1_3\", cohen_kappa_score(human1_naturalness, human3_naturalness))\n",
        "print(\"1_4\", cohen_kappa_score(human1_naturalness, human4_naturalness))\n",
        "print(\"1_5\", cohen_kappa_score(human1_naturalness, human5_naturalness))\n",
        "print(\"2_3\", cohen_kappa_score(human2_naturalness, human3_naturalness))\n",
        "print(\"2_4\", cohen_kappa_score(human2_naturalness, human4_naturalness))\n",
        "print(\"2_5\", cohen_kappa_score(human2_naturalness, human5_naturalness))\n",
        "print(\"3_4\", cohen_kappa_score(human3_naturalness, human4_naturalness))\n",
        "print(\"3_5\", cohen_kappa_score(human3_naturalness, human5_naturalness))\n",
        "print(\"4_5\", cohen_kappa_score(human4_naturalness, human5_naturalness))\n",
        "print(\"Usefulness\")\n",
        "print(\"1_2\", cohen_kappa_score(human1_usefulness, human2_usefulness))\n",
        "print(\"1_3\", cohen_kappa_score(human1_usefulness, human3_usefulness))\n",
        "print(\"1_4\", cohen_kappa_score(human1_usefulness, human4_usefulness))\n",
        "print(\"1_5\", cohen_kappa_score(human1_usefulness, human5_usefulness))\n",
        "print(\"2_3\", cohen_kappa_score(human2_usefulness, human3_usefulness))\n",
        "print(\"2_4\", cohen_kappa_score(human2_usefulness, human4_usefulness))\n",
        "print(\"2_5\", cohen_kappa_score(human2_usefulness, human5_usefulness))\n",
        "print(\"3_4\", cohen_kappa_score(human3_usefulness, human4_usefulness))\n",
        "print(\"3_5\", cohen_kappa_score(human3_usefulness, human5_usefulness))\n",
        "print(\"4_5\", cohen_kappa_score(human4_usefulness, human5_usefulness))\n",
        "\n",
        "\n",
        "human1_naturalness = zerogpt2_df['human1_naturalness'].tolist() + rewriting_df['human1_naturalness'].tolist() + ftgpt2_df['human1_naturalness'].tolist() + sekulic_df['human1_naturalness'].tolist()\n",
        "human2_naturalness = zerogpt2_df['human2_naturalness'].tolist() + rewriting_df['human2_naturalness'].tolist() + ftgpt2_df['human2_naturalness'].tolist() + sekulic_df['human2_naturalness'].tolist()\n",
        "human3_naturalness = zerogpt2_df['human3_naturalness'].tolist() + rewriting_df['human3_naturalness'].tolist() + ftgpt2_df['human3_naturalness'].tolist() + sekulic_df['human3_naturalness'].tolist()\n",
        "human4_naturalness = zerogpt2_df['human4_naturalness'].tolist() + rewriting_df['human4_naturalness'].tolist() + ftgpt2_df['human4_naturalness'].tolist() + sekulic_df['human4_naturalness'].tolist()\n",
        "human5_naturalness = zerogpt2_df['human5_naturalness'].tolist() + rewriting_df['human5_naturalness'].tolist() + ftgpt2_df['human5_naturalness'].tolist() + sekulic_df['human5_naturalness'].tolist()\n",
        "\n",
        "human1_usefulness = zerogpt2_df['human1_usefulness'].tolist() + rewriting_df['human1_usefulness'].tolist() + ftgpt2_df['human1_usefulness'].tolist() + sekulic_df['human1_usefulness'].tolist()\n",
        "human2_usefulness = zerogpt2_df['human2_usefulness'].tolist() + rewriting_df['human2_usefulness'].tolist() + ftgpt2_df['human2_usefulness'].tolist() + sekulic_df['human2_usefulness'].tolist()\n",
        "human3_usefulness = zerogpt2_df['human3_usefulness'].tolist() + rewriting_df['human3_usefulness'].tolist() + ftgpt2_df['human3_usefulness'].tolist() + sekulic_df['human3_usefulness'].tolist()\n",
        "human4_usefulness = zerogpt2_df['human4_usefulness'].tolist() + rewriting_df['human4_usefulness'].tolist() + ftgpt2_df['human4_usefulness'].tolist() + sekulic_df['human4_usefulness'].tolist()\n",
        "human5_usefulness = zerogpt2_df['human5_usefulness'].tolist() + rewriting_df['human5_usefulness'].tolist() + ftgpt2_df['human5_usefulness'].tolist() + sekulic_df['human5_usefulness'].tolist()\n",
        "\n",
        "\n",
        "n_1_2 = cohen_kappa_score(human1_naturalness, human2_naturalness)\n",
        "n_1_3 = cohen_kappa_score(human1_naturalness, human3_naturalness)\n",
        "n_1_4 = cohen_kappa_score(human1_naturalness, human4_naturalness)\n",
        "n_1_5 = cohen_kappa_score(human1_naturalness, human5_naturalness)\n",
        "n_2_3 = cohen_kappa_score(human2_naturalness, human3_naturalness)\n",
        "n_2_4 = cohen_kappa_score(human2_naturalness, human4_naturalness)\n",
        "n_2_5 = cohen_kappa_score(human2_naturalness, human5_naturalness)\n",
        "n_3_4 = cohen_kappa_score(human3_naturalness, human4_naturalness)\n",
        "n_3_5 = cohen_kappa_score(human3_naturalness, human5_naturalness)\n",
        "n_4_5 = cohen_kappa_score(human4_naturalness, human5_naturalness)\n",
        "\n",
        "\n",
        "u_1_2 = cohen_kappa_score(human1_usefulness, human2_usefulness)\n",
        "u_1_3 = cohen_kappa_score(human1_usefulness, human3_usefulness)\n",
        "u_1_4 = cohen_kappa_score(human1_usefulness, human4_usefulness)\n",
        "u_1_5 = cohen_kappa_score(human1_usefulness, human5_usefulness)\n",
        "u_2_3 = cohen_kappa_score(human2_usefulness, human3_usefulness)\n",
        "u_2_4 = cohen_kappa_score(human2_usefulness, human4_usefulness)\n",
        "u_2_5 = cohen_kappa_score(human2_usefulness, human5_usefulness)\n",
        "u_3_4 = cohen_kappa_score(human3_usefulness, human4_usefulness)\n",
        "u_3_5 = cohen_kappa_score(human3_usefulness, human5_usefulness)\n",
        "u_4_5 = cohen_kappa_score(human4_usefulness, human5_usefulness)\n",
        "\n",
        "\n",
        "print(\"Agreements all\")\n",
        "print(\"Naturalness\")\n",
        "print((n_1_2 + n_1_3 + n_1_4 + n_1_5 + n_2_3 + n_2_4 + n_2_5 + n_3_4 + n_3_5 + n_4_5)/10)\n",
        "print(\"Usefulness\")\n",
        "print((u_1_2 + u_1_3 + u_1_4 + u_1_5 + u_2_3 + u_2_4 + u_2_5 + u_3_4 + u_3_5 + u_4_5)/10)\n",
        "\n",
        "\n",
        "print(\"Agreements 1,2,5\")\n",
        "print(\"Naturalness\")\n",
        "print((n_1_2 + n_1_5 + n_2_5)/3)\n",
        "print(\"Usefulness\")\n",
        "print((u_1_2 + u_1_5 + u_2_5)/3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13 (default, Mar 28 2022, 11:38:47) \n[GCC 7.5.0]"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "f4424f5eb994e42c45bf033a434afaefa04afd4f7d58b4d8480962b7c9c86062"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
