{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 0. imports, definitions, processing, hyperparameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import json\n",
        "import sys\n",
        "import time\n",
        "import math\n",
        "import pprint\n",
        "import numpy as np\n",
        "import torch as T\n",
        "import openai\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
        "from transformers import GPT2LMHeadModel,  GPT2Tokenizer, GPT2Config\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from transformers import PhrasalConstraint\n",
        "from transformers import pipeline, set_seed\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from word_forms.word_forms import get_word_forms\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "import rouge \n",
        "rs = rouge.Rouge()\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk.data\n",
        "nltk_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "\n",
        "import logging\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.CRITICAL)\n",
        "\n",
        "import spacy\n",
        "spacy.prefer_gpu()\n",
        "\n",
        "pos_tagger = spacy.load('en_core_web_sm')\n",
        "ps = PorterStemmer()\n",
        "rs = rouge.Rouge()\n",
        "\n",
        "template_len = 4\n",
        "batch_size = 32\n",
        "sample_every = 100\n",
        "epochs = 8\n",
        "learning_rate = 5e-5\n",
        "warmup_steps = 1e2\n",
        "epsilon = 1e-8\n",
        "max_length = 128\n",
        "device = T.device(\"cuda\")\n",
        "\n",
        "SEP = '[SEP]'\n",
        "PAD = '[PAD]'\n",
        "BOS = '[BOS]'\n",
        "EOS = '[EOS]'\n",
        "\n",
        "starting_texts = [\n",
        "                ## template from this paper https://dl.acm.org/doi/abs/10.1145/3409256.3409817#:~:text=Recent%20research%20on%20conversational%20search%20highlights%20the%20importance,lexical%20baseline%2Cthat%20significantly%20outperforms%20the%20existing%20naive%20baselines.\n",
        "                \"[SEP] are you looking for\",\n",
        "                \"[SEP] do you want to know\",\n",
        "                \"[SEP] would you like to\",\n",
        "                \"[SEP] are you interested in\",\n",
        "                \"[SEP] do you need information\",\n",
        "                \"[SEP] do you want information\",\n",
        "                \"[SEP] do you need to\",\n",
        "                \"[SEP] do you want to\",\n",
        "            ]\n",
        "\n",
        "openai.api_key = ''\n",
        "gpt3_examples = [\"Find condos in Florida. Ask a question that contains words in the list ['specific', 'city']. Are you interested in any specific city in florida?\",\n",
        "        \"What should I know about living in India? Ask a question that contains words in the list ['challenges']. Would you like to know about the economic challenges of living in India?\"\n",
        "        \"Tell me more about Euclid. Ask a question that contains words in the list ['greece', 'math']. would you like to know what impact Euclid had on mathematics in ancient Greece?\"\n",
        "]\n",
        "\n",
        "seed_val = 2022\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "T.manual_seed(seed_val)\n",
        "T.cuda.manual_seed_all(seed_val)\n",
        "set_seed(seed_val)\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', bos_token=BOS, eos_token=EOS, pad_token=PAD)\n",
        "configuration = GPT2Config.from_pretrained('gpt2', output_hidden_states=False, bos_token=BOS, eos_token=EOS, pad_token=PAD)\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=configuration) \n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model.cuda()\n",
        "\n",
        "ppl_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "ppl_configuration = GPT2Config.from_pretrained('gpt2', output_hidden_states=False)\n",
        "ppl_model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=ppl_configuration) \n",
        "ppl_model.resize_token_embeddings(len(ppl_tokenizer))\n",
        "ppl_model.cuda()\n",
        "\n",
        "## create train/test split for reproduction, RUN ONLY ONCE!\n",
        "if not (os.path.exists('data/question_cases_answered_test.csv') and os.path.exists('data/question_cases_answered_train_dev.csv')):\n",
        "    print(\"Generating Usi train/test split for reproduction.\")\n",
        "    usi_train_file = 'data/question_cases_answered.csv'\n",
        "    usi_train_data = pd.read_csv(usi_train_file) \n",
        "\n",
        "    topic_id_set = set(usi_train_data['topic_id'])\n",
        "    test_topic = random.sample(topic_id_set, int(0.2 * len(topic_id_set)))\n",
        "\n",
        "    usi_test = usi_train_data.loc[usi_train_data['topic_id'].isin(test_topic)]\n",
        "    usi_train_dev = usi_train_data.drop(usi_test.index)\n",
        "\n",
        "    usi_test = usi_test.sort_values(by='topic_id')\n",
        "    usi_train_dev = usi_train_dev.sort_values(by='topic_id')\n",
        "\n",
        "    usi_test.to_csv('data/question_cases_answered_test.csv', index=False)\n",
        "    usi_train_dev.to_csv('data/question_cases_answered_train_dev.csv', index=False)\n",
        "\n",
        "## create train/test split for reproduction, RUN ONLY ONCE!\n",
        "if not (os.path.exists('data/clariq_f/ClariQ-FKw-trial.tsv') and os.path.exists('data/clariq_f/ClariQ-FKw-train_no_trial.tsv') ):\n",
        "    print(\"Generating clariq-f train/test split for reproduction.\")\n",
        "    train_file = 'data/clariq_f/ClariQ-FKw-train.tsv'\n",
        "    train_data = pd.read_csv(train_file, sep='\\t') \n",
        "\n",
        "    topic_id_set = set(train_data['topic_id'])\n",
        "    random.seed(17)\n",
        "    topics = random.sample(topic_id_set, int(0.056 * len(topic_id_set)))\n",
        "\n",
        "    trial_data = train_data.loc[train_data['topic_id'].isin(topics)]\n",
        "    train_no_trial = train_data.drop(trial_data.index)\n",
        "\n",
        "    trial_data = trial_data.sort_values(by='topic_id')\n",
        "    train_no_trial = train_no_trial.sort_values(by='topic_id')\n",
        "\n",
        "    trial_data.to_csv('data/clariq_f/ClariQ-FKw-trial.tsv', index=False, sep = '\\t')\n",
        "    train_no_trial.to_csv('data/clariq_f/ClariQ-FKw-train_no_trial.tsv', index=False, sep = '\\t')\n",
        "\n",
        "def process_clariq_f(data):\n",
        "    data_dict = {}\n",
        "    data = data.dropna(subset=['question', 'initial_request'])\n",
        "    for iter, row in data.iterrows():\n",
        "        q = str(data.at[iter, 'initial_request'])\n",
        "        cq = str(data.at[iter, 'question'])\n",
        "        f = str(data.at[iter, 'facet_desc'])\n",
        "\n",
        "        data.at[iter, 'f_q'] = f + SEP + q\n",
        "        data.at[iter, 'f_q_cq'] = f + SEP + q + BOS + cq + EOS\n",
        "        data.at[iter, 'q_f'] = q + SEP + f\n",
        "        data.at[iter, 'q_f_cq'] = q + SEP + f + BOS + cq + EOS\n",
        "        data.at[iter, 'instructional_q_f_cq'] = q + ' '+ \"Ask a question that contains words in the list\" + ' ' + \"[\" + \", \".join([\"'\"+w+\"'\" for w in f.split()])  + '].' + ' ' + cq\n",
        "        data.at[iter, 'instructional_q_f'] = q + ' '+ \"Ask a question that contains words in the list\" + ' ' + \"[\" + \", \".join([\"'\"+w+\"'\" for w in f.split()])  + '].'\n",
        "        \n",
        "    return data_dict, data\n",
        "\n",
        "def compute_average_rouge(rouge_list):\n",
        "    '''\n",
        "    this function computes the average rouge f,p,r of a list of rouge scores\n",
        "    rouge_list is a list of dictionaries in the following format:\n",
        "    {\n",
        "        \"rouge-1\": {\n",
        "            \"f\": 0.4786324739396596,\n",
        "            \"p\": 0.6363636363636364,\n",
        "            \"r\": 0.3835616438356164\n",
        "            },\n",
        "        \"rouge-2\": {\n",
        "            \"f\": 0.2608695605353498,\n",
        "            \"p\": 0.3488372093023256,\n",
        "            \"r\": 0.20833333333333334\n",
        "            },\n",
        "        \"rouge-l\": {\n",
        "            \"f\": 0.44705881864636676,\n",
        "            \"p\": 0.5277777777777778,\n",
        "            \"r\": 0.3877551020408163\n",
        "            }\n",
        "    }\n",
        "    '''\n",
        "    # if length of rouge_list is 1 or it is not cast as a list of dicts\n",
        "    if isinstance(rouge_list, dict):\n",
        "        return rouge_list\n",
        "    r_dict = {\n",
        "        \"rouge-1\": {\n",
        "            \"f\": 0,\n",
        "            \"p\": 0,\n",
        "            \"r\": 0\n",
        "            },\n",
        "        \"rouge-2\": {\n",
        "            \"f\": 0,\n",
        "            \"p\": 0,\n",
        "            \"r\": 0\n",
        "            },\n",
        "        \"rouge-l\": {\n",
        "            \"f\": 0,\n",
        "            \"p\": 0,\n",
        "            \"r\": 0\n",
        "            }\n",
        "    }\n",
        "    for d in rouge_list:\n",
        "        for k_len in d.keys():\n",
        "            for k in d[k_len].keys():\n",
        "                r_dict[k_len][k] += d[k_len][k]\n",
        "    n_hyps = len(rouge_list)\n",
        "    for k_len in r_dict.keys():\n",
        "        for k in r_dict[k_len].keys():\n",
        "            r_dict[k_len][k] /= n_hyps\n",
        "    return r_dict\n",
        "\n",
        "def calculatePerplexity(sentence,model,tokenizer):\n",
        "    tokenize_input = tokenizer.tokenize(sentence)\n",
        "    tensor_input = T.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)]).to(device)\n",
        "    loss=model(tensor_input, labels=tensor_input)[0]\n",
        "    return math.exp(loss)\n",
        "\n",
        "def process_generation(generation):\n",
        "    processed_generation = re.sub(r'\\[SEP\\]',' ', generation)  # remove [SEP]\n",
        "    processed_generation = re.sub(r'[^\\w\\s]',' ', processed_generation)  # remove punctuation\n",
        "    return processed_generation\n",
        "\n",
        "def calculate_WSDM(query, doc_list):\n",
        "    lambda_t = 1\n",
        "    lambda_o = 1\n",
        "    lambda_u = 1\n",
        "    mu = 25\n",
        "    collection = ' '.join(doc_list)\n",
        "    collection_size = len(collection)\n",
        "\n",
        "    def tfq(word, doc):\n",
        "        many_forms = get_word_forms(word)\n",
        "        word_forms = [word for k in many_forms.keys() for word in many_forms[k]] \n",
        "        return sum( [sum([1 if w == wf else 0 for w in doc]) for wf in word_forms])\n",
        "\n",
        "    def tf1(qk, qk1, doc):    \n",
        "        many_formsk = get_word_forms(qk)\n",
        "        word_formsk = list(set([word for k in many_formsk.keys() for word in many_formsk[k]] + [qk])) \n",
        "        many_formsk1 = get_word_forms(qk1)\n",
        "        word_formsk1 = list(set([word for k in many_formsk1.keys() for word in many_formsk1[k]] + [qk1]))\n",
        "        return sum( [sum([1 if qkf == doc[k] and qk1f == doc[k+1] else 0 for k in range(len(doc)-1)]) for qkf in word_formsk for qk1f in word_formsk1])\n",
        "    \n",
        "    def tfuw(qk, qj, doc):\n",
        "        wsz = 2\n",
        "        many_formsk = get_word_forms(qk)\n",
        "        word_formsk = list(set([word for k in many_formsk.keys() for word in many_formsk[k]] + [qk])) \n",
        "        many_formsj = get_word_forms(qj)\n",
        "        word_formsj = list(set([word for k in many_formsj.keys() for word in many_formsj[k]] + [qj])) \n",
        "        return sum( [sum([1 if qkf == doc[k] and qjf in doc[max(k-wsz,0):min(k+wsz,len(doc))] else 0 for k in range(len(doc))]) for qkf in word_formsk for qjf in word_formsj])\n",
        "\n",
        "    def f_t(query, doc, collection):\n",
        "        res = sum([(tfq(word, doc.split()) + mu * tfq(word, collection.split())/collection_size) / (len(doc.split()) + mu) for word in query.split() ])\n",
        "        #print(query, doc)\n",
        "        #print(\"ft\", res)\n",
        "        return res\n",
        "    \n",
        "    def f_o(query, doc, collection):\n",
        "        query = query.split()\n",
        "        if len(query) < 2:\n",
        "            return 0\n",
        "        res = sum([(tf1(query[k], query[k+1], doc.split()) + mu * tf1(query[k], query[k+1], collection.split())/collection_size) / (len(doc.split()) + mu)  for k in range(len(query)-1)])\n",
        "        \n",
        "        #print(query, doc)\n",
        "        #print(\"fo\", res)\n",
        "        return res\n",
        "\n",
        "    def f_u(query, doc, collection):\n",
        "        query = list(set(query.split()))\n",
        "        l = len(query)\n",
        "        if l < 2:\n",
        "            return 0\n",
        "        res = sum([(tfuw(query[k], query[j], doc.split()) + mu * tfuw(query[k], query[j], collection.split())/collection_size) / (len(doc.split()) + mu)  for k in range(l) for j in range(k+1, l)])\n",
        "        #print(query, doc)\n",
        "        #print(\"fu\", res)\n",
        "        return res\n",
        "\n",
        "    return {\n",
        "        doc:lambda_t * f_t(query, doc, collection) + \\\n",
        "            lambda_o * f_o(query, doc, collection) + \\\n",
        "            lambda_u * f_u(query, doc, collection) \n",
        "        for doc in doc_list\n",
        "    }\n",
        "\n",
        "def round_metric(num):\n",
        "    return round(num * 100, 2)\n",
        "\n",
        "def auto_evaluation(ref, hyp, facet):\n",
        "# we changed the weights in sentence_bleu, but the cell ouputs in this file is using previous wrong weights. Rerunning this notebook will get the results as in the paper.",
        "    ref = ref.strip()\n",
        "    hyp = hyp.strip()\n",
        "    tokenized_ref = word_tokenize(ref)\n",
        "    tokenized_hyp = word_tokenize(hyp)\n",
        "\n",
        "    rouge_score = rs.get_scores(hyp, ref)[0]['rouge-l']['f'] if hyp != '' else 0\n",
        "\n",
        "    return sentence_bleu([tokenized_ref], \n",
        "                            tokenized_hyp, \n",
        "                            weights=(1, 0, 0, 0),\n",
        "                            smoothing_function = SmoothingFunction().method1),\\\n",
        "            sentence_bleu([tokenized_ref], \n",
        "                            tokenized_hyp, \n",
        "                            weights=(1, 1, 0, 0),\n",
        "                            smoothing_function = SmoothingFunction().method1),\\\n",
        "            sentence_bleu([tokenized_ref], \n",
        "                            tokenized_hyp, \n",
        "                            weights=(1, 1, 1, 0),\n",
        "                            smoothing_function = SmoothingFunction().method1),\\\n",
        "            sentence_bleu([tokenized_ref], \n",
        "                            tokenized_hyp, \n",
        "                            weights=(1, 1, 1, 1),\n",
        "                            smoothing_function = SmoothingFunction().method1),\\\n",
        "            meteor_score([' '.join(tokenized_ref)], ' '.join(tokenized_hyp)),\\\n",
        "            rouge_score,\\\n",
        "            1 - sum([1 if ps.stem(constraint) not in set([ps.stem(w) for w in word_tokenize(hyp)]) else 0 for constraint in facet.split() ]) / len(facet.split())\n",
        "\n",
        "def evaluate_from_output(model_output):\n",
        "    \n",
        "    b1, b2, b3, b4 = [], [], [], []\n",
        "    m = []\n",
        "    r = []\n",
        "    c = []\n",
        "\n",
        "    t_b1, t_b2, t_b3, t_b4 = [], [], [], []\n",
        "    t_m = []\n",
        "    t_r = []\n",
        "    t_c = []\n",
        "\n",
        "    model_output_data = pd.read_csv(model_output)\n",
        "    for iter, row in model_output_data.iterrows():\n",
        "        query = model_output_data.at[iter, 'query']\n",
        "        facet = model_output_data.at[iter, 'facet']\n",
        "        ref = model_output_data.at[iter, 'reference']\n",
        "        generated_cq = model_output_data.at[iter, 'candidate']\n",
        "        \n",
        "        if iter % sample_every == 0: \n",
        "            print(iter, query, \"-\", facet, '-', generated_cq)\n",
        "        \n",
        "        # full reference evaluation\n",
        "        hyp_b1, hyp_b2, hyp_b3, hyp_b4, hyp_m, hyp_r, hyp_c = auto_evaluation(ref, generated_cq, facet)\n",
        "\n",
        "        b1.append(hyp_b1)\n",
        "        b2.append(hyp_b2)\n",
        "        b3.append(hyp_b3)\n",
        "        b4.append(hyp_b4)\n",
        "        m.append(hyp_m)\n",
        "        r.append(hyp_r)\n",
        "        c.append(hyp_c)\n",
        "\n",
        "        # question body evaluation\n",
        "        truncate_ref = ' '.join(ref.split()[template_len:])\n",
        "        truncate_generated_cq = ' '.join(generated_cq.split()[template_len:])\n",
        "        \n",
        "        t_hyp_b1, t_hyp_b2, t_hyp_b3, t_hyp_b4, t_hyp_m, t_hyp_r, t_hyp_c = auto_evaluation(truncate_ref, truncate_generated_cq, facet)\n",
        "\n",
        "        t_b1.append(t_hyp_b1)\n",
        "        t_b2.append(t_hyp_b2)\n",
        "        t_b3.append(t_hyp_b3)\n",
        "        t_b4.append(t_hyp_b4)\n",
        "        t_m.append(t_hyp_m)\n",
        "        t_r.append(t_hyp_r)\n",
        "        t_c.append(t_hyp_c)\n",
        "    \n",
        "    return b1, b2, b3, b4, m, r, c,\\\n",
        "           t_b1, t_b2, t_b3, t_b4, t_m, t_r, t_c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# RQ1.  How effective is facet information for clarifying question generation?\n",
        "\n",
        "To answer this question, we compare our proposed zero-shot facet-constrained approach with a similar method but using query subject instead of facet for constraints."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 1.1 No-facet (query subject) + neurologic decoding + WSDM ranker\n",
        "### 1.1.1 Generate inputs for neurologic decoding. \n",
        "\n",
        "* Generate constraints file from query subjects\n",
        "* Generate generation inputs file in the form of {query} + {template}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from word_forms.word_forms import get_word_forms\n",
        "facet_test_file = 'data/clariq_f/ClariQ-FKw-dev.tsv'\n",
        "facet_test_data = pd.read_csv(facet_test_file, sep='\\t')\n",
        "\n",
        "write_to_file = 'neurologic_decoding/dataset/clean/constraint/test.constraint.json'\n",
        "prompt_write_to_file = 'neurologic_decoding/dataset/clean/init/commongen.test.init.txt'\n",
        "no_prompt_write_to_file = 'neurologic_decoding/dataset/clean/init/commongen_no_prompt.test.init.txt'\n",
        "\n",
        "pos_tagger = spacy.load('en_core_web_sm')\n",
        "\n",
        "all_queries= []\n",
        "all_constraints = []\n",
        "for iter, row in facet_test_data.iterrows():\n",
        "    facet = facet_test_data.at[iter, 'facet_desc']\n",
        "    query = facet_test_data.at[iter, 'initial_request']\n",
        "    noun_in_query = [token.text for token in pos_tagger(query) if token.pos_ == 'NOUN']\n",
        "    propn_in_query = ' '.join([token.text.lower() for token in pos_tagger(query) if token.pos_ == 'PROPN'])\n",
        "    all_queries.append(query)\n",
        "    constraints = [[term] for term in noun_in_query]\n",
        "    if propn_in_query != '':\n",
        "        constraints += [[propn_in_query]] \n",
        "    all_constraints.append(constraints)\n",
        "\n",
        "\n",
        "with open(write_to_file, 'w') as output:\n",
        "    for constraints in all_constraints:\n",
        "        for k, prompt in enumerate(starting_texts):\n",
        "            json_str = json.dumps(constraints)\n",
        "            output.write(json_str)\n",
        "            output.write('\\n')\n",
        "\n",
        "with open(prompt_write_to_file, 'w') as output:\n",
        "    for query in all_queries:\n",
        "        for k, prompt in enumerate(starting_texts):\n",
        "            output.write(query + prompt)\n",
        "            output.write('\\n')\n",
        "\n",
        "with open(no_prompt_write_to_file, 'w') as output:\n",
        "    for query in all_queries:\n",
        "        output.write(query)\n",
        "        output.write('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 1.1.2 Run neurologic decoding.\n",
        "In AML terminal:\n",
        "\n",
        "Set up neurologic decoding environment.\n",
        "```\n",
        "cd neurologic_decoding\n",
        "conda create -n hug python=3.7\n",
        "conda activate hug\n",
        "pip install -r huggingface.txt\n",
        "```\n",
        "Run the generation code.\n",
        "```\n",
        "cd neurologic_decoding/zero_shot\n",
        "conda activate hug\n",
        "export PYTHONPATH=/home/azureuser/cloudfiles/code/Users/t-zhendwang/srconvsearch/neurologic_decoding\n",
        "bash decode_pt.sh 0 test gpt2nofacet\n",
        "``` \n",
        "\n",
        "Make sure we get the generation file 'gpt2nofacet'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 1.1.3 Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 tell me about cass county missouri - list homes sale - do you want to know what cass county cass county is cass county\n",
            "100 Find information on ontario california airport. - directions location - would you like to visit ontario ontario information ontario\n",
            "200 Where can I buy pressure washers? - washer - are you looking for washers or pressure washer\n",
            "300 Tell me more about Rocky Mountain News - recent events historical - are you looking for news about rocky mountain news\n",
            "400 Where should I order dog clean-up bags - specif bag type - would you like to order dog cleaning bags\n",
            "================================================================\n",
            "Full reference evaluation\n",
            "================================================================\n",
            "b1 0.2915715948377834 b2 0.12806654260239403 b3 0.07602362595745463 b4 0.051360580789430024\n",
            "rouge-L 0.34784465264878195\n",
            "m 0.28592940767560077\n",
            "c 0.09819607843137255\n",
            "================================================================\n",
            "Question body evaluation\n",
            "================================================================\n",
            "b1 0.1451359435328646 b2 0.043740224796047354 b3 0.020391620599505958 b4 0.025673300004377137\n",
            "rouge-L 0.1939367199929307\n",
            "m 0.14507277229967497\n",
            "c 0.09447058823529413\n"
          ]
        }
      ],
      "source": [
        "facet_test_file = 'data/clariq_f/ClariQ-FKw-dev.tsv'\n",
        "facet_test_data = pd.read_csv(facet_test_file, sep='\\t')\n",
        "_, facet_test_data = process_clariq_f(facet_test_data)\n",
        "\n",
        "r = []\n",
        "b1, b2, b3, b4 = [], [], [], []\n",
        "m = []\n",
        "c = []\n",
        "\n",
        "t_r = []\n",
        "t_b1, t_b2, t_b3, t_b4 = [], [], [], []\n",
        "t_m = []\n",
        "t_c = []\n",
        "\n",
        "model_output = 'zeroshot_subject_nd_wsdm.csv'\n",
        "generated_file = 'neurologic_decoding/zero_shot/gpt2nofacet'\n",
        "\n",
        "if os.path.isfile(model_output):\n",
        "    b1, b2, b3, b4, m, r, c, t_b1, t_b2, t_b3, t_b4, t_m, t_r, t_c = evaluate_from_output(model_output)\n",
        "\n",
        "else:\n",
        "    generated_cq_all_templates = open(generated_file, 'r').readlines()\n",
        "    generated_cq_grouped = [[generated_cq_all_templates[len(starting_texts) * k + l] \n",
        "                                for l in range(len(starting_texts))] \n",
        "                                for k in range(int(len(generated_cq_all_templates)/8))]\n",
        "    for iter, row in facet_test_data.iterrows():\n",
        "        query = facet_test_data.at[iter, 'initial_request']\n",
        "        facet = facet_test_data.at[iter, 'facet_desc']\n",
        "        ref = facet_test_data.at[iter, 'question']\n",
        "\n",
        "        generated_cqs = []\n",
        "        for full_sentence in generated_cq_grouped[iter]:\n",
        "            query = re.sub('\\[SEP\\]', '&', full_sentence).split('&')[0].strip()\n",
        "            generated_follow_up = re.sub('\\[SEP\\]', '&', full_sentence).split('&')[1].strip()\n",
        "            generated_cq = re.sub('[.?]', '&', generated_follow_up).split('&')[0].strip()\n",
        "            generated_cqs.append(generated_cq)\n",
        "        \n",
        "        noun_in_query = [token.text for token in pos_tagger(query) if token.pos_ == 'NOUN']\n",
        "        propn_in_query = [token.text.lower() for token in pos_tagger(query) if token.pos_ == 'PROPN']\n",
        "\n",
        "        template_scores = calculate_WSDM(query=' '.join(noun_in_query+propn_in_query), doc_list=generated_cqs)\n",
        "        sorted_template_scores = sorted(template_scores.items(), key = lambda x: x[1], reverse=True)\n",
        "        generated_cq = sorted(template_scores.keys(), key = lambda x: template_scores[x], reverse=True)[0] # Tie breaker? \n",
        "        facet_test_data.at[iter, 'generated'] = generated_cq\n",
        "        \n",
        "        if iter % sample_every == 0: \n",
        "            print(iter, query, \"-\", facet, '-', generated_cq)\n",
        "            #pprint.pprint(sorted_template_scores)\n",
        "\n",
        "        # full reference evaluation\n",
        "        hyp_b1, hyp_b2, hyp_b3, hyp_b4, hyp_m, hyp_r, hyp_c = auto_evaluation(ref, generated_cq, facet)\n",
        "\n",
        "        b1.append(hyp_b1)\n",
        "        b2.append(hyp_b2)\n",
        "        b3.append(hyp_b3)\n",
        "        b4.append(hyp_b4)\n",
        "        m.append(hyp_m)\n",
        "        r.append(hyp_r)\n",
        "        c.append(hyp_c)\n",
        "\n",
        "        # question body evaluation\n",
        "        truncate_ref = ' '.join(ref.split()[template_len:])\n",
        "        truncate_generated_cq = ' '.join(generated_cq.split()[template_len:])\n",
        "        \n",
        "        t_hyp_b1, t_hyp_b2, t_hyp_b3, t_hyp_b4, t_hyp_m, t_hyp_r, t_hyp_c = auto_evaluation(truncate_ref, truncate_generated_cq, facet)\n",
        "\n",
        "        t_b1.append(t_hyp_b1)\n",
        "        t_b2.append(t_hyp_b2)\n",
        "        t_b3.append(t_hyp_b3)\n",
        "        t_b4.append(t_hyp_b4)\n",
        "        t_m.append(t_hyp_m)\n",
        "        t_r.append(t_hyp_r)\n",
        "        t_c.append(t_hyp_c)\n",
        "    \n",
        "    output_df = facet_test_data[['initial_request', 'facet_desc', 'question', 'generated']]\n",
        "    output_df.columns = ['query', 'facet', 'reference', 'candidate']\n",
        "    output_df.to_csv(model_output)\n",
        "\n",
        "# full reference results\n",
        "print(\"================================================================\")\n",
        "print(\"Full reference evaluation\")\n",
        "print(\"================================================================\")\n",
        "print(\"b1\", np.mean(b1), \"b2\", np.mean(b2), \"b3\", np.mean(b3), \"b4\", np.mean(b4))\n",
        "print(\"rouge-L\", np.mean(r))\n",
        "print(\"m\", np.mean(m))\n",
        "print(\"c\", np.mean(c))\n",
        "\n",
        "zero_subject_nd_b1 = np.mean(b1)\n",
        "zero_subject_nd_b2 = np.mean(b2)\n",
        "zero_subject_nd_b3 = np.mean(b3)\n",
        "zero_subject_nd_b4 = np.mean(b4)\n",
        "zero_subject_nd_m = np.mean(m)\n",
        "zero_subject_nd_r = np.mean(r)\n",
        "zero_subject_nd_c = np.mean(c)\n",
        "\n",
        "# question body results\n",
        "print(\"================================================================\")\n",
        "print(\"Question body evaluation\")\n",
        "print(\"================================================================\")\n",
        "print(\"b1\", np.mean(t_b1), \"b2\", np.mean(t_b2), \"b3\", np.mean(t_b3), \"b4\", np.mean(t_b4))\n",
        "print(\"rouge-L\", np.mean(t_r))\n",
        "print(\"m\", np.mean(t_m))\n",
        "print(\"c\", np.mean(t_c))\n",
        "\n",
        "t_zero_subject_nd_b1 = np.mean(t_b1)\n",
        "t_zero_subject_nd_b2 = np.mean(t_b2)\n",
        "t_zero_subject_nd_b3 = np.mean(t_b3)\n",
        "t_zero_subject_nd_b4 = np.mean(t_b4)\n",
        "t_zero_subject_nd_m = np.mean(t_m)\n",
        "t_zero_subject_nd_r = np.mean(t_r)\n",
        "t_zero_subject_nd_c = np.mean(t_c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 1.2 Using facet\n",
        "### 1.2.1 Generate inputs for neurologic decoding. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from word_forms.word_forms import get_word_forms\n",
        "facet_test_file = 'data/clariq_f/ClariQ-FKw-dev.tsv'\n",
        "facet_test_data = pd.read_csv(facet_test_file, sep='\\t')\n",
        "\n",
        "write_to_file = 'neurologic_decoding/dataset/clean/constraint/test.constraint.json'\n",
        "prompt_write_to_file = 'neurologic_decoding/dataset/clean/init/commongen.test.init.txt'\n",
        "no_prompt_write_to_file = 'neurologic_decoding/dataset/clean/init/commongen_no_prompt.test.init.txt'\n",
        "\n",
        "pos_tagger = spacy.load('en_core_web_sm')\n",
        "\n",
        "all_queries= []\n",
        "all_constraints = []\n",
        "for iter, row in facet_test_data.iterrows():\n",
        "    facet = facet_test_data.at[iter, 'facet_desc']\n",
        "    query = facet_test_data.at[iter, 'initial_request']\n",
        "    noun_in_query = [token.text for token in pos_tagger(query) if token.pos_ == 'NOUN']\n",
        "    propn_in_query = [token.text for token in pos_tagger(query) if token.pos_ == 'PROPN']\n",
        "    all_queries.append(query)\n",
        "\n",
        "    constraints = [[term] for term in facet.split()]\n",
        "    #if propn_in_query != []: constraints += [[' '.join(propn_in_query)]]\n",
        "    #for facet_word in facet.split():\n",
        "    #for facet_word in facet.split():\n",
        "    #    many_forms = get_word_forms(facet_word)\n",
        "    #    constraints.append(list(set([word for k in many_forms.keys() for word in many_forms[k] ]+[facet_word])))\n",
        "    all_constraints.append(constraints)\n",
        "\n",
        "with open(write_to_file, 'w') as output:\n",
        "    for constraints in all_constraints:\n",
        "        for k, prompt in enumerate(starting_texts):\n",
        "            json_str = json.dumps(constraints)\n",
        "            output.write(json_str)\n",
        "            output.write('\\n')\n",
        "\n",
        "with open(prompt_write_to_file, 'w') as output:\n",
        "    for query in all_queries:\n",
        "        for k, prompt in enumerate(starting_texts):\n",
        "            output.write(query + prompt)\n",
        "            output.write('\\n')\n",
        "\n",
        "with open(no_prompt_write_to_file, 'w') as output:\n",
        "    for query in all_queries:\n",
        "        output.write(query)\n",
        "        output.write('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 1.2.2 Run neurologic decoding.\n",
        "In a terminal:\n",
        "\n",
        "Run the generation code.\n",
        "```\n",
        "cd neurologic_decoding/zero\n",
        "conda activate hug\n",
        "export PYTHONPATH=/home/azureuser/cloudfiles/code/Users/t-zhendwang/srconvsearch/neurologic_decoding\n",
        "bash decode_pt.sh 0 test gpt2facet\n",
        "``` \n",
        "\n",
        "Make sure we get the generation file 'gpt2facet'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 1.2.3 Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 tell me about cass county missouri - list homes sale - do you need information on the homes sale list\n",
            "100 Find information on ontario california airport. - directions location - do you want information and directions to your location\n",
            "200 Where can I buy pressure washers? - washer - would you like to buy washer washers\n",
            "300 Tell me more about Rocky Mountain News - recent events historical - are you interested in recent historical events\n",
            "400 Where should I order dog clean-up bags - specif bag type - do you want to know specifical bag type\n",
            "================================================================\n",
            "Full reference evaluation\n",
            "================================================================\n",
            "b1 0.41802206389295793 b2 0.17567385672035452 b3 0.0970553332319387 b4 0.0644596743271716\n",
            "rouge-L 0.4418867567785013\n",
            "m 0.3851975009273996\n",
            "c 0.9896078431372548\n",
            "================================================================\n",
            "Question body evaluation\n",
            "================================================================\n",
            "b1 0.3850744050543095 b2 0.18168457375984648 b3 0.1080585210854704 b4 0.08461452029751804\n",
            "rouge-L 0.43791545548265426\n",
            "m 0.3746952017378225\n",
            "c 0.9866666666666666\n"
          ]
        }
      ],
      "source": [
        "facet_test_file = 'data/clariq_f/ClariQ-FKw-dev.tsv'\n",
        "facet_test_data = pd.read_csv(facet_test_file, sep='\\t')\n",
        "_, facet_test_data = process_clariq_f(facet_test_data)\n",
        "\n",
        "b1, b2, b3, b4 = [], [], [], []\n",
        "m = []\n",
        "r = []\n",
        "c = []\n",
        "\n",
        "t_b1, t_b2, t_b3, t_b4 = [], [], [], []\n",
        "t_m = []\n",
        "t_r = []\n",
        "t_c = []\n",
        "\n",
        "model_output = 'zeroshot_nd_wsdm.csv'\n",
        "generated_file = 'neurologic_decoding/zero_shot/gpt2facet'\n",
        "\n",
        "if os.path.isfile(model_output):\n",
        "    b1, b2, b3, b4, m, r, c, t_b1, t_b2, t_b3, t_b4, t_m, t_r, t_c = evaluate_from_output(model_output)\n",
        "\n",
        "else:\n",
        "    generated_cq_all_templates = open(generated_file, 'r').readlines()\n",
        "    generated_cq_grouped = [[generated_cq_all_templates[len(starting_texts) * k + l] \n",
        "                                for l in range(len(starting_texts))] \n",
        "                                for k in range(int(len(generated_cq_all_templates)/8))]\n",
        "    for iter, row in facet_test_data.iterrows():\n",
        "        facet = facet_test_data.at[iter, 'facet_desc']\n",
        "        ref = facet_test_data.at[iter, 'question']\n",
        "        tokenized_ref = word_tokenize(ref)\n",
        "        facet_list = facet.split()\n",
        "\n",
        "        generated_cqs = []\n",
        "        for full_sentence in generated_cq_grouped[iter]:\n",
        "            query = re.sub('\\[SEP\\]', '&', full_sentence).split('&')[0].strip()\n",
        "            generated_follow_up = re.sub('\\[SEP\\]', '&', full_sentence).split('&')[1].strip()\n",
        "            generated_cq = re.sub('[.?]', '&', generated_follow_up).split('&')[0].strip()\n",
        "            generated_cqs.append(generated_cq)\n",
        "        \n",
        "        noun_in_query = [token.text for token in pos_tagger(query) if token.pos_ == 'NOUN']\n",
        "        propn_in_query = [token.text.lower() for token in pos_tagger(query) if token.pos_ == 'PROPN']\n",
        "\n",
        "        template_scores = calculate_WSDM(query=' '.join(noun_in_query+propn_in_query+facet_list), doc_list=generated_cqs)\n",
        "        sorted_template_scores = sorted(template_scores.items(), key = lambda x: x[1], reverse=True)\n",
        "        generated_cq = sorted(template_scores.keys(), key = lambda x: template_scores[x], reverse=True)[0] \n",
        "        facet_test_data.at[iter, 'generated'] = generated_cq\n",
        "        tokenized_hyp = word_tokenize(facet_test_data.at[iter, 'generated'])\n",
        "        \n",
        "        if iter % sample_every == 0: \n",
        "            print(iter, query, \"-\", facet, '-', generated_cq)\n",
        "            #pprint.pprint(sorted_template_scores)\n",
        "\n",
        "        # full reference evaluation\n",
        "        hyp_b1, hyp_b2, hyp_b3, hyp_b4, hyp_m, hyp_r, hyp_c = auto_evaluation(ref, generated_cq, facet)\n",
        "\n",
        "        b1.append(hyp_b1)\n",
        "        b2.append(hyp_b2)\n",
        "        b3.append(hyp_b3)\n",
        "        b4.append(hyp_b4)\n",
        "        m.append(hyp_m)\n",
        "        r.append(hyp_r)\n",
        "        c.append(hyp_c)\n",
        "\n",
        "        # question body evaluation\n",
        "        truncate_ref = ' '.join(ref.split()[template_len:])\n",
        "        truncate_generated_cq = ' '.join(generated_cq.split()[template_len:])\n",
        "        \n",
        "        t_hyp_b1, t_hyp_b2, t_hyp_b3, t_hyp_b4, t_hyp_m, t_hyp_r, t_hyp_c = auto_evaluation(truncate_ref, truncate_generated_cq, facet)\n",
        "\n",
        "        t_b1.append(t_hyp_b1)\n",
        "        t_b2.append(t_hyp_b2)\n",
        "        t_b3.append(t_hyp_b3)\n",
        "        t_b4.append(t_hyp_b4)\n",
        "        t_m.append(t_hyp_m)\n",
        "        t_r.append(t_hyp_r)\n",
        "        t_c.append(t_hyp_c)\n",
        "\n",
        "    output_df = facet_test_data[['initial_request', 'facet_desc', 'question', 'generated']]\n",
        "    output_df.columns = ['query', 'facet', 'reference', 'candidate']\n",
        "    output_df.to_csv('zeroshot_nd_wsdm.csv')\n",
        "\n",
        "# full reference results\n",
        "print(\"================================================================\")\n",
        "print(\"Full reference evaluation\")\n",
        "print(\"================================================================\")\n",
        "print(\"b1\", np.mean(b1), \"b2\", np.mean(b2), \"b3\", np.mean(b3), \"b4\", np.mean(b4))\n",
        "print(\"rouge-L\", np.mean(r))\n",
        "print(\"m\", np.mean(m))\n",
        "print(\"c\", np.mean(c))\n",
        "\n",
        "zero_nd_wsdm_b1 = np.mean(b1)\n",
        "zero_nd_wsdm_b2 = np.mean(b2)\n",
        "zero_nd_wsdm_b3 = np.mean(b3)\n",
        "zero_nd_wsdm_b4 = np.mean(b4)\n",
        "zero_nd_wsdm_m = np.mean(m)\n",
        "zero_nd_wsdm_r = np.mean(r)\n",
        "zero_nd_wsdm_c = np.mean(c)\n",
        "\n",
        "\n",
        "# question body results\n",
        "print(\"================================================================\")\n",
        "print(\"Question body evaluation\")\n",
        "print(\"================================================================\")\n",
        "print(\"b1\", np.mean(t_b1), \"b2\", np.mean(t_b2), \"b3\", np.mean(t_b3), \"b4\", np.mean(t_b4))\n",
        "print(\"rouge-L\", np.mean(t_r))\n",
        "print(\"m\", np.mean(t_m))\n",
        "print(\"c\", np.mean(t_c))\n",
        "\n",
        "t_zero_nd_wsdm_b1 = np.mean(t_b1)\n",
        "t_zero_nd_wsdm_b2 = np.mean(t_b2)\n",
        "t_zero_nd_wsdm_b3 = np.mean(t_b3)\n",
        "t_zero_nd_wsdm_b4 = np.mean(t_b4)\n",
        "t_zero_nd_wsdm_m = np.mean(t_m)\n",
        "t_zero_nd_wsdm_r = np.mean(t_r)\n",
        "t_zero_nd_wsdm_c = np.mean(t_c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 1.3 Comparing 1.1 and 1.2, and get the conclusion of RQ1: \"Facet is indeed very useful for clarifying question generation\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------------------------------\n",
            "|                               Full reference evaluation                               |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|MODEL                    BLEU1    BLEU2    BLEU3    BLEU4    METEOR   ROUGE    COVERAGE|\n",
            "-----------------------------------------------------------------------------------------\n",
            "|No facet                 29.16    12.81    7.6      5.14     28.59    34.78    9.82    |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|With facet               41.8     17.57    9.71     6.45     38.52    44.19    98.96   |\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "|                                Question body evaluation                               |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|MODEL                    BLEU1    BLEU2    BLEU3    BLEU4    METEOR   ROUGE    COVERAGE|\n",
            "-----------------------------------------------------------------------------------------\n",
            "|No facet                 14.51    4.37     2.04     2.57     14.51    19.39    9.45    |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|With facet               38.51    18.17    10.81    8.46     37.47    43.79    98.67   |\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|                               Full reference evaluation                               |\")\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('MODEL', 'BLEU1','BLEU2','BLEU3','BLEU4','METEOR','ROUGE','COVERAGE'))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('No facet', \n",
        "                                                            round_metric(zero_subject_nd_b1), \n",
        "                                                            round_metric(zero_subject_nd_b2), \n",
        "                                                            round_metric(zero_subject_nd_b3), \n",
        "                                                            round_metric(zero_subject_nd_b4), \n",
        "                                                            round_metric(zero_subject_nd_m), \n",
        "                                                            round_metric(zero_subject_nd_r), \n",
        "                                                            round_metric(zero_subject_nd_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('With facet', \n",
        "                                                            round_metric(zero_nd_wsdm_b1), \n",
        "                                                            round_metric(zero_nd_wsdm_b2), \n",
        "                                                            round_metric(zero_nd_wsdm_b3), \n",
        "                                                            round_metric(zero_nd_wsdm_b4), \n",
        "                                                            round_metric(zero_nd_wsdm_m), \n",
        "                                                            round_metric(zero_nd_wsdm_r),\n",
        "                                                            round_metric(zero_nd_wsdm_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|                                Question body evaluation                               |\")\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('MODEL', 'BLEU1','BLEU2','BLEU3','BLEU4','METEOR','ROUGE','COVERAGE'))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('No facet', \n",
        "                                                            round_metric(t_zero_subject_nd_b1), \n",
        "                                                            round_metric(t_zero_subject_nd_b2), \n",
        "                                                            round_metric(t_zero_subject_nd_b3), \n",
        "                                                            round_metric(t_zero_subject_nd_b4), \n",
        "                                                            round_metric(t_zero_subject_nd_m), \n",
        "                                                            round_metric(t_zero_subject_nd_r), \n",
        "                                                            round_metric(t_zero_subject_nd_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('With facet', \n",
        "                                                            round_metric(t_zero_nd_wsdm_b1), \n",
        "                                                            round_metric(t_zero_nd_wsdm_b2), \n",
        "                                                            round_metric(t_zero_nd_wsdm_b3), \n",
        "                                                            round_metric(t_zero_nd_wsdm_b4), \n",
        "                                                            round_metric(t_zero_nd_wsdm_m), \n",
        "                                                            round_metric(t_zero_nd_wsdm_r),\n",
        "                                                            round_metric(t_zero_nd_wsdm_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# RQ2. How does our zero-shot facet-constrained approach compare to existing facet-driven baselines?\n",
        "\n",
        "To answer this research question, we include some existing methods and a few other reasonable solutions not mentioned by previous works as our baseline models. Some of them are zero-shot, while others are not. However, we still compare their performances altogether to demonstrate the power of our zero-shot approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 2.1 Our approach.\n",
        "The same as in Section 1.2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 2.2 Template append facet approach\n",
        "This method appends facet words directly to question templates. This baseline is not ideal. Admittedly, it can generate good questions such as:\n",
        "\n",
        "𝑞: \"I am looking for information about South Africa.\"\n",
        "\n",
        "𝑓 : \"population\"\n",
        "\n",
        "𝑐𝑞: \"Are you interested in \\[population\\]\"\n",
        "\n",
        "However, sometimes the case is the facet itself cannot form a meaningful question:\n",
        "\n",
        "𝑞: \"I am interested in poker tournaments.\"\n",
        "\n",
        "𝑓 : \"online\"\n",
        "\n",
        "𝑐𝑞: \"Are you interested in \\[online\\]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 tell me about cass county missouri - list homes sale - do you want to know list homes sale\n",
            "100 Find information on ontario california airport. - directions location - do you want to know directions location\n",
            "200 Where can I buy pressure washers? - washer - would you like to washer\n",
            "300 Tell me more about Rocky Mountain News - recent events historical - do you want to know recent events historical\n",
            "400 Where should I order dog clean-up bags - specif bag type - do you want to know specif bag type\n",
            "================================================================\n",
            "Full reference evaluation\n",
            "================================================================\n",
            "b1 0.3855760869404698 b2 0.14068762090838577 b3 0.09981932597720988 b4 0.07806508022694261\n",
            "rouge-L 0.4602337846738828\n",
            "m 0.338690733001687\n",
            "c 1.0\n",
            "================================================================\n",
            "Question body evaluation\n",
            "================================================================\n",
            "b1 0.2548890473046932 b2 0.05751338297329412 b3 0.03272406390161522 b4 0.03350955982568657\n",
            "rouge-L 0.38575429251434623\n",
            "m 0.2208677078751569\n",
            "c 1.0\n"
          ]
        }
      ],
      "source": [
        "facet_test_file = 'data/clariq_f/ClariQ-FKw-dev.tsv'\n",
        "facet_test_data = pd.read_csv(facet_test_file, sep='\\t')\n",
        "_, facet_test_data = process_clariq_f(facet_test_data)\n",
        "\n",
        "b1, b2, b3, b4 = [], [], [], []\n",
        "m = []\n",
        "r = []\n",
        "c = []\n",
        "\n",
        "t_b1, t_b2, t_b3, t_b4 = [], [], [], []\n",
        "t_m = []\n",
        "t_r = []\n",
        "t_c = []\n",
        "\n",
        "model_output = 'template_facet.csv'\n",
        "\n",
        "if os.path.isfile(model_output):\n",
        "    b1, b2, b3, b4, m, r, c, t_b1, t_b2, t_b3, t_b4, t_m, t_r, t_c = evaluate_from_output(model_output)\n",
        "\n",
        "else:\n",
        "    for iter, row in facet_test_data.iterrows():\n",
        "        facet = facet_test_data.at[iter, 'facet_desc']\n",
        "        query = facet_test_data.at[iter, 'initial_request']\n",
        "        ref = facet_test_data.at[iter, 'question']\n",
        "        tokenized_ref = word_tokenize(ref)\n",
        "\n",
        "        template_scores = {}\n",
        "        for s_t in starting_texts:\n",
        "            generated_cq = s_t + ' ' + facet\n",
        "            generated_cq = process_generation(generated_cq)\n",
        "            generated_cq = ' '.join(word_tokenize(generated_cq))\n",
        "            template_scores[generated_cq] = calculatePerplexity(sentence=generated_cq, model=ppl_model, tokenizer=ppl_tokenizer)\n",
        "        \n",
        "        generated_cq = sorted(template_scores.keys(), key = lambda x: template_scores[x])[0] \n",
        "        facet_test_data.at[iter, 'generated'] = generated_cq\n",
        "\n",
        "        if iter % sample_every == 0:\n",
        "            print(iter, query, '-', facet, '-', generated_cq)\n",
        "            #pprint.pprint(template_scores)\n",
        "        \n",
        "        # full reference evaluation\n",
        "        hyp_b1, hyp_b2, hyp_b3, hyp_b4, hyp_m, hyp_r, hyp_c = auto_evaluation(ref, generated_cq, facet)\n",
        "\n",
        "        b1.append(hyp_b1)\n",
        "        b2.append(hyp_b2)\n",
        "        b3.append(hyp_b3)\n",
        "        b4.append(hyp_b4)\n",
        "        m.append(hyp_m)\n",
        "        r.append(hyp_r)\n",
        "        c.append(hyp_c)\n",
        "\n",
        "        # question body evaluation\n",
        "        truncate_ref = ' '.join(ref.split()[template_len:])\n",
        "        truncate_generated_cq = ' '.join(generated_cq.split()[template_len:])\n",
        "        \n",
        "        t_hyp_b1, t_hyp_b2, t_hyp_b3, t_hyp_b4, t_hyp_m, t_hyp_r, t_hyp_c = auto_evaluation(truncate_ref, truncate_generated_cq, facet)\n",
        "\n",
        "        t_b1.append(t_hyp_b1)\n",
        "        t_b2.append(t_hyp_b2)\n",
        "        t_b3.append(t_hyp_b3)\n",
        "        t_b4.append(t_hyp_b4)\n",
        "        t_m.append(t_hyp_m)\n",
        "        t_r.append(t_hyp_r)\n",
        "        t_c.append(t_hyp_c)\n",
        "\n",
        "    output_df = facet_test_data[['initial_request', 'facet_desc', 'question', 'generated']]\n",
        "    output_df.columns = ['query', 'facet', 'reference', 'candidate']\n",
        "    output_df.to_csv(model_output)\n",
        "\n",
        "# full reference results\n",
        "print(\"================================================================\")\n",
        "print(\"Full reference evaluation\")\n",
        "print(\"================================================================\")\n",
        "print(\"b1\", np.mean(b1), \"b2\", np.mean(b2), \"b3\", np.mean(b3), \"b4\", np.mean(b4))\n",
        "print(\"rouge-L\", np.mean(r))\n",
        "print(\"m\", np.mean(m))\n",
        "print(\"c\", np.mean(c))\n",
        "\n",
        "template_facet_b1 = np.mean(b1)\n",
        "template_facet_b2 = np.mean(b2)\n",
        "template_facet_b3 = np.mean(b3)\n",
        "template_facet_b4 = np.mean(b4)\n",
        "template_facet_m = np.mean(m)\n",
        "template_facet_r = np.mean(r)\n",
        "template_facet_c = np.mean(c)\n",
        "\n",
        "# question body results\n",
        "print(\"================================================================\")\n",
        "print(\"Question body evaluation\")\n",
        "print(\"================================================================\")\n",
        "print(\"b1\", np.mean(t_b1), \"b2\", np.mean(t_b2), \"b3\", np.mean(t_b3), \"b4\", np.mean(t_b4))\n",
        "print(\"rouge-L\", np.mean(t_r))\n",
        "print(\"m\", np.mean(t_m))\n",
        "print(\"c\", np.mean(t_c))\n",
        "\n",
        "t_template_facet_b1 = np.mean(t_b1)\n",
        "t_template_facet_b2 = np.mean(t_b2)\n",
        "t_template_facet_b3 = np.mean(t_b3)\n",
        "t_template_facet_b4 = np.mean(t_b4)\n",
        "t_template_facet_m = np.mean(t_m)\n",
        "t_template_facet_r = np.mean(t_r)\n",
        "t_template_facet_c = np.mean(t_c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 2.3 Finetuned GPT2 approach ([Previous SOTA by Sekulic](https://dl.acm.org/doi/abs/10.1145/3471158.3472257)), which uses inputs structured as:\n",
        "\n",
        "## {facet} \\[SEP\\] {query} \\[BOS\\] {clarifying question} \\[EOS\\]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 tell me about cass county missouri - list homes sale - are you looking for a specific list of the county homes sale in south america\n",
            "100 Find information on ontario california airport. - directions location - do you want to know about the location of the ontario cal\n",
            "200 Where can I buy pressure washers? - washer - do you want to know the difference between a vacuum was\n",
            "300 Tell me more about Rocky Mountain News - recent events historical -  would you like to know about historical events that happened\n",
            "400 Where should I order dog clean-up bags - specif bag type - would you like to know the type of bag\n",
            "================================================================\n",
            "Full reference evaluation\n",
            "================================================================\n",
            "b1 0.27745683326901827 b2 0.10785140305649912 b3 0.0603472849754279 b4 0.038306506703975875\n",
            "rouge-L 0.3170536641435762\n",
            "m 0.2855930527921367\n",
            "c 0.20854901960784314\n",
            "================================================================\n",
            "Question body evaluation\n",
            "================================================================\n",
            "b1 0.16383397692101645 b2 0.05368179453201418 b3 0.026705204131886495 b4 0.023738660852511858\n",
            "rouge-L 0.20138411844336415\n",
            "m 0.18413326944858474\n",
            "c 0.20207843137254902\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "sample_every = 100\n",
        "epochs = 8\n",
        "learning_rate = 5e-5\n",
        "warmup_steps = 1e2\n",
        "epsilon = 1e-8\n",
        "max_length = 128\n",
        "prompt_instruction = ''\n",
        "\n",
        "facet_test_file = 'data/clariq_f/ClariQ-FKw-dev.tsv'\n",
        "facet_test_data = pd.read_csv(facet_test_file, sep='\\t')\n",
        "_, facet_test_data = process_clariq_f(facet_test_data)\n",
        "\n",
        "b1, b2, b3, b4 = [], [], [], []\n",
        "m = []\n",
        "r = []\n",
        "c = []\n",
        "\n",
        "t_b1, t_b2, t_b3, t_b4 = [], [], [], []\n",
        "t_m = []\n",
        "t_r = []\n",
        "t_c = []\n",
        "\n",
        "model_output = 'sekulic.csv'\n",
        "\n",
        "if os.path.isfile(model_output):\n",
        "    b1, b2, b3, b4, m, r, c, t_b1, t_b2, t_b3, t_b4, t_m, t_r, t_c = evaluate_from_output(model_output)\n",
        "\n",
        "else:\n",
        "    clariq_f_train_file = 'data/clariq_f/ClariQ-FKw-train.tsv'\n",
        "    clariq_f_train_data = pd.read_csv(clariq_f_train_file, sep='\\t') \n",
        "    clariq_f_train_dict, clariq_f_train_data = process_clariq_f(clariq_f_train_data)\n",
        "    clariq_f_train_text_list = clariq_f_train_data['f_q_cq']\n",
        "\n",
        "    class GPT2Dataset(Dataset):\n",
        "        def __init__(self, txt_list, tokenizer, gpt2_type=\"gpt2\", max_length=768):\n",
        "            self.tokenizer = tokenizer\n",
        "            self.input_ids = []\n",
        "            self.attn_masks = []\n",
        "        \n",
        "            print(\"training text example\", txt_list[0])\n",
        "            for txt in txt_list:\n",
        "                encodings_dict = tokenizer(txt, truncation=True, max_length=max_length, padding=\"max_length\")\n",
        "                self.input_ids.append(T.tensor(encodings_dict['input_ids']))\n",
        "                self.attn_masks.append(T.tensor(encodings_dict['attention_mask']))\n",
        "        \n",
        "        def __len__(self):\n",
        "            return len(self.input_ids)\n",
        "        \n",
        "        def __getitem__(self, idx):\n",
        "            return self.input_ids[idx], self.attn_masks[idx] \n",
        "            \n",
        "    dataset = GPT2Dataset(clariq_f_train_text_list, tokenizer, max_length=max_length)\n",
        "\n",
        "    train_size = int(0.99 * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "\n",
        "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "    print('{:>5,} train /{:>5,} val'.format(train_size, val_size))\n",
        "\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset,\n",
        "        sampler = RandomSampler(train_dataset),\n",
        "        batch_size = batch_size\n",
        "        )\n",
        "\n",
        "    validation_dataloader = DataLoader(\n",
        "        val_dataset,\n",
        "        sampler = SequentialSampler(val_dataset),\n",
        "        batch_size = batch_size\n",
        "        )\n",
        "\n",
        "    device = T.device(\"cuda\")\n",
        "    model.cuda()\n",
        "\n",
        "    optimizer = AdamW(model.parameters(),\n",
        "        lr = learning_rate,\n",
        "        eps = epsilon\n",
        "        )\n",
        "\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "        num_warmup_steps = warmup_steps, \n",
        "        num_training_steps = total_steps\n",
        "        )\n",
        "\n",
        "    training_stats = []\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    for epoch_i in range(0, epochs):\n",
        "        # ========================================\n",
        "        #               Training\n",
        "        # ========================================\n",
        "        print(\"\")\n",
        "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "        print('Training...')\n",
        "\n",
        "        total_train_loss = 0\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "            b_input_ids = batch[0].to(device)\n",
        "            b_labels = batch[0].to(device)\n",
        "            b_masks = batch[1].to(device)\n",
        "\n",
        "            model.zero_grad()        \n",
        "\n",
        "            outputs = model(  b_input_ids,\n",
        "                            labels=b_labels, \n",
        "                            attention_mask = b_masks,\n",
        "                            token_type_ids=None\n",
        "                            )\n",
        "            loss = outputs[0]  \n",
        "\n",
        "            batch_loss = loss.item()\n",
        "            total_train_loss += batch_loss\n",
        "\n",
        "            # Get sample every x batches.\n",
        "            if step % sample_every == 0 and not step == 0:\n",
        "                print('  Batch {:>5,}  of  {:>5,}. Loss: {:>5,}.'.format(step, len(train_dataloader), batch_loss))\n",
        "                model.eval()\n",
        "                model.train()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "        # Calculate the average loss over all of the batches.\n",
        "        avg_train_loss = total_train_loss / len(train_dataloader)       \n",
        "        \n",
        "        # Measure how long this epoch took.\n",
        "\n",
        "        print(\"\")\n",
        "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))       \n",
        "        # ========================================\n",
        "        #               Validation\n",
        "        # ========================================\n",
        "        print(\"\")\n",
        "        print(\"Running Validation...\")\n",
        "        model.eval()\n",
        "        total_eval_loss = 0\n",
        "        nb_eval_steps = 0\n",
        "\n",
        "        # Evaluate data for one epoch\n",
        "        for batch in validation_dataloader:       \n",
        "            b_input_ids = batch[0].to(device)\n",
        "            b_labels = batch[0].to(device)\n",
        "            b_masks = batch[1].to(device)\n",
        "            \n",
        "            with T.no_grad():        \n",
        "                outputs  = model(b_input_ids, \n",
        "                                attention_mask = b_masks,\n",
        "                                labels=b_labels)         \n",
        "                loss = outputs[0]  \n",
        "                \n",
        "            batch_loss = loss.item()\n",
        "            total_eval_loss += batch_loss        \n",
        "\n",
        "        avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "\n",
        "        print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "\n",
        "        # Record all statistics from this epoch.\n",
        "        training_stats.append(\n",
        "            {\n",
        "                'epoch': epoch_i + 1,\n",
        "                'Training Loss': avg_train_loss,\n",
        "                'Valid. Loss': avg_val_loss,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Training complete!\")\n",
        "\n",
        "    output_dir = './model_save/'+prompt_instruction+str(epochs)+'/'\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "    # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "    # They can then be reloaded using `from_pretrained()`\n",
        "    model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "    model_to_save.save_pretrained(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "    clariq_f_test_file = 'data/clariq_f/ClariQ-FKw-dev.tsv'\n",
        "    clariq_f_test_data = pd.read_csv(clariq_f_test_file, sep='\\t') \n",
        "    clariq_f_test_dict, clariq_f_test_data = process_clariq_f(clariq_f_test_data)\n",
        "\n",
        "    rs = rouge.Rouge()\n",
        "    rs_list = []\n",
        "    b1, b2, b3, b4 = [], [], [], []\n",
        "    m = []\n",
        "    \n",
        "    for iter, row in clariq_f_test_data.iterrows():\n",
        "        query = clariq_f_test_data.at[iter, 'f_q']\n",
        "        ref = clariq_f_test_data.at[iter, 'question']\n",
        "        prompt_input = prompt_instruction + query\n",
        "        prompt_input_BOS = prompt_input + BOS\n",
        "        tokenized_prompt_input = T.tensor(tokenizer.encode(prompt_input_BOS)).unsqueeze(0)\n",
        "        tokenized_prompt_input = tokenized_prompt_input.to(device)\n",
        "        generated_text = ''\n",
        "        generated_cq = ''\n",
        "        attempt, max_attempt = 0, 4\n",
        "        while generated_cq == '' and attempt <= max_attempt: # to ensure the generation is not empty\n",
        "            attempt += 1\n",
        "            sample_outputs = model.generate(\n",
        "                tokenized_prompt_input,\n",
        "                do_sample=True,   \n",
        "                top_k=0, \n",
        "                max_length = len(tokenized_prompt_input[0]) + 10,\n",
        "                top_p=0.9, \n",
        "                temperature = 0.7,\n",
        "                num_return_sequences=1,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "                )\n",
        "\n",
        "            generated_text = tokenizer.decode(sample_outputs[0], skip_special_tokens=True)\n",
        "            generated_cq = generated_text[len(prompt_input):]\n",
        "            \n",
        "            clariq_f_test_data.at[iter, 'generated'] = process_generation(generated_cq)\n",
        "\n",
        "            if generated_cq == '':\n",
        "                generated_cq = 'nan'\n",
        "\n",
        "        # full reference evaluation\n",
        "        hyp_b1, hyp_b2, hyp_b3, hyp_b4, hyp_m, hyp_r, hyp_c = auto_evaluation(ref, generated_cq, facet)\n",
        "\n",
        "        b1.append(hyp_b1)\n",
        "        b2.append(hyp_b2)\n",
        "        b3.append(hyp_b3)\n",
        "        b4.append(hyp_b4)\n",
        "        m.append(hyp_m)\n",
        "        r.append(hyp_r)\n",
        "        c.append(hyp_c)\n",
        "\n",
        "        # question body evaluation\n",
        "        truncate_ref = ' '.join(ref.split()[template_len:])\n",
        "        truncate_generated_cq = ' '.join(generated_cq.split()[template_len:])\n",
        "        \n",
        "        t_hyp_b1, t_hyp_b2, t_hyp_b3, t_hyp_b4, t_hyp_m, t_hyp_r, t_hyp_c = auto_evaluation(truncate_ref, truncate_generated_cq, facet)\n",
        "\n",
        "        t_b1.append(t_hyp_b1)\n",
        "        t_b2.append(t_hyp_b2)\n",
        "        t_b3.append(t_hyp_b3)\n",
        "        t_b4.append(t_hyp_b4)\n",
        "        t_m.append(t_hyp_m)\n",
        "        t_r.append(t_hyp_r)\n",
        "        t_c.append(t_hyp_c)\n",
        "\n",
        "    output_df = clariq_f_test_data[['initial_request', 'facet_desc', 'question', 'generated']]\n",
        "    output_df.columns = ['query', 'facet', 'reference', 'candidate']\n",
        "    output_df.to_csv(model_output)\n",
        "\n",
        "# full reference results\n",
        "print(\"================================================================\")\n",
        "print(\"Full reference evaluation\")\n",
        "print(\"================================================================\")\n",
        "print(\"b1\", np.mean(b1), \"b2\", np.mean(b2), \"b3\", np.mean(b3), \"b4\", np.mean(b4))\n",
        "print(\"rouge-L\", np.mean(r))\n",
        "print(\"m\", np.mean(m))\n",
        "print(\"c\", np.mean(c))\n",
        "\n",
        "sekulic_b1 = np.mean(b1)\n",
        "sekulic_b2 = np.mean(b2)\n",
        "sekulic_b3 = np.mean(b3)\n",
        "sekulic_b4 = np.mean(b4)\n",
        "sekulic_m = np.mean(m)\n",
        "sekulic_r = np.mean(r)\n",
        "sekulic_c = np.mean(c)\n",
        "\n",
        "# question body results\n",
        "print(\"================================================================\")\n",
        "print(\"Question body evaluation\")\n",
        "print(\"================================================================\")\n",
        "print(\"b1\", np.mean(t_b1), \"b2\", np.mean(t_b2), \"b3\", np.mean(t_b3), \"b4\", np.mean(t_b4))\n",
        "print(\"rouge-L\", np.mean(t_r))\n",
        "print(\"m\", np.mean(t_m))\n",
        "print(\"c\", np.mean(t_c))\n",
        "\n",
        "t_sekulic_b1 = np.mean(t_b1)\n",
        "t_sekulic_b2 = np.mean(t_b2)\n",
        "t_sekulic_b3 = np.mean(t_b3)\n",
        "t_sekulic_b4 = np.mean(t_b4)\n",
        "t_sekulic_m = np.mean(t_m)\n",
        "t_sekulic_r = np.mean(t_r)\n",
        "t_sekulic_c = np.mean(t_c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 2.4 Prompt-based finetuned GPT2 approach, which uses inputs structured as:\n",
        "\n",
        "## {query} Ask a question that contains words in the list \\[{facet}\\] {clarifying question}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 tell me about cass county missouri - list homes sale - are you looking for homes in the city of missouri\n",
            "100 Find information on ontario california airport. - directions location - are you looking for directions to the nearest airport\n",
            "200 Where can I buy pressure washers? - washer - are you looking for a pressure was are you looking for a washing machine or a washing machine\n",
            "300 Tell me more about Rocky Mountain News - recent events historical - are you interested in historical events in the Rocky Mountain National park\n",
            "400 Where should I order dog clean-up bags - specif bag type - are you looking for a type of bag for the dog that contains the specif referring to\n",
            "================================================================\n",
            "Full reference evaluation\n",
            "================================================================\n",
            "b1 0.3278795935788096 b2 0.1457659141540518 b3 0.08566811950120864 b4 0.05633487804157568\n",
            "rouge-L 0.40809193442439157\n",
            "m 0.37547626523857963\n",
            "c 0.7254901960784315\n",
            "================================================================\n",
            "Question body evaluation\n",
            "================================================================\n",
            "b1 0.24126668552229252 b2 0.10373898361432152 b3 0.05732044392802645 b4 0.04493044603526353\n",
            "rouge-L 0.32992141402571434\n",
            "m 0.3184044180182667\n",
            "c 0.7141176470588235\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "sample_every = 100\n",
        "epochs = 8\n",
        "learning_rate = 5e-5\n",
        "warmup_steps = 1e2\n",
        "epsilon = 1e-8\n",
        "max_length = 128\n",
        "prompt_instruction = ''\n",
        "\n",
        "temperature = 0.1\n",
        "\n",
        "facet_test_file = 'data/clariq_f/ClariQ-FKw-dev.tsv'\n",
        "facet_test_data = pd.read_csv(facet_test_file, sep='\\t')\n",
        "_, facet_test_data = process_clariq_f(facet_test_data)\n",
        "\n",
        "b1, b2, b3, b4 = [], [], [], []\n",
        "m = []\n",
        "r = []\n",
        "c = []\n",
        "\n",
        "t_b1, t_b2, t_b3, t_b4 = [], [], [], []\n",
        "t_m = []\n",
        "t_r = []\n",
        "t_c = []\n",
        "\n",
        "model_output = 'ftgpt2_prompt' + '_temp' + str(temperature) + '.csv'\n",
        "\n",
        "if os.path.isfile(model_output):\n",
        "    b1, b2, b3, b4, m, r, c, t_b1, t_b2, t_b3, t_b4, t_m, t_r, t_c = evaluate_from_output(model_output)\n",
        "\n",
        "else:  \n",
        "    print(\"Output file not found, generating output.\") \n",
        "    model_dir = './model_save/'+str(epochs)+'/'\n",
        "    if os.path.exists(model_dir):\n",
        "        tokenizer = GPT2Tokenizer.from_pretrained(model_dir, bos_token=BOS, eos_token=EOS, pad_token=PAD) \n",
        "        configuration = GPT2Config.from_pretrained(model_dir, output_hidden_states=False)\n",
        "        model = GPT2LMHeadModel.from_pretrained(model_dir, config=configuration)\n",
        "        model.resize_token_embeddings(len(tokenizer))\n",
        "        model.cuda()\n",
        "    else:\n",
        "        print(\"Model checkpoint not found, finetuning.\")\n",
        "        clariq_f_train_file = 'data/clariq_f/ClariQ-FKw-train.tsv'\n",
        "        clariq_f_train_data = pd.read_csv(clariq_f_train_file, sep='\\t') \n",
        "        clariq_f_train_dict, clariq_f_train_data = process_clariq_f(clariq_f_train_data)\n",
        "        clariq_f_train_text_list = clariq_f_train_data['instructional_q_f_cq']\n",
        "\n",
        "        class GPT2Dataset(Dataset):\n",
        "            def __init__(self, txt_list, tokenizer, gpt2_type=\"gpt2\", max_length=768):\n",
        "                self.tokenizer = tokenizer\n",
        "                self.input_ids = []\n",
        "                self.attn_masks = []\n",
        "            \n",
        "                print(\"training text example\", txt_list[0])\n",
        "                for txt in txt_list:\n",
        "                    encodings_dict = tokenizer(txt, truncation=True, max_length=max_length, padding=\"max_length\")\n",
        "                    self.input_ids.append(T.tensor(encodings_dict['input_ids']))\n",
        "                    self.attn_masks.append(T.tensor(encodings_dict['attention_mask']))\n",
        "            \n",
        "            def __len__(self):\n",
        "                return len(self.input_ids)\n",
        "            \n",
        "            def __getitem__(self, idx):\n",
        "                return self.input_ids[idx], self.attn_masks[idx] \n",
        "            \n",
        "        dataset = GPT2Dataset(clariq_f_train_text_list, tokenizer, max_length=max_length)\n",
        "\n",
        "        train_size = int(0.99 * len(dataset))\n",
        "        val_size = len(dataset) - train_size\n",
        "\n",
        "        train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "        print('{:>5,} train /{:>5,} val'.format(train_size, val_size))\n",
        "\n",
        "        train_dataloader = DataLoader(\n",
        "        train_dataset,\n",
        "            sampler = RandomSampler(train_dataset),\n",
        "            batch_size = batch_size\n",
        "        )\n",
        "\n",
        "        validation_dataloader = DataLoader(\n",
        "        val_dataset,\n",
        "            sampler = SequentialSampler(val_dataset),\n",
        "            batch_size = batch_size\n",
        "        )\n",
        "\n",
        "        optimizer = AdamW(model.parameters(),\n",
        "            lr = learning_rate,\n",
        "            eps = epsilon\n",
        "        )\n",
        "\n",
        "        total_steps = len(train_dataloader) * epochs\n",
        "        scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "            num_warmup_steps = warmup_steps, \n",
        "            num_training_steps = total_steps\n",
        "        )\n",
        "\n",
        "        training_stats = []\n",
        "\n",
        "        for epoch_i in range(0, epochs):\n",
        "            # ========================================\n",
        "            #               Training\n",
        "            # ========================================\n",
        "            print(\"\")\n",
        "            print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "            print('Training...')\n",
        "\n",
        "            total_train_loss = 0\n",
        "\n",
        "            model.train()\n",
        "\n",
        "            for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "                b_input_ids = batch[0].to(device)\n",
        "                b_labels = batch[0].to(device)\n",
        "                b_masks = batch[1].to(device)\n",
        "\n",
        "                model.zero_grad()        \n",
        "\n",
        "                outputs = model(  b_input_ids,\n",
        "                                labels=b_labels, \n",
        "                                attention_mask = b_masks,\n",
        "                                token_type_ids=None\n",
        "                                )\n",
        "                loss = outputs[0]  \n",
        "\n",
        "                batch_loss = loss.item()\n",
        "                total_train_loss += batch_loss\n",
        "\n",
        "                # Get sample every x batches.\n",
        "                if step % sample_every == 0 and not step == 0:\n",
        "                    print('  Batch {:>5,}  of  {:>5,}. Loss: {:>5,}.'.format(step, len(train_dataloader), batch_loss))\n",
        "                    model.eval()\n",
        "                    model.train()\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "\n",
        "            # Calculate the average loss over all of the batches.\n",
        "            avg_train_loss = total_train_loss / len(train_dataloader)       \n",
        "            \n",
        "            # Measure how long this epoch took.\n",
        "\n",
        "            print(\"\")\n",
        "            print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))       \n",
        "            # ========================================\n",
        "            #               Validation\n",
        "            # ========================================\n",
        "            print(\"\")\n",
        "            print(\"Running Validation...\")\n",
        "            model.eval()\n",
        "            total_eval_loss = 0\n",
        "            nb_eval_steps = 0\n",
        "\n",
        "            # Evaluate data for one epoch\n",
        "            for batch in validation_dataloader:       \n",
        "                b_input_ids = batch[0].to(device)\n",
        "                b_labels = batch[0].to(device)\n",
        "                b_masks = batch[1].to(device)\n",
        "                \n",
        "                with T.no_grad():        \n",
        "                    outputs  = model(b_input_ids, \n",
        "                                    attention_mask = b_masks,\n",
        "                                    labels=b_labels)         \n",
        "                    loss = outputs[0]  \n",
        "                    \n",
        "                batch_loss = loss.item()\n",
        "                total_eval_loss += batch_loss        \n",
        "\n",
        "            avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "\n",
        "\n",
        "            print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "\n",
        "            # Record all statistics from this epoch.\n",
        "            training_stats.append(\n",
        "                {\n",
        "                    'epoch': epoch_i + 1,\n",
        "                    'Training Loss': avg_train_loss,\n",
        "                    'Valid. Loss': avg_val_loss,\n",
        "                }\n",
        "            )\n",
        "\n",
        "        print(\"\")\n",
        "        print(\"Training complete!\")\n",
        "\n",
        "        output_dir = model_dir\n",
        "        if not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "\n",
        "        print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "        # They can then be reloaded using `from_pretrained()`\n",
        "        model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "        model_to_save.save_pretrained(output_dir)\n",
        "        tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "    clariq_f_test_file = 'data/clariq_f/ClariQ-FKw-dev.tsv'\n",
        "    clariq_f_test_data = pd.read_csv(clariq_f_test_file, sep='\\t') \n",
        "    clariq_f_test_dict, clariq_f_test_data = process_clariq_f(clariq_f_test_data)\n",
        "\n",
        "    rs = rouge.Rouge()\n",
        "    rs_list = []\n",
        "    b1, b2, b3, b4 = [], [], [], []\n",
        "    m = []\n",
        "    \n",
        "    for iter, row in clariq_f_test_data.iterrows():\n",
        "        query = clariq_f_test_data.at[iter, 'instructional_q_f']\n",
        "        ref = clariq_f_test_data.at[iter, 'question']\n",
        "        tokenized_input = T.tensor(tokenizer.encode(query)).unsqueeze(0).to(device)\n",
        "        generated_text = ''\n",
        "        generated_cq = ''\n",
        "        \n",
        "        sample_outputs = model.generate(\n",
        "                tokenized_input,\n",
        "                do_sample=True,   \n",
        "                top_k=20, \n",
        "                max_length = len(tokenized_input[0]) + 32,\n",
        "                top_p=0.9, \n",
        "                temperature = temperature,\n",
        "                num_return_sequences=1,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        generated_text = tokenizer.decode(sample_outputs[0], skip_special_tokens=True)\n",
        "        \n",
        "        generated_cq = generated_text[len(query):].strip()\n",
        "        generated_cq = re.sub('\\[SEP\\]', ' ', generated_cq).strip()\n",
        "        generated_cq = re.sub('[.?]', '&', generated_cq).split('&')[0].strip()\n",
        "\n",
        "        clariq_f_test_data.at[iter, 'generated'] = generated_cq\n",
        "\n",
        "        if iter % sample_every == 0: \n",
        "            print(iter, query, '-', generated_cq)\n",
        "\n",
        "        # full reference evaluation\n",
        "        hyp_b1, hyp_b2, hyp_b3, hyp_b4, hyp_m, hyp_r, hyp_c = auto_evaluation(ref, generated_cq, facet)\n",
        "\n",
        "        b1.append(hyp_b1)\n",
        "        b2.append(hyp_b2)\n",
        "        b3.append(hyp_b3)\n",
        "        b4.append(hyp_b4)\n",
        "        m.append(hyp_m)\n",
        "        r.append(hyp_r)\n",
        "        c.append(hyp_c)\n",
        "\n",
        "        # question body evaluation\n",
        "        truncate_ref = ' '.join(ref.split()[template_len:])\n",
        "        truncate_generated_cq = ' '.join(generated_cq.split()[template_len:])\n",
        "        \n",
        "        t_hyp_b1, t_hyp_b2, t_hyp_b3, t_hyp_b4, t_hyp_m, t_hyp_r, t_hyp_c = auto_evaluation(truncate_ref, truncate_generated_cq, facet)\n",
        "\n",
        "        t_b1.append(t_hyp_b1)\n",
        "        t_b2.append(t_hyp_b2)\n",
        "        t_b3.append(t_hyp_b3)\n",
        "        t_b4.append(t_hyp_b4)\n",
        "        t_m.append(t_hyp_m)\n",
        "        t_r.append(t_hyp_r)\n",
        "        t_c.append(t_hyp_c)\n",
        "\n",
        "    output_df = clariq_f_test_data[['initial_request', 'facet_desc', 'question', 'generated']]\n",
        "    output_df.columns = ['query', 'facet', 'reference', 'candidate']\n",
        "    output_df.to_csv(model_output)\n",
        "\n",
        "\n",
        "# full reference results\n",
        "print(\"================================================================\")\n",
        "print(\"Full reference evaluation\")\n",
        "print(\"================================================================\")\n",
        "print(\"b1\", np.mean(b1), \"b2\", np.mean(b2), \"b3\", np.mean(b3), \"b4\", np.mean(b4))\n",
        "print(\"rouge-L\", np.mean(r))\n",
        "print(\"m\", np.mean(m))\n",
        "print(\"c\", np.mean(c))\n",
        "\n",
        "ftgpt2_prompt_b1 = np.mean(b1)\n",
        "ftgpt2_prompt_b2 = np.mean(b2)\n",
        "ftgpt2_prompt_b3 = np.mean(b3)\n",
        "ftgpt2_prompt_b4 = np.mean(b4)\n",
        "ftgpt2_prompt_m = np.mean(m)\n",
        "ftgpt2_prompt_r = np.mean(r)\n",
        "ftgpt2_prompt_c = np.mean(c)\n",
        "\n",
        "# question body results\n",
        "print(\"================================================================\")\n",
        "print(\"Question body evaluation\")\n",
        "print(\"================================================================\")\n",
        "print(\"b1\", np.mean(t_b1), \"b2\", np.mean(t_b2), \"b3\", np.mean(t_b3), \"b4\", np.mean(t_b4))\n",
        "print(\"rouge-L\", np.mean(t_r))\n",
        "print(\"m\", np.mean(t_m))\n",
        "print(\"c\", np.mean(t_c))\n",
        "\n",
        "t_ftgpt2_prompt_b1 = np.mean(t_b1)\n",
        "t_ftgpt2_prompt_b2 = np.mean(t_b2)\n",
        "t_ftgpt2_prompt_b3 = np.mean(t_b3)\n",
        "t_ftgpt2_prompt_b4 = np.mean(t_b4)\n",
        "t_ftgpt2_prompt_m = np.mean(t_m)\n",
        "t_ftgpt2_prompt_r = np.mean(t_r)\n",
        "t_ftgpt2_prompt_c = np.mean(t_c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 2.5 Comparing 2.1-2.4, and get the conclusion for RQ2. \"Our zero-shot facet-constrained approach significantly improve baseline methods.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------------------------------\n",
            "|                               Full reference evaluation                               |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|MODEL                    BLEU1    BLEU2    BLEU3    BLEU4    METEOR   ROUGE    COVERAGE|\n",
            "-----------------------------------------------------------------------------------------\n",
            "|Template append facet    38.56    14.07    9.98     7.81     33.87    46.02    100.0   |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|Existing finetuned GPT2  27.75    10.79    6.03     3.83     28.56    31.71    20.85   |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|Prompt finetuned GPT2    32.79    14.58    8.57     5.63     37.55    40.81    72.55   |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|Ours                     41.8     17.57    9.71     6.45     38.52    44.19    98.96   |\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "|                                Question body evaluation                               |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|MODEL                    BLEU1    BLEU2    BLEU3    BLEU4    METEOR   ROUGE    COVERAGE|\n",
            "-----------------------------------------------------------------------------------------\n",
            "|Template append facet    25.49    5.75     3.27     3.35     22.09    38.58    100.0   |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|Existing finetuned GPT2  16.38    5.37     2.67     2.37     18.41    20.14    20.21   |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|Prompt finetuned GPT2    24.13    10.37    5.73     4.49     31.84    32.99    71.41   |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|Ours                     38.51    18.17    10.81    8.46     37.47    43.79    98.67   |\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|                               Full reference evaluation                               |\")\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('MODEL', 'BLEU1','BLEU2','BLEU3','BLEU4','METEOR','ROUGE','COVERAGE'))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('Template append facet', \n",
        "                                                            round_metric(template_facet_b1), \n",
        "                                                            round_metric(template_facet_b2), \n",
        "                                                            round_metric(template_facet_b3), \n",
        "                                                            round_metric(template_facet_b4), \n",
        "                                                            round_metric(template_facet_m), \n",
        "                                                            round_metric(template_facet_r), \n",
        "                                                            round_metric(template_facet_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('Existing finetuned GPT2', \n",
        "                                                            round_metric(sekulic_b1), \n",
        "                                                            round_metric(sekulic_b2), \n",
        "                                                            round_metric(sekulic_b3), \n",
        "                                                            round_metric(sekulic_b4), \n",
        "                                                            round_metric(sekulic_m), \n",
        "                                                            round_metric(sekulic_r),\n",
        "                                                            round_metric(sekulic_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('Prompt finetuned GPT2', \n",
        "                                                            round_metric(ftgpt2_prompt_b1), \n",
        "                                                            round_metric(ftgpt2_prompt_b2), \n",
        "                                                            round_metric(ftgpt2_prompt_b3), \n",
        "                                                            round_metric(ftgpt2_prompt_b4), \n",
        "                                                            round_metric(ftgpt2_prompt_m), \n",
        "                                                            round_metric(ftgpt2_prompt_r),\n",
        "                                                            round_metric(ftgpt2_prompt_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('Ours', \n",
        "                                                            round_metric(zero_nd_wsdm_b1), \n",
        "                                                            round_metric(zero_nd_wsdm_b2), \n",
        "                                                            round_metric(zero_nd_wsdm_b3), \n",
        "                                                            round_metric(zero_nd_wsdm_b4), \n",
        "                                                            round_metric(zero_nd_wsdm_m), \n",
        "                                                            round_metric(zero_nd_wsdm_r),\n",
        "                                                            round_metric(zero_nd_wsdm_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|                                Question body evaluation                               |\")\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('MODEL', 'BLEU1','BLEU2','BLEU3','BLEU4','METEOR','ROUGE','COVERAGE'))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('Template append facet', \n",
        "                                                            round_metric(t_template_facet_b1), \n",
        "                                                            round_metric(t_template_facet_b2), \n",
        "                                                            round_metric(t_template_facet_b3), \n",
        "                                                            round_metric(t_template_facet_b4), \n",
        "                                                            round_metric(t_template_facet_m), \n",
        "                                                            round_metric(t_template_facet_r), \n",
        "                                                            round_metric(t_template_facet_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('Existing finetuned GPT2', \n",
        "                                                            round_metric(t_sekulic_b1), \n",
        "                                                            round_metric(t_sekulic_b2), \n",
        "                                                            round_metric(t_sekulic_b3), \n",
        "                                                            round_metric(t_sekulic_b4), \n",
        "                                                            round_metric(t_sekulic_m), \n",
        "                                                            round_metric(t_sekulic_r),\n",
        "                                                            round_metric(t_sekulic_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('Prompt finetuned GPT2', \n",
        "                                                            round_metric(t_ftgpt2_prompt_b1), \n",
        "                                                            round_metric(t_ftgpt2_prompt_b2), \n",
        "                                                            round_metric(t_ftgpt2_prompt_b3), \n",
        "                                                            round_metric(t_ftgpt2_prompt_b4), \n",
        "                                                            round_metric(t_ftgpt2_prompt_m), \n",
        "                                                            round_metric(t_ftgpt2_prompt_r),\n",
        "                                                            round_metric(t_ftgpt2_prompt_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('Ours', \n",
        "                                                            round_metric(t_zero_nd_wsdm_b1), \n",
        "                                                            round_metric(t_zero_nd_wsdm_b2), \n",
        "                                                            round_metric(t_zero_nd_wsdm_b3), \n",
        "                                                            round_metric(t_zero_nd_wsdm_b4), \n",
        "                                                            round_metric(t_zero_nd_wsdm_m), \n",
        "                                                            round_metric(t_zero_nd_wsdm_r),\n",
        "                                                            round_metric(t_zero_nd_wsdm_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# RQ3. How does our question ranking model compare to other methods?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 3.1 Perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 tell me about cass county missouri - list homes sale - do you want to know the list of sale homes\n",
            "100 Find information on ontario california airport. - directions location - do you want to know the location and directions of the airport\n",
            "200 Where can I buy pressure washers? - washer - are you interested in buying washer/dryer washers or do you have any questions\n",
            "300 Tell me more about Rocky Mountain News - recent events historical - would you like to know more about recent historical events\n",
            "400 Where should I order dog clean-up bags - specif bag type - do you need to be specifical about the type of bag you want to order\n",
            "================================================================\n",
            "Full reference evaluation\n",
            "================================================================\n",
            "b1 0.4277656503772235 b2 0.19007325089147045 b3 0.11408309937495881 b4 0.07559809979039746\n",
            "rouge-L 0.45158240338278777\n",
            "m 0.40982020211837566\n",
            "c 0.9782352941176471\n",
            "================================================================\n",
            "Question body evaluation\n",
            "================================================================\n",
            "b1 0.36796210388220796 b2 0.1661403144295007 b3 0.0912581858918981 b4 0.06500624937497966\n",
            "rouge-L 0.40577732395656707\n",
            "m 0.37771407676902025\n",
            "c 0.9747058823529412\n"
          ]
        }
      ],
      "source": [
        "facet_test_file = 'data/clariq_f/ClariQ-FKw-dev.tsv'\n",
        "facet_test_data = pd.read_csv(facet_test_file, sep='\\t')\n",
        "_, facet_test_data = process_clariq_f(facet_test_data)\n",
        "\n",
        "r = []\n",
        "b1, b2, b3, b4 = [], [], [], []\n",
        "m = []\n",
        "c = []\n",
        "\n",
        "t_r = []\n",
        "t_b1, t_b2, t_b3, t_b4 = [], [], [], []\n",
        "t_m = []\n",
        "t_c = []\n",
        "\n",
        "model_output = 'zeroshot_nd_pp.csv'\n",
        "generated_file = 'neurologic_decoding/zero_shot/gpt2facet'\n",
        "\n",
        "if os.path.isfile(model_output):\n",
        "    b1, b2, b3, b4, m, r, c, t_b1, t_b2, t_b3, t_b4, t_m, t_r, t_c = evaluate_from_output(model_output)\n",
        "\n",
        "else:\n",
        "    generated_cq_all_templates = open(generated_file, 'r').readlines()\n",
        "    generated_cq_grouped = [[generated_cq_all_templates[len(starting_texts) * k + l] \n",
        "                                for l in range(len(starting_texts))] \n",
        "                                for k in range(int(len(generated_cq_all_templates)/8))]\n",
        "\n",
        "    for iter, row in facet_test_data.iterrows():\n",
        "        facet = facet_test_data.at[iter, 'facet_desc']\n",
        "        ref = facet_test_data.at[iter, 'question']\n",
        "\n",
        "        template_scores = {}\n",
        "        for full_sentence in generated_cq_grouped[iter]:\n",
        "            query = re.sub('\\[SEP\\]', '&', full_sentence).split('&')[0].strip()\n",
        "            generated_follow_up = re.sub('\\[SEP\\]', '&', full_sentence).split('&')[1].strip()\n",
        "            generated_cq = re.sub('[.?]', '&', generated_follow_up).split('&')[0].strip()\n",
        "            constraint_penalty = 1\n",
        "            for constraint in force_flexible:\n",
        "                if ps.stem(constraint) not in set([ps.stem(w) for w in word_tokenize(generated_cq)]):\n",
        "                    constraint_penalty *= 2\n",
        "            template_scores[generated_cq] = calculatePerplexity(sentence=full_sentence, model=ppl_model, tokenizer=ppl_tokenizer) * constraint_penalty\n",
        "        \n",
        "        generated_cq = sorted(template_scores.keys(), key = lambda x: template_scores[x])[0] \n",
        "        facet_test_data.at[iter, 'generated'] = generated_cq\n",
        "        \n",
        "        # full reference evaluation\n",
        "        hyp_b1, hyp_b2, hyp_b3, hyp_b4, hyp_m, hyp_r, hyp_c = auto_evaluation(ref, generated_cq, facet)\n",
        "\n",
        "        b1.append(hyp_b1)\n",
        "        b2.append(hyp_b2)\n",
        "        b3.append(hyp_b3)\n",
        "        b4.append(hyp_b4)\n",
        "        m.append(hyp_m)\n",
        "        r.append(hyp_r)\n",
        "        c.append(hyp_c)\n",
        "\n",
        "        # question body evaluation\n",
        "        truncate_ref = ' '.join(ref.split()[template_len:])\n",
        "        truncate_generated_cq = ' '.join(generated_cq.split()[template_len:])\n",
        "        \n",
        "        t_hyp_b1, t_hyp_b2, t_hyp_b3, t_hyp_b4, t_hyp_m, t_hyp_r, t_hyp_c = auto_evaluation(truncate_ref, truncate_generated_cq, facet)\n",
        "\n",
        "        t_b1.append(t_hyp_b1)\n",
        "        t_b2.append(t_hyp_b2)\n",
        "        t_b3.append(t_hyp_b3)\n",
        "        t_b4.append(t_hyp_b4)\n",
        "        t_m.append(t_hyp_m)\n",
        "        t_r.append(t_hyp_r)\n",
        "        t_c.append(t_hyp_c)\n",
        "    \n",
        "    output_df = facet_test_data[['initial_request', 'facet_desc', 'question', 'generated']]\n",
        "    output_df.columns = ['query', 'facet', 'reference', 'candidate']\n",
        "    output_df.to_csv(model_output)\n",
        "\n",
        "\n",
        "# full reference results\n",
        "print(\"================================================================\")\n",
        "print(\"Full reference evaluation\")\n",
        "print(\"================================================================\")\n",
        "print(\"b1\", np.mean(b1), \"b2\", np.mean(b2), \"b3\", np.mean(b3), \"b4\", np.mean(b4))\n",
        "print(\"rouge-L\", np.mean(r))\n",
        "print(\"m\", np.mean(m))\n",
        "print(\"c\", np.mean(c))\n",
        "\n",
        "zero_nd_pp_b1 = np.mean(b1)\n",
        "zero_nd_pp_b2 = np.mean(b2)\n",
        "zero_nd_pp_b3 = np.mean(b3)\n",
        "zero_nd_pp_b4 = np.mean(b4)\n",
        "zero_nd_pp_m = np.mean(m)\n",
        "zero_nd_pp_r = np.mean(r)\n",
        "zero_nd_pp_c = np.mean(c)\n",
        "\n",
        "# question body results\n",
        "print(\"================================================================\")\n",
        "print(\"Question body evaluation\")\n",
        "print(\"================================================================\")\n",
        "print(\"b1\", np.mean(t_b1), \"b2\", np.mean(t_b2), \"b3\", np.mean(t_b3), \"b4\", np.mean(t_b4))\n",
        "print(\"rouge-L\", np.mean(t_r))\n",
        "print(\"m\", np.mean(t_m))\n",
        "print(\"c\", np.mean(t_c))\n",
        "\n",
        "t_zero_nd_pp_b1 = np.mean(t_b1)\n",
        "t_zero_nd_pp_b2 = np.mean(t_b2)\n",
        "t_zero_nd_pp_b3 = np.mean(t_b3)\n",
        "t_zero_nd_pp_b4 = np.mean(t_b4)\n",
        "t_zero_nd_pp_m = np.mean(t_m)\n",
        "t_zero_nd_pp_r = np.mean(t_r)\n",
        "t_zero_nd_pp_c = np.mean(t_c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 3.2 AutoScores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 tell me about cass county missouri - list homes sale - are you looking for a list of sale homes\n",
            "100 Find information on ontario california airport. - directions location - would you like to send directions to your location\n",
            "200 Where can I buy pressure washers? - washer - are you looking for a washer\n",
            "300 Tell me more about Rocky Mountain News - recent events historical - do you need information on recent historical events\n",
            "400 Where should I order dog clean-up bags - specif bag type - do you want to know specifical bag type\n",
            "================================================================\n",
            "Full reference evaluation\n",
            "================================================================\n",
            "b1 0.4342383247669103 b2 0.20474687017114682 b3 0.12927541734987133 b4 0.09641742120931246\n",
            "rouge-L 0.4786832211408991\n",
            "m 0.41033188202381593\n",
            "c 0.9827843137254902\n",
            "================================================================\n",
            "Question body evaluation\n",
            "================================================================\n",
            "b1 0.3488173362287711 b2 0.1624716052304113 b3 0.10472807656908299 b4 0.08069872668358835\n",
            "rouge-L 0.434214050123522\n",
            "m 0.3388249958920614\n",
            "c 0.9582745098039215\n"
          ]
        }
      ],
      "source": [
        "facet_test_file = 'data/clariq_f/ClariQ-FKw-dev.tsv'\n",
        "facet_test_data = pd.read_csv(facet_test_file, sep='\\t')\n",
        "_, facet_test_data = process_clariq_f(facet_test_data)\n",
        "\n",
        "r = []\n",
        "b1, b2, b3, b4 = [], [], [], []\n",
        "m = []\n",
        "c = []\n",
        "\n",
        "t_r = []\n",
        "t_b1, t_b2, t_b3, t_b4 = [], [], [], []\n",
        "t_m = []\n",
        "t_c = []\n",
        "\n",
        "model_output = 'zeroshot_nd_auto.csv'\n",
        "generated_file = 'neurologic_decoding/zero_shot/gpt2facet'\n",
        "\n",
        "if os.path.isfile(model_output):\n",
        "    b1, b2, b3, b4, m, r, c, t_b1, t_b2, t_b3, t_b4, t_m, t_r, t_c = evaluate_from_output(model_output)\n",
        "\n",
        "else:\n",
        "    generated_cq_all_templates = open(generated_file, 'r').readlines()\n",
        "    generated_cq_grouped = [[generated_cq_all_templates[len(starting_texts) * k + l] \n",
        "                                for l in range(len(starting_texts))] \n",
        "                                for k in range(int(len(generated_cq_all_templates)/8))]\n",
        "\n",
        "    for iter, row in facet_test_data.iterrows():\n",
        "        query = facet_test_data.at[iter, 'initial_request']\n",
        "        facet = facet_test_data.at[iter, 'facet_desc']\n",
        "        ref = facet_test_data.at[iter, 'question']\n",
        "        facet_list = facet.split()\n",
        "\n",
        "        noun_in_query = [token.text for token in pos_tagger(query) if token.pos_ == 'NOUN']\n",
        "        propn_in_query = [token.text.lower() for token in pos_tagger(query) if token.pos_ == 'PROPN']\n",
        "\n",
        "        template_scores = {}\n",
        "        for full_sentence in generated_cq_grouped[iter]:\n",
        "            query = re.sub('\\[SEP\\]', '&', full_sentence).split('&')[0].strip()\n",
        "            generated_follow_up = re.sub('\\[SEP\\]', '&', full_sentence).split('&')[1].strip()\n",
        "            generated_cq = re.sub('[.?]', '&', generated_follow_up).split('&')[0].strip()  \n",
        "            template_scores[generated_cq] = rs.get_scores(generated_cq, ' '.join(noun_in_query + propn_in_query + facet_list))[0]['rouge-l']['f'] \\\n",
        "                                + sentence_bleu([word_tokenize(' '.join(noun_in_query + propn_in_query + facet_list))], word_tokenize(generated_cq), \n",
        "                                weights=(1, 1, 1, 1), smoothing_function = SmoothingFunction().method1) + \\\n",
        "                                meteor_score([word_tokenize(' '.join(noun_in_query + propn_in_query + facet_list))], word_tokenize(generated_cq))\n",
        "        \n",
        "        sorted_template_scores = sorted(template_scores.items(), key = lambda x: x[1], reverse=True)\n",
        "        generated_cq = sorted(template_scores.keys(), key = lambda x: template_scores[x], reverse=True)[0] \n",
        "        facet_test_data.at[iter, 'generated'] = generated_cq\n",
        "\n",
        "        # full reference evaluation\n",
        "        hyp_b1, hyp_b2, hyp_b3, hyp_b4, hyp_m, hyp_r, hyp_c = auto_evaluation(ref, generated_cq, facet)\n",
        "\n",
        "        b1.append(hyp_b1)\n",
        "        b2.append(hyp_b2)\n",
        "        b3.append(hyp_b3)\n",
        "        b4.append(hyp_b4)\n",
        "        m.append(hyp_m)\n",
        "        r.append(hyp_r)\n",
        "        c.append(hyp_c)\n",
        "\n",
        "        # question body evaluation\n",
        "        truncate_ref = ' '.join(ref.split()[template_len:])\n",
        "        truncate_generated_cq = ' '.join(generated_cq.split()[template_len:])\n",
        "        \n",
        "        t_hyp_b1, t_hyp_b2, t_hyp_b3, t_hyp_b4, t_hyp_m, t_hyp_r, t_hyp_c = auto_evaluation(truncate_ref, truncate_generated_cq, facet)\n",
        "\n",
        "        t_b1.append(t_hyp_b1)\n",
        "        t_b2.append(t_hyp_b2)\n",
        "        t_b3.append(t_hyp_b3)\n",
        "        t_b4.append(t_hyp_b4)\n",
        "        t_m.append(t_hyp_m)\n",
        "        t_r.append(t_hyp_r)\n",
        "        t_c.append(t_hyp_c)\n",
        "\n",
        "    output_df = facet_test_data[['initial_request', 'facet_desc', 'question', 'generated']]\n",
        "    output_df.columns = ['query', 'facet', 'reference', 'candidate']\n",
        "    output_df.to_csv(model_output)\n",
        "\n",
        "# full reference results\n",
        "print(\"================================================================\")\n",
        "print(\"Full reference evaluation\")\n",
        "print(\"================================================================\")\n",
        "print(\"b1\", np.mean(b1), \"b2\", np.mean(b2), \"b3\", np.mean(b3), \"b4\", np.mean(b4))\n",
        "print(\"rouge-L\", np.mean(r))\n",
        "print(\"m\", np.mean(m))\n",
        "print(\"c\", np.mean(c))\n",
        "\n",
        "zero_nd_auto_b1 = np.mean(b1)\n",
        "zero_nd_auto_b2 = np.mean(b2)\n",
        "zero_nd_auto_b3 = np.mean(b3)\n",
        "zero_nd_auto_b4 = np.mean(b4)\n",
        "zero_nd_auto_m = np.mean(m)\n",
        "zero_nd_auto_r = np.mean(r)\n",
        "zero_nd_auto_c = np.mean(c)\n",
        "\n",
        "# question body results\n",
        "print(\"================================================================\")\n",
        "print(\"Question body evaluation\")\n",
        "print(\"================================================================\")\n",
        "print(\"b1\", np.mean(t_b1), \"b2\", np.mean(t_b2), \"b3\", np.mean(t_b3), \"b4\", np.mean(t_b4))\n",
        "print(\"rouge-L\", np.mean(t_r))\n",
        "print(\"m\", np.mean(t_m))\n",
        "print(\"c\", np.mean(t_c))\n",
        "\n",
        "t_zero_nd_auto_b1 = np.mean(t_b1)\n",
        "t_zero_nd_auto_b2 = np.mean(t_b2)\n",
        "t_zero_nd_auto_b3 = np.mean(t_b3)\n",
        "t_zero_nd_auto_b4 = np.mean(t_b4)\n",
        "t_zero_nd_auto_m = np.mean(t_m)\n",
        "t_zero_nd_auto_r = np.mean(t_r)\n",
        "t_zero_nd_auto_c = np.mean(t_c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 3.3 [Cross-encoder](https://www.bing.com/search?q=poly+encoder+paper&cvid=46293035bd454d6d9745d27396391cfc&aqs=edge..69i57j0l2j69i59j69i64j69i11004.7939j0j1&pglt=41&FORM=ANNAB1&PC=LCTS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 tell me about cass county missouri - list homes sale - do you want to know the list of sale homes\n",
            "100 Find information on ontario california airport. - directions location - do you want to get directions to this location\n",
            "200 Where can I buy pressure washers? - washer - do you want to know which washer to use\n",
            "300 Tell me more about Rocky Mountain News - recent events historical - are you looking for historical information about recent events\n",
            "400 Where should I order dog clean-up bags - specif bag type - are you looking for specifc type bag\n",
            "================================================================\n",
            "Full reference evaluation\n",
            "================================================================\n",
            "b1 0.41373244461884806 b2 0.176513247123985 b3 0.10313598447532396 b4 0.06910221676005299\n",
            "rouge-L 0.4417860678647769\n",
            "m 0.3977539356223055\n",
            "c 0.9165490196078431\n",
            "================================================================\n",
            "Question body evaluation\n",
            "================================================================\n",
            "b1 0.3625420522173297 b2 0.1688791182506196 b3 0.09992607000759766 b4 0.07243788969725326\n",
            "rouge-L 0.40447814113736086\n",
            "m 0.3724821276449544\n",
            "c 0.9087058823529411\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.append(\"/home/azureuser/cloudfiles/code/Users/t-zhendwang/srconvsearch\")\n",
        "sys.path.append(\"/home/azureuser/cloudfiles/code/Users/t-zhendwang/srconvsearch/conversationalQA/ParlAI\")\n",
        "from conversationalQA.ParlAI.parlai.scripts.interactive import Interactive, rerank\n",
        "\n",
        "facet_test_file = 'data/clariq_f/ClariQ-FKw-dev.tsv'\n",
        "facet_test_data = pd.read_csv(facet_test_file, sep='\\t')\n",
        "_, facet_test_data = process_clariq_f(facet_test_data)\n",
        "\n",
        "r = []\n",
        "b1, b2, b3, b4 = [], [], [], []\n",
        "m = []\n",
        "c = []\n",
        "\n",
        "t_r = []\n",
        "t_b1, t_b2, t_b3, t_b4 = [], [], [], []\n",
        "t_m = []\n",
        "t_c = []\n",
        "\n",
        "model_output = 'zeroshot_nd_cross.csv'\n",
        "generated_file = 'neurologic_decoding/zero_shot/gpt2facet'\n",
        "\n",
        "if os.path.isfile(model_output):\n",
        "    b1, b2, b3, b4, m, r, c, t_b1, t_b2, t_b3, t_b4, t_m, t_r, t_c = evaluate_from_output(model_output)\n",
        "    \n",
        "else:\n",
        "    question_reranker = Interactive.main(model = 'transformer/crossencoder', \\\n",
        "                            model_file = 'zoo:pretrained_transformers/cross_model_huge_reddit/model',  \\\n",
        "                            encode_candidate_vecs = False,  eval_candidates = 'inline', interactive_candidates = 'inline',\n",
        "                            return_cand_scores = True)\n",
        "\n",
        "    generated_cq_all_templates = open(generated_file, 'r').readlines()\n",
        "    generated_cq_grouped = [[generated_cq_all_templates[len(starting_texts) * k + l] \n",
        "                                for l in range(len(starting_texts))] \n",
        "                                for k in range(int(len(generated_cq_all_templates)/8))]\n",
        "\n",
        "    for iter, row in facet_test_data.iterrows():\n",
        "        query = facet_test_data.at[iter, 'initial_request']\n",
        "        facet = facet_test_data.at[iter, 'facet_desc']\n",
        "        ref = facet_test_data.at[iter, 'question']\n",
        "        \n",
        "        generated_follow_ups = [re.sub('\\[SEP\\]', '&', full_sentence).split('&')[1].strip() for full_sentence in generated_cq_grouped[iter]]\n",
        "        generated_cqs = [re.sub('[.?]', '&', generated_follow_up).split('&')[0].strip() for generated_follow_up in generated_follow_ups]\n",
        "\n",
        "        questions, questions_scores = rerank(question_reranker, query, '', generated_cqs)\n",
        "        generated_cq = questions[0]\n",
        "        facet_test_data.at[iter, 'generated'] = generated_cq\n",
        "\n",
        "        # full reference evaluation\n",
        "        hyp_b1, hyp_b2, hyp_b3, hyp_b4, hyp_m, hyp_r, hyp_c = auto_evaluation(ref, generated_cq, facet)\n",
        "\n",
        "        b1.append(hyp_b1)\n",
        "        b2.append(hyp_b2)\n",
        "        b3.append(hyp_b3)\n",
        "        b4.append(hyp_b4)\n",
        "        m.append(hyp_m)\n",
        "        r.append(hyp_r)\n",
        "        c.append(hyp_c)\n",
        "\n",
        "        # question body evaluation\n",
        "        truncate_ref = ' '.join(ref.split()[template_len:])\n",
        "        truncate_generated_cq = ' '.join(generated_cq.split()[template_len:])\n",
        "        \n",
        "        t_hyp_b1, t_hyp_b2, t_hyp_b3, t_hyp_b4, t_hyp_m, t_hyp_r, t_hyp_c = auto_evaluation(truncate_ref, truncate_generated_cq, facet)\n",
        "\n",
        "        t_b1.append(t_hyp_b1)\n",
        "        t_b2.append(t_hyp_b2)\n",
        "        t_b3.append(t_hyp_b3)\n",
        "        t_b4.append(t_hyp_b4)\n",
        "        t_m.append(t_hyp_m)\n",
        "        t_r.append(t_hyp_r)\n",
        "        t_c.append(t_hyp_c)\n",
        "\n",
        "    output_df = facet_test_data[['initial_request', 'facet_desc', 'question', 'generated']]\n",
        "    output_df.columns = ['query', 'facet', 'reference', 'candidate']\n",
        "    output_df.to_csv(model_output)\n",
        "    \n",
        "# full reference results\n",
        "print(\"================================================================\")\n",
        "print(\"Full reference evaluation\")\n",
        "print(\"================================================================\")\n",
        "print(\"b1\", np.mean(b1), \"b2\", np.mean(b2), \"b3\", np.mean(b3), \"b4\", np.mean(b4))\n",
        "print(\"rouge-L\", np.mean(r))\n",
        "print(\"m\", np.mean(m))\n",
        "print(\"c\", np.mean(c))\n",
        "\n",
        "zero_nd_cross_b1 = np.mean(b1)\n",
        "zero_nd_cross_b2 = np.mean(b2)\n",
        "zero_nd_cross_b3 = np.mean(b3)\n",
        "zero_nd_cross_b4 = np.mean(b4)\n",
        "zero_nd_cross_m = np.mean(m)\n",
        "zero_nd_cross_r = np.mean(r)\n",
        "zero_nd_cross_c = np.mean(c)\n",
        "\n",
        "# question body results\n",
        "print(\"================================================================\")\n",
        "print(\"Question body evaluation\")\n",
        "print(\"================================================================\")\n",
        "print(\"b1\", np.mean(t_b1), \"b2\", np.mean(t_b2), \"b3\", np.mean(t_b3), \"b4\", np.mean(t_b4))\n",
        "print(\"rouge-L\", np.mean(t_r))\n",
        "print(\"m\", np.mean(t_m))\n",
        "print(\"c\", np.mean(t_c))\n",
        "\n",
        "t_zero_nd_cross_b1 = np.mean(t_b1)\n",
        "t_zero_nd_cross_b2 = np.mean(t_b2)\n",
        "t_zero_nd_cross_b3 = np.mean(t_b3)\n",
        "t_zero_nd_cross_b4 = np.mean(t_b4)\n",
        "t_zero_nd_cross_m = np.mean(t_m)\n",
        "t_zero_nd_cross_r = np.mean(t_r)\n",
        "t_zero_nd_cross_c = np.mean(t_c)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 3.4 [NTES](https://arxiv.org/pdf/2010.14202.pdf) (Pretrained clarifying question ranker)\n",
        "In AML terminal, run\n",
        "\n",
        "```\n",
        "cd Clariq_System\n",
        "python rank.py\n",
        "```\n",
        "Make sure you get the generated question output \"zero\\_nd\\_ntes.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 tell me about cass county missouri - list homes sale - would you like to list the sale of homes\n",
            "\n",
            "100 Find information on ontario california airport. - directions location - are you interested in getting directions to this location\n",
            "\n",
            "200 Where can I buy pressure washers? - washer - are you interested in buying washer/dryer washers or do you have any questions\n",
            "\n",
            "300 Tell me more about Rocky Mountain News - recent events historical - do you want information about recent historical events\n",
            "\n",
            "400 Where should I order dog clean-up bags - specif bag type - are you looking for specifc type bag\n",
            "\n",
            "================================================================\n",
            "Full reference evaluation\n",
            "================================================================\n",
            "b1 0.36920461276522515 b2 0.16492584626033616 b3 0.09993981802772525 b4 0.0738805427686033\n",
            "rouge-L 0.41492680176106006\n",
            "m 0.3569165665608735\n",
            "c 0.7732156862745099\n",
            "================================================================\n",
            "Question body evaluation\n",
            "================================================================\n",
            "b1 0.29458733795768594 b2 0.1403471043416165 b3 0.09190463977437048 b4 0.07100761161471664\n",
            "rouge-L 0.33816654471658264\n",
            "m 0.2796704582312601\n",
            "c 0.7732156862745099\n"
          ]
        }
      ],
      "source": [
        "facet_test_file = 'data/clariq_f/ClariQ-FKw-dev.tsv'\n",
        "facet_test_data = pd.read_csv(facet_test_file, sep='\\t')\n",
        "_, facet_test_data = process_clariq_f(facet_test_data)\n",
        "\n",
        "r = []\n",
        "b1, b2, b3, b4 = [], [], [], []\n",
        "m = []\n",
        "c = []\n",
        "\n",
        "t_r = []\n",
        "t_b1, t_b2, t_b3, t_b4 = [], [], [], []\n",
        "t_m = []\n",
        "t_c = []\n",
        "\n",
        "model_output = 'zeroshot_nd_ntes.csv'\n",
        "generated_file = 'neurologic_decoding/zero_shot/gpt2facet'\n",
        "\n",
        "if os.path.isfile(model_output):\n",
        "    b1, b2, b3, b4, m, r, c, t_b1, t_b2, t_b3, t_b4, t_m, t_r, t_c = evaluate_from_output(model_output)\n",
        "else:\n",
        "    print(\"missing output file from NTES code, please run the ranker first\")\n",
        "\n",
        "\n",
        "# full reference results\n",
        "print(\"================================================================\")\n",
        "print(\"Full reference evaluation\")\n",
        "print(\"================================================================\")\n",
        "print(\"b1\", np.mean(b1), \"b2\", np.mean(b2), \"b3\", np.mean(b3), \"b4\", np.mean(b4))\n",
        "print(\"rouge-L\", np.mean(r))\n",
        "print(\"m\", np.mean(m))\n",
        "print(\"c\", np.mean(c))\n",
        "\n",
        "zero_nd_ntes_b1 = np.mean(b1)\n",
        "zero_nd_ntes_b2 = np.mean(b2)\n",
        "zero_nd_ntes_b3 = np.mean(b3)\n",
        "zero_nd_ntes_b4 = np.mean(b4)\n",
        "zero_nd_ntes_m = np.mean(m)\n",
        "zero_nd_ntes_r = np.mean(r)\n",
        "zero_nd_ntes_c = np.mean(c)\n",
        "\n",
        "# question body results\n",
        "print(\"================================================================\")\n",
        "print(\"Question body evaluation\")\n",
        "print(\"================================================================\")\n",
        "print(\"b1\", np.mean(t_b1), \"b2\", np.mean(t_b2), \"b3\", np.mean(t_b3), \"b4\", np.mean(t_b4))\n",
        "print(\"rouge-L\", np.mean(t_r))\n",
        "print(\"m\", np.mean(t_m))\n",
        "print(\"c\", np.mean(t_c))\n",
        "\n",
        "t_zero_nd_ntes_b1 = np.mean(t_b1)\n",
        "t_zero_nd_ntes_b2 = np.mean(t_b2)\n",
        "t_zero_nd_ntes_b3 = np.mean(t_b3)\n",
        "t_zero_nd_ntes_b4 = np.mean(t_b4)\n",
        "t_zero_nd_ntes_m = np.mean(t_m)\n",
        "t_zero_nd_ntes_r = np.mean(t_r)\n",
        "t_zero_nd_ntes_c = np.mean(t_c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 3.5 WSDM (Ours)\n",
        "\n",
        "Same as 1.2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 3.6 Oracle ranker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 tell me about cass county missouri - list homes sale - are you interested in a list of sale homes\n",
            "100 Find information on ontario california airport. - directions location - would you like to send directions to your location\n",
            "200 Where can I buy pressure washers? - washer - are you looking for a washer\n",
            "300 Tell me more about Rocky Mountain News - recent events historical - do you want information about recent historical events\n",
            "400 Where should I order dog clean-up bags - specif bag type - are you looking for specifc type bag\n",
            "================================================================\n",
            "Full reference evaluation\n",
            "================================================================\n",
            "b1 0.6160936744403871 b2 0.43807769108461336 b3 0.3346436810798287 b4 0.2414089762434254\n",
            "rouge-L 0.6897123207824376\n",
            "m 0.6517379490599675\n",
            "c 0.9269019607843136\n",
            "================================================================\n",
            "Question body evaluation\n",
            "================================================================\n",
            "b1 0.4074544213117534 b2 0.21095173470334003 b3 0.13615480970344712 b4 0.09920045448032966\n",
            "rouge-L 0.479522132674582\n",
            "m 0.4090943786312325\n",
            "c 0.9123921568627451\n"
          ]
        }
      ],
      "source": [
        "facet_test_file = 'data/clariq_f/ClariQ-FKw-dev.tsv'\n",
        "facet_test_data = pd.read_csv(facet_test_file, sep='\\t')\n",
        "_, facet_test_data = process_clariq_f(facet_test_data)\n",
        "\n",
        "r = []\n",
        "b1, b2, b3, b4 = [], [], [], []\n",
        "m = []\n",
        "c = []\n",
        "\n",
        "t_r = []\n",
        "t_b1, t_b2, t_b3, t_b4 = [], [], [], []\n",
        "t_m = []\n",
        "t_c = []\n",
        "\n",
        "model_output = 'zeroshot_nd_oracle.csv'\n",
        "generated_file = 'neurologic_decoding/zero_shot/gpt2facet'\n",
        "\n",
        "if os.path.isfile(model_output):\n",
        "    b1, b2, b3, b4, m, r, c, t_b1, t_b2, t_b3, t_b4, t_m, t_r, t_c = evaluate_from_output(model_output)\n",
        "\n",
        "else:\n",
        "    generated_cq_all_templates = open(generated_file, 'r').readlines()\n",
        "    generated_cq_grouped = [[generated_cq_all_templates[len(starting_texts) * k + l] \n",
        "                                for l in range(len(starting_texts))] \n",
        "                                for k in range(int(len(generated_cq_all_templates)/8))]\n",
        "\n",
        "    for iter, row in facet_test_data.iterrows():\n",
        "        facet = facet_test_data.at[iter, 'facet_desc']\n",
        "        ref = facet_test_data.at[iter, 'question']\n",
        "\n",
        "        template_scores = {}\n",
        "        for full_sentence in generated_cq_grouped[iter]:\n",
        "            query = re.sub('\\[SEP\\]', '&', full_sentence).split('&')[0].strip()\n",
        "            generated_cq = re.sub('[.?]', '&', generated_follow_up).split('&')[0].strip()  \n",
        "            template_scores[generated_cq] = rs.get_scores(generated_cq, ref)[0]['rouge-l']['f']\n",
        "        \n",
        "        generated_cq = sorted(template_scores.keys(), key = lambda x: template_scores[x], reverse=True)[0] \n",
        "        facet_test_data.at[iter, 'generated'] = generated_cq\n",
        "\n",
        "        # full reference evaluation\n",
        "        hyp_b1, hyp_b2, hyp_b3, hyp_b4, hyp_m, hyp_r, hyp_c = auto_evaluation(ref, generated_cq, facet)\n",
        "\n",
        "        b1.append(hyp_b1)\n",
        "        b2.append(hyp_b2)\n",
        "        b3.append(hyp_b3)\n",
        "        b4.append(hyp_b4)\n",
        "        m.append(hyp_m)\n",
        "        r.append(hyp_r)\n",
        "        c.append(hyp_c)\n",
        "\n",
        "        # question body evaluation\n",
        "        truncate_ref = ' '.join(ref.split()[template_len:])\n",
        "        truncate_generated_cq = ' '.join(generated_cq.split()[template_len:])\n",
        "        \n",
        "        t_hyp_b1, t_hyp_b2, t_hyp_b3, t_hyp_b4, t_hyp_m, t_hyp_r, t_hyp_c = auto_evaluation(truncate_ref, truncate_generated_cq, facet)\n",
        "\n",
        "        t_b1.append(t_hyp_b1)\n",
        "        t_b2.append(t_hyp_b2)\n",
        "        t_b3.append(t_hyp_b3)\n",
        "        t_b4.append(t_hyp_b4)\n",
        "        t_m.append(t_hyp_m)\n",
        "        t_r.append(t_hyp_r)\n",
        "        t_c.append(t_hyp_c)\n",
        "\n",
        "    output_df = facet_test_data[['initial_request', 'facet_desc', 'question', 'generated']]\n",
        "    output_df.columns = ['query', 'facet', 'reference', 'candidate']\n",
        "    output_df.to_csv(model_output)\n",
        "\n",
        "# full reference results\n",
        "print(\"================================================================\")\n",
        "print(\"Full reference evaluation\")\n",
        "print(\"================================================================\")\n",
        "print(\"b1\", np.mean(b1), \"b2\", np.mean(b2), \"b3\", np.mean(b3), \"b4\", np.mean(b4))\n",
        "print(\"rouge-L\", np.mean(r))\n",
        "print(\"m\", np.mean(m))\n",
        "print(\"c\", np.mean(c))\n",
        "\n",
        "zero_nd_oracle_b1 = np.mean(b1)\n",
        "zero_nd_oracle_b2 = np.mean(b2)\n",
        "zero_nd_oracle_b3 = np.mean(b3)\n",
        "zero_nd_oracle_b4 = np.mean(b4)\n",
        "zero_nd_oracle_m = np.mean(m)\n",
        "zero_nd_oracle_r = np.mean(r)\n",
        "zero_nd_oracle_c = np.mean(c)\n",
        "\n",
        "# question body results\n",
        "print(\"================================================================\")\n",
        "print(\"Question body evaluation\")\n",
        "print(\"================================================================\")\n",
        "print(\"b1\", np.mean(t_b1), \"b2\", np.mean(t_b2), \"b3\", np.mean(t_b3), \"b4\", np.mean(t_b4))\n",
        "print(\"rouge-L\", np.mean(t_r))\n",
        "print(\"m\", np.mean(t_m))\n",
        "print(\"c\", np.mean(t_c))\n",
        "\n",
        "t_zero_nd_oracle_b1 = np.mean(t_b1)\n",
        "t_zero_nd_oracle_b2 = np.mean(t_b2)\n",
        "t_zero_nd_oracle_b3 = np.mean(t_b3)\n",
        "t_zero_nd_oracle_b4 = np.mean(t_b4)\n",
        "t_zero_nd_oracle_m = np.mean(t_m)\n",
        "t_zero_nd_oracle_r = np.mean(t_r)\n",
        "t_zero_nd_oracle_c = np.mean(t_c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 3.7 Comparing 3.1-3.6, getting the conclusion that \"WSDM\" is the best ranker choice in terms of question body quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------------------------------\n",
            "|                               Full reference evaluation                               |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|MODEL                    BLEU1    BLEU2    BLEU3    BLEU4    METEOR   ROUGE    COVERAGE|\n",
            "-----------------------------------------------------------------------------------------\n",
            "|Perplexity               42.78    19.01    11.41    7.56     40.98    45.16    97.82   |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|AutoScore                43.42    20.47    12.93    9.64     41.03    47.87    98.28   |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|Cross encoder            41.37    17.65    10.31    6.91     39.78    44.18    91.65   |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|NTES                     36.92    16.49    9.99     7.39     35.69    41.49    77.32   |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|WSDM                     41.8     17.57    9.71     6.45     38.52    44.19    98.96   |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|Oracle                   61.61    43.81    33.46    24.14    65.17    68.97    92.69   |\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "|                                Question body evaluation                               |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|MODEL                    BLEU1    BLEU2    BLEU3    BLEU4    METEOR   ROUGE    COVERAGE|\n",
            "-----------------------------------------------------------------------------------------\n",
            "|Perplexity               36.8     16.61    9.13     6.5      37.77    40.58    97.47   |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|AutoScore                34.88    16.25    10.47    8.07     33.88    43.42    95.83   |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|Cross encoder            36.25    16.89    9.99     7.24     37.25    40.45    90.87   |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|NTES                     29.46    14.03    9.19     7.1      27.97    33.82    77.32   |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|WSDM                     38.51    18.17    10.81    8.46     37.47    43.79    98.67   |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|Oracle                   40.75    21.1     13.62    9.92     40.91    47.95    91.24   |\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|                               Full reference evaluation                               |\")\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('MODEL', 'BLEU1','BLEU2','BLEU3','BLEU4','METEOR','ROUGE','COVERAGE'))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('Perplexity', \n",
        "                                                            round_metric(zero_nd_pp_b1), \n",
        "                                                            round_metric(zero_nd_pp_b2), \n",
        "                                                            round_metric(zero_nd_pp_b3), \n",
        "                                                            round_metric(zero_nd_pp_b4), \n",
        "                                                            round_metric(zero_nd_pp_m), \n",
        "                                                            round_metric(zero_nd_pp_r), \n",
        "                                                            round_metric(zero_nd_pp_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('AutoScore', \n",
        "                                                            round_metric(zero_nd_auto_b1), \n",
        "                                                            round_metric(zero_nd_auto_b2), \n",
        "                                                            round_metric(zero_nd_auto_b3), \n",
        "                                                            round_metric(zero_nd_auto_b4), \n",
        "                                                            round_metric(zero_nd_auto_m), \n",
        "                                                            round_metric(zero_nd_auto_r), \n",
        "                                                            round_metric(zero_nd_auto_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('Cross encoder', \n",
        "                                                            round_metric(zero_nd_cross_b1), \n",
        "                                                            round_metric(zero_nd_cross_b2), \n",
        "                                                            round_metric(zero_nd_cross_b3), \n",
        "                                                            round_metric(zero_nd_cross_b4), \n",
        "                                                            round_metric(zero_nd_cross_m), \n",
        "                                                            round_metric(zero_nd_cross_r), \n",
        "                                                            round_metric(zero_nd_cross_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('NTES', \n",
        "                                                            round_metric(zero_nd_ntes_b1), \n",
        "                                                            round_metric(zero_nd_ntes_b2), \n",
        "                                                            round_metric(zero_nd_ntes_b3), \n",
        "                                                            round_metric(zero_nd_ntes_b4), \n",
        "                                                            round_metric(zero_nd_ntes_m), \n",
        "                                                            round_metric(zero_nd_ntes_r), \n",
        "                                                            round_metric(zero_nd_ntes_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('WSDM', \n",
        "                                                            round_metric(zero_nd_wsdm_b1), \n",
        "                                                            round_metric(zero_nd_wsdm_b2), \n",
        "                                                            round_metric(zero_nd_wsdm_b3), \n",
        "                                                            round_metric(zero_nd_wsdm_b4), \n",
        "                                                            round_metric(zero_nd_wsdm_m), \n",
        "                                                            round_metric(zero_nd_wsdm_r),\n",
        "                                                            round_metric(zero_nd_wsdm_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('Oracle', \n",
        "                                                            round_metric(zero_nd_oracle_b1), \n",
        "                                                            round_metric(zero_nd_oracle_b2), \n",
        "                                                            round_metric(zero_nd_oracle_b3), \n",
        "                                                            round_metric(zero_nd_oracle_b4), \n",
        "                                                            round_metric(zero_nd_oracle_m), \n",
        "                                                            round_metric(zero_nd_oracle_r), \n",
        "                                                            round_metric(zero_nd_oracle_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|                                Question body evaluation                               |\")\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('MODEL', 'BLEU1','BLEU2','BLEU3','BLEU4','METEOR','ROUGE','COVERAGE'))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('Perplexity', \n",
        "                                                            round_metric(t_zero_nd_pp_b1), \n",
        "                                                            round_metric(t_zero_nd_pp_b2), \n",
        "                                                            round_metric(t_zero_nd_pp_b3), \n",
        "                                                            round_metric(t_zero_nd_pp_b4), \n",
        "                                                            round_metric(t_zero_nd_pp_m), \n",
        "                                                            round_metric(t_zero_nd_pp_r), \n",
        "                                                            round_metric(t_zero_nd_pp_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('AutoScore', \n",
        "                                                            round_metric(t_zero_nd_auto_b1), \n",
        "                                                            round_metric(t_zero_nd_auto_b2), \n",
        "                                                            round_metric(t_zero_nd_auto_b3), \n",
        "                                                            round_metric(t_zero_nd_auto_b4), \n",
        "                                                            round_metric(t_zero_nd_auto_m), \n",
        "                                                            round_metric(t_zero_nd_auto_r), \n",
        "                                                            round_metric(t_zero_nd_auto_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('Cross encoder', \n",
        "                                                            round_metric(t_zero_nd_cross_b1), \n",
        "                                                            round_metric(t_zero_nd_cross_b2), \n",
        "                                                            round_metric(t_zero_nd_cross_b3), \n",
        "                                                            round_metric(t_zero_nd_cross_b4), \n",
        "                                                            round_metric(t_zero_nd_cross_m), \n",
        "                                                            round_metric(t_zero_nd_cross_r), \n",
        "                                                            round_metric(t_zero_nd_cross_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('NTES', \n",
        "                                                            round_metric(t_zero_nd_ntes_b1), \n",
        "                                                            round_metric(t_zero_nd_ntes_b2), \n",
        "                                                            round_metric(t_zero_nd_ntes_b3), \n",
        "                                                            round_metric(t_zero_nd_ntes_b4), \n",
        "                                                            round_metric(t_zero_nd_ntes_m), \n",
        "                                                            round_metric(t_zero_nd_ntes_r), \n",
        "                                                            round_metric(t_zero_nd_ntes_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('WSDM', \n",
        "                                                            round_metric(t_zero_nd_wsdm_b1), \n",
        "                                                            round_metric(t_zero_nd_wsdm_b2), \n",
        "                                                            round_metric(t_zero_nd_wsdm_b3), \n",
        "                                                            round_metric(t_zero_nd_wsdm_b4), \n",
        "                                                            round_metric(t_zero_nd_wsdm_m), \n",
        "                                                            round_metric(t_zero_nd_wsdm_r),\n",
        "                                                            round_metric(t_zero_nd_wsdm_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('Oracle', \n",
        "                                                            round_metric(t_zero_nd_oracle_b1), \n",
        "                                                            round_metric(t_zero_nd_oracle_b2), \n",
        "                                                            round_metric(t_zero_nd_oracle_b3), \n",
        "                                                            round_metric(t_zero_nd_oracle_b4), \n",
        "                                                            round_metric(t_zero_nd_oracle_m), \n",
        "                                                            round_metric(t_zero_nd_oracle_r), \n",
        "                                                            round_metric(t_zero_nd_oracle_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# RQ4. How good is Neurologic Decoding for facet-driven clarifying? Specifically, how much does it improve over other facet-driven methods and how far is it from perfect?\n",
        "\n",
        "Can be merged with RQ2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# RQ5. How does our GPT-2-based zero-shot facet-constrained approach compare to using Large Language Models such as zero-shot GPT-3? \n",
        "To answer this question, we compare our proposed method with a few-shot prompt guided GPT-3 method, but we mainly compare zero-shot with zero-shot.\n",
        "The few-shot GPT-3 method uses the prompt structure as in our proposed GPT-2 finetune method, which is:\n",
        "\n",
        "## {query} Ask a question that contains words in the list \\[{facet}\\] {clarifying question}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 5.1 Zero-shot GPT3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 tell me about cass county missouri - list homes sale - are you looking for a list of homes for sale in cass county missouri\n",
            "100 Find information on ontario california airport. - directions location - do you need information on the ontario california airport\n",
            "200 Where can I buy pressure washers? - washer - do you want information about a washing machine or a pressure washer\n",
            "300 Tell me more about Rocky Mountain News - recent events historical - do you need information on recent events or historical events\n",
            "400 Where should I order dog clean-up bags - specif bag type - do you want to know what type of bag to use for your dog\n",
            "================================================================\n",
            "Full reference evaluation\n",
            "================================================================\n",
            "b1 0.42959420323127595 b2 0.2144905020365997 b3 0.12975964645585925 b4 0.08756123899640086\n",
            "rouge-L 0.4668552513632458\n",
            "m 0.4595009686068181\n",
            "c 0.8526666666666668\n",
            "================================================================\n",
            "Question body evaluation\n",
            "================================================================\n",
            "b1 0.3943690841916204 b2 0.22815387204349083 b3 0.15282352826624315 b4 0.1193501579145087\n",
            "rouge-L 0.4620705224770397\n",
            "m 0.4636303615409833\n",
            "c 0.829529411764706\n"
          ]
        }
      ],
      "source": [
        "facet_test_file = 'data/clariq_f/ClariQ-FKw-dev.tsv'\n",
        "facet_test_data = pd.read_csv(facet_test_file, sep='\\t')\n",
        "_, facet_test_data = process_clariq_f(facet_test_data)\n",
        "\n",
        "b1, b2, b3, b4 = [], [], [], []\n",
        "m = []\n",
        "r = []\n",
        "c = []\n",
        "\n",
        "t_b1, t_b2, t_b3, t_b4 = [], [], [], []\n",
        "t_m = []\n",
        "t_r = []\n",
        "t_c = []\n",
        "\n",
        "temperature = 0\n",
        "use_examples = 0\n",
        "model_output = 'fewshot_gpt3' + '_examples' + str(use_examples) + '_temp' + str(temperature) + '.csv'\n",
        "model_output_all_templates = 'fewshot_gpt3' + '_examples' + str(use_examples) + '_temp' + str(temperature) +'_all'\n",
        "\n",
        "all_generations = [] # cache the generations to save time and load from calling gpt3\n",
        "\n",
        "if os.path.isfile(model_output):\n",
        "    b1, b2, b3, b4, m, r, c, t_b1, t_b2, t_b3, t_b4, t_m, t_r, t_c = evaluate_from_output(model_output)\n",
        "    \n",
        "else:\n",
        "    if os.path.isfile(model_output_all_templates):\n",
        "        generated_cq_all_templates = open(model_output_all_templates, 'r').readlines()\n",
        "        generated_cq_grouped = [[generated_cq_all_templates[len(starting_texts) * k + l] \n",
        "                                    for l in range(len(starting_texts))] \n",
        "                                    for k in range(int(len(generated_cq_all_templates)/8))]\n",
        "\n",
        "        for iter, row in facet_test_data.iterrows():\n",
        "            facet = facet_test_data.at[iter, 'facet_desc']\n",
        "            ref = facet_test_data.at[iter, 'question']\n",
        "            facet_list = facet.split()\n",
        "\n",
        "            generated_cqs = []\n",
        "            for full_sentence in generated_cq_grouped[iter]:\n",
        "                query = re.sub('\\[SEP\\]', '&', full_sentence).split('&')[0].strip()\n",
        "                generated_follow_up = re.sub('\\[SEP\\]', '&', full_sentence).split('&')[1].strip()\n",
        "                generated_cq = re.sub('[.?]', '&', generated_follow_up).split('&')[0].strip()\n",
        "                generated_cqs.append(generated_cq)\n",
        "            \n",
        "            noun_in_query = [token.text for token in pos_tagger(query) if token.pos_ == 'NOUN']\n",
        "            propn_in_query = [token.text.lower() for token in pos_tagger(query) if token.pos_ == 'PROPN']\n",
        "\n",
        "            template_scores = calculate_WSDM(query=' '.join(noun_in_query+propn_in_query+facet_list), doc_list=generated_cqs)\n",
        "            sorted_template_scores = sorted(template_scores.items(), key = lambda x: x[1], reverse=True)\n",
        "            generated_cq = sorted(template_scores.keys(), key = lambda x: template_scores[x], reverse=True)[0] \n",
        "            facet_test_data.at[iter, 'generated'] = generated_cq\n",
        "            \n",
        "            if iter % sample_every == 0: \n",
        "                print(iter, query, \"-\", facet, '-', generated_cq)\n",
        "                pprint.pprint(sorted_template_scores)\n",
        "\n",
        "            # full reference evaluation\n",
        "            hyp_b1, hyp_b2, hyp_b3, hyp_b4, hyp_m, hyp_r, hyp_c = auto_evaluation(ref, generated_cq, facet)\n",
        "\n",
        "            b1.append(hyp_b1)\n",
        "            b2.append(hyp_b2)\n",
        "            b3.append(hyp_b3)\n",
        "            b4.append(hyp_b4)\n",
        "            m.append(hyp_m)\n",
        "            r.append(hyp_r)\n",
        "            c.append(hyp_c)\n",
        "\n",
        "            # question body evaluation\n",
        "            truncate_ref = ' '.join(ref.split()[template_len:])\n",
        "            truncate_generated_cq = ' '.join(generated_cq.split()[template_len:])\n",
        "            \n",
        "            t_hyp_b1, t_hyp_b2, t_hyp_b3, t_hyp_b4, t_hyp_m, t_hyp_r, t_hyp_c = auto_evaluation(truncate_ref, truncate_generated_cq, facet)\n",
        "\n",
        "            t_b1.append(t_hyp_b1)\n",
        "            t_b2.append(t_hyp_b2)\n",
        "            t_b3.append(t_hyp_b3)\n",
        "            t_b4.append(t_hyp_b4)\n",
        "            t_m.append(t_hyp_m)\n",
        "            t_r.append(t_hyp_r)\n",
        "            t_c.append(t_hyp_c)\n",
        "\n",
        "    else:\n",
        "        for iter, row in facet_test_data.iterrows():\n",
        "            query = facet_test_data.at[iter, 'initial_request']\n",
        "            facet = facet_test_data.at[iter, 'facet_desc']\n",
        "            ref = facet_test_data.at[iter, 'question']\n",
        "            facet_list = facet.split()\n",
        "\n",
        "            generated_cqs = []\n",
        "            for s_t in starting_texts:\n",
        "                s_t = re.sub('\\[SEP\\]', ' ', s_t).strip()\n",
        "                prompt = ' '.join(gpt3_examples[:use_examples]) + ' ' + query + ' ' + \"Ask a question that contains words in the list\" + ' ' + \"[\" + \", \".join([\"'\"+f+\"'\" for f in facet.split()])  + '].' + s_t\n",
        "                response = openai.Completion.create(\n",
        "                    model=\"text-davinci-002\",\n",
        "                    prompt= prompt,\n",
        "                    temperature=temperature,\n",
        "                    max_tokens=32,\n",
        "                    top_p=1,\n",
        "                    frequency_penalty=0.0,\n",
        "                    presence_penalty=0.0,\n",
        "                    stop=[\"\\n\"]\n",
        "                )\n",
        "\n",
        "                generated_cq = s_t + response['choices'][0]['text']\n",
        "                generated_cq = re.sub('\\[SEP\\]', ' ', generated_cq).strip()\n",
        "                generated_cq = re.sub('[.?]', '&', generated_cq).split('&')[0].strip()\n",
        "                generated_cqs.append(generated_cq)\n",
        "                all_generations.append(generated_cq)\n",
        "\n",
        "            noun_in_query = [token.text for token in pos_tagger(query) if token.pos_ == 'NOUN']\n",
        "            propn_in_query = [token.text.lower() for token in pos_tagger(query) if token.pos_ == 'PROPN']\n",
        "\n",
        "            template_scores = calculate_WSDM(query=' '.join(noun_in_query+propn_in_query+facet_list), doc_list=generated_cqs)\n",
        "            sorted_template_scores = sorted(template_scores.items(), key = lambda x: x[1], reverse=True)\n",
        "            generated_cq = sorted(template_scores.keys(), key = lambda x: template_scores[x], reverse=True)[0] \n",
        "            facet_test_data.at[iter, 'generated'] = generated_cq\n",
        "\n",
        "            if iter % sample_every == 0: \n",
        "                print(iter, query, \"-\", facet_list, '-', ' '.join(tokenized_hyp))\n",
        "                pprint.pprint(sorted_template_scores)\n",
        "        \n",
        "            # full reference evaluation\n",
        "            hyp_b1, hyp_b2, hyp_b3, hyp_b4, hyp_m, hyp_r, hyp_c = auto_evaluation(ref, generated_cq, facet)\n",
        "\n",
        "            b1.append(hyp_b1)\n",
        "            b2.append(hyp_b2)\n",
        "            b3.append(hyp_b3)\n",
        "            b4.append(hyp_b4)\n",
        "            m.append(hyp_m)\n",
        "            r.append(hyp_r)\n",
        "            c.append(hyp_c)\n",
        "\n",
        "            # question body evaluation\n",
        "            truncate_ref = ' '.join(ref.split()[template_len:])\n",
        "            truncate_generated_cq = ' '.join(generated_cq.split()[template_len:])\n",
        "            \n",
        "            t_hyp_b1, t_hyp_b2, t_hyp_b3, t_hyp_b4, t_hyp_m, t_hyp_r, t_hyp_c = auto_evaluation(truncate_ref, truncate_generated_cq, facet)\n",
        "\n",
        "            t_b1.append(t_hyp_b1)\n",
        "            t_b2.append(t_hyp_b2)\n",
        "            t_b3.append(t_hyp_b3)\n",
        "            t_b4.append(t_hyp_b4)\n",
        "            t_m.append(t_hyp_m)\n",
        "            t_r.append(t_hyp_r)\n",
        "            t_c.append(t_hyp_c)\n",
        "\n",
        "    output_df = facet_test_data[['initial_request', 'facet_desc', 'question', 'generated']]\n",
        "    output_df.columns = ['query', 'facet', 'reference', 'candidate']\n",
        "    output_df.to_csv(model_output)\n",
        "\n",
        "with open(model_output_all_templates, 'w') as outputfile:\n",
        "    for generation in all_generations:\n",
        "        outputfile.write(generation)\n",
        "        outputfile.write('\\n')\n",
        "\n",
        "# full reference results\n",
        "print(\"================================================================\")\n",
        "print(\"Full reference evaluation\")\n",
        "print(\"================================================================\")\n",
        "print(\"b1\", np.mean(b1), \"b2\", np.mean(b2), \"b3\", np.mean(b3), \"b4\", np.mean(b4))\n",
        "print(\"rouge-L\", np.mean(r))\n",
        "print(\"m\", np.mean(m))\n",
        "print(\"c\", np.mean(c))\n",
        "\n",
        "gpt3_0_b1 = np.mean(b1)\n",
        "gpt3_0_b2 = np.mean(b2)\n",
        "gpt3_0_b3 = np.mean(b3)\n",
        "gpt3_0_b4 = np.mean(b4)\n",
        "gpt3_0_m = np.mean(m)\n",
        "gpt3_0_r = np.mean(r)\n",
        "gpt3_0_c = np.mean(c)\n",
        "\n",
        "# question body results\n",
        "print(\"================================================================\")\n",
        "print(\"Question body evaluation\")\n",
        "print(\"================================================================\")\n",
        "print(\"b1\", np.mean(t_b1), \"b2\", np.mean(t_b2), \"b3\", np.mean(t_b3), \"b4\", np.mean(t_b4))\n",
        "print(\"rouge-L\", np.mean(t_r))\n",
        "print(\"m\", np.mean(t_m))\n",
        "print(\"c\", np.mean(t_c))\n",
        "\n",
        "t_gpt3_0_b1 = np.mean(t_b1)\n",
        "t_gpt3_0_b2 = np.mean(t_b2)\n",
        "t_gpt3_0_b3 = np.mean(t_b3)\n",
        "t_gpt3_0_b4 = np.mean(t_b4)\n",
        "t_gpt3_0_m = np.mean(t_m)\n",
        "t_gpt3_0_r = np.mean(t_r)\n",
        "t_gpt3_0_c = np.mean(t_c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 5.2 One-shot GPT3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 tell me about cass county missouri - list homes sale - do you want information about a specific home or a list of homes for sale\n",
            "100 Find information on ontario california airport. - directions location - do you want information on the ontario california airport location or directions to the ontario california airport\n",
            "200 Where can I buy pressure washers? - washer - do you need to buy a pressure washer\n",
            "300 Tell me more about Rocky Mountain News - recent events historical - are you looking for recent events or historical events\n",
            "400 Where should I order dog clean-up bags - specif bag type - do you want to know about a specific bag or type of bag\n",
            "================================================================\n",
            "Full reference evaluation\n",
            "================================================================\n",
            "b1 0.45450442193977114 b2 0.23043683433896323 b3 0.1453323074712589 b4 0.10288596669634774\n",
            "rouge-L 0.48206610468118216\n",
            "m 0.46551388301551\n",
            "c 0.9543529411764706\n",
            "================================================================\n",
            "Question body evaluation\n",
            "================================================================\n",
            "b1 0.4253424715913936 b2 0.23735638076510057 b3 0.15818386619872948 b4 0.11904117520780316\n",
            "rouge-L 0.47590761996212216\n",
            "m 0.45983232302338706\n",
            "c 0.9382745098039216\n"
          ]
        }
      ],
      "source": [
        "facet_test_file = 'data/clariq_f/ClariQ-FKw-dev.tsv'\n",
        "facet_test_data = pd.read_csv(facet_test_file, sep='\\t')\n",
        "_, facet_test_data = process_clariq_f(facet_test_data)\n",
        "\n",
        "b1, b2, b3, b4 = [], [], [], []\n",
        "m = []\n",
        "r = []\n",
        "c = []\n",
        "\n",
        "t_b1, t_b2, t_b3, t_b4 = [], [], [], []\n",
        "t_m = []\n",
        "t_r = []\n",
        "t_c = []\n",
        "\n",
        "temperature = 0\n",
        "use_examples = 1\n",
        "model_output = 'fewshot_gpt3' + '_examples' + str(use_examples) + '_temp' + str(temperature) + '.csv'\n",
        "model_output_all_templates = 'fewshot_gpt3' + '_examples' + str(use_examples) + '_temp' + str(temperature) +'_all'\n",
        "\n",
        "all_generations = [] # cache the generations to save time and load from calling gpt3\n",
        "\n",
        "if os.path.isfile(model_output):\n",
        "    b1, b2, b3, b4, m, r, c, t_b1, t_b2, t_b3, t_b4, t_m, t_r, t_c = evaluate_from_output(model_output)\n",
        "    \n",
        "else:\n",
        "    if os.path.isfile(model_output_all_templates):\n",
        "        generated_cq_all_templates = open(model_output_all_templates, 'r').readlines()\n",
        "        generated_cq_grouped = [[generated_cq_all_templates[len(starting_texts) * k + l] \n",
        "                                    for l in range(len(starting_texts))] \n",
        "                                    for k in range(int(len(generated_cq_all_templates)/8))]\n",
        "\n",
        "        for iter, row in facet_test_data.iterrows():\n",
        "            facet = facet_test_data.at[iter, 'facet_desc']\n",
        "            ref = facet_test_data.at[iter, 'question']\n",
        "            facet_list = facet.split()\n",
        "\n",
        "            generated_cqs = []\n",
        "            for full_sentence in generated_cq_grouped[iter]:\n",
        "                query = re.sub('\\[SEP\\]', '&', full_sentence).split('&')[0].strip()\n",
        "                generated_follow_up = re.sub('\\[SEP\\]', '&', full_sentence).split('&')[1].strip()\n",
        "                generated_cq = re.sub('[.?]', '&', generated_follow_up).split('&')[0].strip()\n",
        "                generated_cqs.append(generated_cq)\n",
        "            \n",
        "            noun_in_query = [token.text for token in pos_tagger(query) if token.pos_ == 'NOUN']\n",
        "            propn_in_query = [token.text.lower() for token in pos_tagger(query) if token.pos_ == 'PROPN']\n",
        "\n",
        "            template_scores = calculate_WSDM(query=' '.join(noun_in_query+propn_in_query+facet_list), doc_list=generated_cqs)\n",
        "            sorted_template_scores = sorted(template_scores.items(), key = lambda x: x[1], reverse=True)\n",
        "            generated_cq = sorted(template_scores.keys(), key = lambda x: template_scores[x], reverse=True)[0] \n",
        "            facet_test_data.at[iter, 'generated'] = generated_cq\n",
        "            \n",
        "            if iter % sample_every == 0: \n",
        "                print(iter, query, \"-\", facet, '-', generated_cq)\n",
        "                pprint.pprint(sorted_template_scores)\n",
        "\n",
        "            # full reference evaluation\n",
        "            hyp_b1, hyp_b2, hyp_b3, hyp_b4, hyp_m, hyp_r, hyp_c = auto_evaluation(ref, generated_cq, facet)\n",
        "\n",
        "            b1.append(hyp_b1)\n",
        "            b2.append(hyp_b2)\n",
        "            b3.append(hyp_b3)\n",
        "            b4.append(hyp_b4)\n",
        "            m.append(hyp_m)\n",
        "            r.append(hyp_r)\n",
        "            c.append(hyp_c)\n",
        "\n",
        "            # question body evaluation\n",
        "            truncate_ref = ' '.join(ref.split()[template_len:])\n",
        "            truncate_generated_cq = ' '.join(generated_cq.split()[template_len:])\n",
        "            \n",
        "            t_hyp_b1, t_hyp_b2, t_hyp_b3, t_hyp_b4, t_hyp_m, t_hyp_r, t_hyp_c = auto_evaluation(truncate_ref, truncate_generated_cq, facet)\n",
        "\n",
        "            t_b1.append(t_hyp_b1)\n",
        "            t_b2.append(t_hyp_b2)\n",
        "            t_b3.append(t_hyp_b3)\n",
        "            t_b4.append(t_hyp_b4)\n",
        "            t_m.append(t_hyp_m)\n",
        "            t_r.append(t_hyp_r)\n",
        "            t_c.append(t_hyp_c)\n",
        "\n",
        "    else:\n",
        "        for iter, row in facet_test_data.iterrows():\n",
        "            query = facet_test_data.at[iter, 'initial_request']\n",
        "            facet = facet_test_data.at[iter, 'facet_desc']\n",
        "            ref = facet_test_data.at[iter, 'question']\n",
        "            facet_list = facet.split()\n",
        "\n",
        "            generated_cqs = []\n",
        "            for s_t in starting_texts:\n",
        "                s_t = re.sub('\\[SEP\\]', ' ', s_t).strip()\n",
        "                prompt = ' '.join(gpt3_examples[:use_examples]) + ' ' + query + ' ' + \"Ask a question that contains words in the list\" + ' ' + \"[\" + \", \".join([\"'\"+f+\"'\" for f in facet.split()])  + '].' + s_t\n",
        "                response = openai.Completion.create(\n",
        "                    model=\"text-davinci-002\",\n",
        "                    prompt= prompt,\n",
        "                    temperature=temperature,\n",
        "                    max_tokens=32,\n",
        "                    top_p=1,\n",
        "                    frequency_penalty=0.0,\n",
        "                    presence_penalty=0.0,\n",
        "                    stop=[\"\\n\"]\n",
        "                )\n",
        "\n",
        "                generated_cq = s_t + response['choices'][0]['text']\n",
        "                generated_cq = re.sub('\\[SEP\\]', ' ', generated_cq).strip()\n",
        "                generated_cq = re.sub('[.?]', '&', generated_cq).split('&')[0].strip()\n",
        "                generated_cqs.append(generated_cq)\n",
        "                all_generations.append(generated_cq)\n",
        "\n",
        "            noun_in_query = [token.text for token in pos_tagger(query) if token.pos_ == 'NOUN']\n",
        "            propn_in_query = [token.text.lower() for token in pos_tagger(query) if token.pos_ == 'PROPN']\n",
        "\n",
        "            template_scores = calculate_WSDM(query=' '.join(noun_in_query+propn_in_query+facet_list), doc_list=generated_cqs)\n",
        "            sorted_template_scores = sorted(template_scores.items(), key = lambda x: x[1], reverse=True)\n",
        "            generated_cq = sorted(template_scores.keys(), key = lambda x: template_scores[x], reverse=True)[0] \n",
        "            facet_test_data.at[iter, 'generated'] = generated_cq\n",
        "\n",
        "            if iter % sample_every == 0: \n",
        "                print(iter, query, \"-\", facet_list, '-', ' '.join(tokenized_hyp))\n",
        "                pprint.pprint(sorted_template_scores)\n",
        "        \n",
        "            # full reference evaluation\n",
        "            hyp_b1, hyp_b2, hyp_b3, hyp_b4, hyp_m, hyp_r, hyp_c = auto_evaluation(ref, generated_cq, facet)\n",
        "\n",
        "            b1.append(hyp_b1)\n",
        "            b2.append(hyp_b2)\n",
        "            b3.append(hyp_b3)\n",
        "            b4.append(hyp_b4)\n",
        "            m.append(hyp_m)\n",
        "            r.append(hyp_r)\n",
        "            c.append(hyp_c)\n",
        "\n",
        "            # question body evaluation\n",
        "            truncate_ref = ' '.join(ref.split()[template_len:])\n",
        "            truncate_generated_cq = ' '.join(generated_cq.split()[template_len:])\n",
        "            \n",
        "            t_hyp_b1, t_hyp_b2, t_hyp_b3, t_hyp_b4, t_hyp_m, t_hyp_r, t_hyp_c = auto_evaluation(truncate_ref, truncate_generated_cq, facet)\n",
        "\n",
        "            t_b1.append(t_hyp_b1)\n",
        "            t_b2.append(t_hyp_b2)\n",
        "            t_b3.append(t_hyp_b3)\n",
        "            t_b4.append(t_hyp_b4)\n",
        "            t_m.append(t_hyp_m)\n",
        "            t_r.append(t_hyp_r)\n",
        "            t_c.append(t_hyp_c)\n",
        "\n",
        "    output_df = facet_test_data[['initial_request', 'facet_desc', 'question', 'generated']]\n",
        "    output_df.columns = ['query', 'facet', 'reference', 'candidate']\n",
        "    output_df.to_csv(model_output)\n",
        "\n",
        "with open(model_output_all_templates, 'w') as outputfile:\n",
        "    for generation in all_generations:\n",
        "        outputfile.write(generation)\n",
        "        outputfile.write('\\n')\n",
        "\n",
        "# full reference results\n",
        "print(\"================================================================\")\n",
        "print(\"Full reference evaluation\")\n",
        "print(\"================================================================\")\n",
        "print(\"b1\", np.mean(b1), \"b2\", np.mean(b2), \"b3\", np.mean(b3), \"b4\", np.mean(b4))\n",
        "print(\"rouge-L\", np.mean(r))\n",
        "print(\"m\", np.mean(m))\n",
        "print(\"c\", np.mean(c))\n",
        "\n",
        "gpt3_1_b1 = np.mean(b1)\n",
        "gpt3_1_b2 = np.mean(b2)\n",
        "gpt3_1_b3 = np.mean(b3)\n",
        "gpt3_1_b4 = np.mean(b4)\n",
        "gpt3_1_m = np.mean(m)\n",
        "gpt3_1_r = np.mean(r)\n",
        "gpt3_1_c = np.mean(c)\n",
        "\n",
        "# question body results\n",
        "print(\"================================================================\")\n",
        "print(\"Question body evaluation\")\n",
        "print(\"================================================================\")\n",
        "print(\"b1\", np.mean(t_b1), \"b2\", np.mean(t_b2), \"b3\", np.mean(t_b3), \"b4\", np.mean(t_b4))\n",
        "print(\"rouge-L\", np.mean(t_r))\n",
        "print(\"m\", np.mean(t_m))\n",
        "print(\"c\", np.mean(t_c))\n",
        "\n",
        "t_gpt3_1_b1 = np.mean(t_b1)\n",
        "t_gpt3_1_b2 = np.mean(t_b2)\n",
        "t_gpt3_1_b3 = np.mean(t_b3)\n",
        "t_gpt3_1_b4 = np.mean(t_b4)\n",
        "t_gpt3_1_m = np.mean(t_m)\n",
        "t_gpt3_1_r = np.mean(t_r)\n",
        "t_gpt3_1_c = np.mean(t_c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 5.3 Two-shot GPT3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 tell me about cass county missouri - list homes sale - are you looking for a list of homes for sale in cass county missouri\n",
            "100 Find information on ontario california airport. - directions location - do you want information on how to get to the ontario california airport or information about its location\n",
            "200 Where can I buy pressure washers? - washer - are you interested in purchasing a pressure washer\n",
            "300 Tell me more about Rocky Mountain News - recent events historical - are you looking for recent events or historical events\n",
            "400 Where should I order dog clean-up bags - specif bag type - are you interested in a specific type of bag or just any bag that will work for dog clean-up\n",
            "================================================================\n",
            "Full reference evaluation\n",
            "================================================================\n",
            "b1 0.42323949261930216 b2 0.21974406668664828 b3 0.13476144197480297 b4 0.09177524838662333\n",
            "rouge-L 0.4577721705677639\n",
            "m 0.5238679886110711\n",
            "c 0.9356470588235294\n",
            "================================================================\n",
            "Question body evaluation\n",
            "================================================================\n",
            "b1 0.3894379076796257 b2 0.22635225534200076 b3 0.14553205532907226 b4 0.10370774309698337\n",
            "rouge-L 0.4362571834961392\n",
            "m 0.550147025333538\n",
            "c 0.9191764705882353\n"
          ]
        }
      ],
      "source": [
        "facet_test_file = 'data/clariq_f/ClariQ-FKw-dev.tsv'\n",
        "facet_test_data = pd.read_csv(facet_test_file, sep='\\t')\n",
        "_, facet_test_data = process_clariq_f(facet_test_data)\n",
        "\n",
        "b1, b2, b3, b4 = [], [], [], []\n",
        "m = []\n",
        "r = []\n",
        "c = []\n",
        "\n",
        "t_b1, t_b2, t_b3, t_b4 = [], [], [], []\n",
        "t_m = []\n",
        "t_r = []\n",
        "t_c = []\n",
        "\n",
        "temperature = 0\n",
        "use_examples = 2\n",
        "model_output = 'fewshot_gpt3' + '_examples' + str(use_examples) + '_temp' + str(temperature) + '.csv'\n",
        "model_output_all_templates = 'fewshot_gpt3' + '_examples' + str(use_examples) + '_temp' + str(temperature) +'_all'\n",
        "\n",
        "all_generations = [] # cache the generations to save time and load from calling gpt3\n",
        "\n",
        "if os.path.isfile(model_output):\n",
        "    b1, b2, b3, b4, m, r, c, t_b1, t_b2, t_b3, t_b4, t_m, t_r, t_c = evaluate_from_output(model_output)\n",
        "    \n",
        "else:\n",
        "    if os.path.isfile(model_output_all_templates):\n",
        "        generated_cq_all_templates = open(model_output_all_templates, 'r').readlines()\n",
        "        generated_cq_grouped = [[generated_cq_all_templates[len(starting_texts) * k + l] \n",
        "                                    for l in range(len(starting_texts))] \n",
        "                                    for k in range(int(len(generated_cq_all_templates)/8))]\n",
        "\n",
        "        for iter, row in facet_test_data.iterrows():\n",
        "            facet = facet_test_data.at[iter, 'facet_desc']\n",
        "            ref = facet_test_data.at[iter, 'question']\n",
        "            facet_list = facet.split()\n",
        "\n",
        "            generated_cqs = []\n",
        "            for full_sentence in generated_cq_grouped[iter]:\n",
        "                query = re.sub('\\[SEP\\]', '&', full_sentence).split('&')[0].strip()\n",
        "                generated_follow_up = re.sub('\\[SEP\\]', '&', full_sentence).split('&')[1].strip()\n",
        "                generated_cq = re.sub('[.?]', '&', generated_follow_up).split('&')[0].strip()\n",
        "                generated_cqs.append(generated_cq)\n",
        "            \n",
        "            noun_in_query = [token.text for token in pos_tagger(query) if token.pos_ == 'NOUN']\n",
        "            propn_in_query = [token.text.lower() for token in pos_tagger(query) if token.pos_ == 'PROPN']\n",
        "\n",
        "            template_scores = calculate_WSDM(query=' '.join(noun_in_query+propn_in_query+facet_list), doc_list=generated_cqs)\n",
        "            sorted_template_scores = sorted(template_scores.items(), key = lambda x: x[1], reverse=True)\n",
        "            generated_cq = sorted(template_scores.keys(), key = lambda x: template_scores[x], reverse=True)[0] \n",
        "            facet_test_data.at[iter, 'generated'] = generated_cq\n",
        "            \n",
        "            if iter % sample_every == 0: \n",
        "                print(iter, query, \"-\", facet, '-', generated_cq)\n",
        "                pprint.pprint(sorted_template_scores)\n",
        "\n",
        "            # full reference evaluation\n",
        "            hyp_b1, hyp_b2, hyp_b3, hyp_b4, hyp_m, hyp_r, hyp_c = auto_evaluation(ref, generated_cq, facet)\n",
        "\n",
        "            b1.append(hyp_b1)\n",
        "            b2.append(hyp_b2)\n",
        "            b3.append(hyp_b3)\n",
        "            b4.append(hyp_b4)\n",
        "            m.append(hyp_m)\n",
        "            r.append(hyp_r)\n",
        "            c.append(hyp_c)\n",
        "\n",
        "            # question body evaluation\n",
        "            truncate_ref = ' '.join(ref.split()[template_len:])\n",
        "            truncate_generated_cq = ' '.join(generated_cq.split()[template_len:])\n",
        "            \n",
        "            t_hyp_b1, t_hyp_b2, t_hyp_b3, t_hyp_b4, t_hyp_m, t_hyp_r, t_hyp_c = auto_evaluation(truncate_ref, truncate_generated_cq, facet)\n",
        "\n",
        "            t_b1.append(t_hyp_b1)\n",
        "            t_b2.append(t_hyp_b2)\n",
        "            t_b3.append(t_hyp_b3)\n",
        "            t_b4.append(t_hyp_b4)\n",
        "            t_m.append(t_hyp_m)\n",
        "            t_r.append(t_hyp_r)\n",
        "            t_c.append(t_hyp_c)\n",
        "\n",
        "    else:\n",
        "        for iter, row in facet_test_data.iterrows():\n",
        "            query = facet_test_data.at[iter, 'initial_request']\n",
        "            facet = facet_test_data.at[iter, 'facet_desc']\n",
        "            ref = facet_test_data.at[iter, 'question']\n",
        "            facet_list = facet.split()\n",
        "\n",
        "            generated_cqs = []\n",
        "            for s_t in starting_texts:\n",
        "                s_t = re.sub('\\[SEP\\]', ' ', s_t).strip()\n",
        "                prompt = ' '.join(gpt3_examples[:use_examples]) + ' ' + query + ' ' + \"Ask a question that contains words in the list\" + ' ' + \"[\" + \", \".join([\"'\"+f+\"'\" for f in facet.split()])  + '].' + s_t\n",
        "                response = openai.Completion.create(\n",
        "                    model=\"text-davinci-002\",\n",
        "                    prompt= prompt,\n",
        "                    temperature=temperature,\n",
        "                    max_tokens=32,\n",
        "                    top_p=1,\n",
        "                    frequency_penalty=0.0,\n",
        "                    presence_penalty=0.0,\n",
        "                    stop=[\"\\n\"]\n",
        "                )\n",
        "\n",
        "                generated_cq = s_t + response['choices'][0]['text']\n",
        "                generated_cq = re.sub('\\[SEP\\]', ' ', generated_cq).strip()\n",
        "                generated_cq = re.sub('[.?]', '&', generated_cq).split('&')[0].strip()\n",
        "                generated_cqs.append(generated_cq)\n",
        "                all_generations.append(generated_cq)\n",
        "\n",
        "            noun_in_query = [token.text for token in pos_tagger(query) if token.pos_ == 'NOUN']\n",
        "            propn_in_query = [token.text.lower() for token in pos_tagger(query) if token.pos_ == 'PROPN']\n",
        "\n",
        "            template_scores = calculate_WSDM(query=' '.join(noun_in_query+propn_in_query+facet_list), doc_list=generated_cqs)\n",
        "            sorted_template_scores = sorted(template_scores.items(), key = lambda x: x[1], reverse=True)\n",
        "            generated_cq = sorted(template_scores.keys(), key = lambda x: template_scores[x], reverse=True)[0] \n",
        "            facet_test_data.at[iter, 'generated'] = generated_cq\n",
        "\n",
        "            if iter % sample_every == 0: \n",
        "                print(iter, query, \"-\", facet_list, '-', ' '.join(tokenized_hyp))\n",
        "                pprint.pprint(sorted_template_scores)\n",
        "        \n",
        "            # full reference evaluation\n",
        "            hyp_b1, hyp_b2, hyp_b3, hyp_b4, hyp_m, hyp_r, hyp_c = auto_evaluation(ref, generated_cq, facet)\n",
        "\n",
        "            b1.append(hyp_b1)\n",
        "            b2.append(hyp_b2)\n",
        "            b3.append(hyp_b3)\n",
        "            b4.append(hyp_b4)\n",
        "            m.append(hyp_m)\n",
        "            r.append(hyp_r)\n",
        "            c.append(hyp_c)\n",
        "\n",
        "            # question body evaluation\n",
        "            truncate_ref = ' '.join(ref.split()[template_len:])\n",
        "            truncate_generated_cq = ' '.join(generated_cq.split()[template_len:])\n",
        "            \n",
        "            t_hyp_b1, t_hyp_b2, t_hyp_b3, t_hyp_b4, t_hyp_m, t_hyp_r, t_hyp_c = auto_evaluation(truncate_ref, truncate_generated_cq, facet)\n",
        "\n",
        "            t_b1.append(t_hyp_b1)\n",
        "            t_b2.append(t_hyp_b2)\n",
        "            t_b3.append(t_hyp_b3)\n",
        "            t_b4.append(t_hyp_b4)\n",
        "            t_m.append(t_hyp_m)\n",
        "            t_r.append(t_hyp_r)\n",
        "            t_c.append(t_hyp_c)\n",
        "\n",
        "    output_df = facet_test_data[['initial_request', 'facet_desc', 'question', 'generated']]\n",
        "    output_df.columns = ['query', 'facet', 'reference', 'candidate']\n",
        "    output_df.to_csv(model_output)\n",
        "\n",
        "with open(model_output_all_templates, 'w') as outputfile:\n",
        "    for generation in all_generations:\n",
        "        outputfile.write(generation)\n",
        "        outputfile.write('\\n')\n",
        "\n",
        "# full reference results\n",
        "print(\"================================================================\")\n",
        "print(\"Full reference evaluation\")\n",
        "print(\"================================================================\")\n",
        "print(\"b1\", np.mean(b1), \"b2\", np.mean(b2), \"b3\", np.mean(b3), \"b4\", np.mean(b4))\n",
        "print(\"rouge-L\", np.mean(r))\n",
        "print(\"m\", np.mean(m))\n",
        "print(\"c\", np.mean(c))\n",
        "\n",
        "gpt3_2_b1 = np.mean(b1)\n",
        "gpt3_2_b2 = np.mean(b2)\n",
        "gpt3_2_b3 = np.mean(b3)\n",
        "gpt3_2_b4 = np.mean(b4)\n",
        "gpt3_2_m = np.mean(m)\n",
        "gpt3_2_r = np.mean(r)\n",
        "gpt3_2_c = np.mean(c)\n",
        "\n",
        "# question body results\n",
        "print(\"================================================================\")\n",
        "print(\"Question body evaluation\")\n",
        "print(\"================================================================\")\n",
        "print(\"b1\", np.mean(t_b1), \"b2\", np.mean(t_b2), \"b3\", np.mean(t_b3), \"b4\", np.mean(t_b4))\n",
        "print(\"rouge-L\", np.mean(t_r))\n",
        "print(\"m\", np.mean(t_m))\n",
        "print(\"c\", np.mean(t_c))\n",
        "\n",
        "t_gpt3_2_b1 = np.mean(t_b1)\n",
        "t_gpt3_2_b2 = np.mean(t_b2)\n",
        "t_gpt3_2_b3 = np.mean(t_b3)\n",
        "t_gpt3_2_b4 = np.mean(t_b4)\n",
        "t_gpt3_2_m = np.mean(t_m)\n",
        "t_gpt3_2_r = np.mean(t_r)\n",
        "t_gpt3_2_c = np.mean(t_c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 5.4 Three-shot GPT3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 tell me about cass county missouri - list homes sale - are you looking for a list of homes for sale in cass county missouri\n",
            "100 Find information on ontario california airport. - directions location - do you want information on how to get to the ontario california airport or information about its location\n",
            "200 Where can I buy pressure washers? - washer - are you interested in purchasing a pressure washer\n",
            "300 Tell me more about Rocky Mountain News - recent events historical - are you looking for recent events or historical events\n",
            "400 Where should I order dog clean-up bags - specif bag type - are you interested in a specific type of bag or just any bag that will work for dog clean-up\n",
            "================================================================\n",
            "Full reference evaluation\n",
            "================================================================\n",
            "b1 0.42387925535792526 b2 0.2209577594993678 b3 0.13653434343394935 b4 0.09335854470351058\n",
            "rouge-L 0.4580263700523444\n",
            "m 0.5252491996168146\n",
            "c 0.9352549019607843\n",
            "================================================================\n",
            "Question body evaluation\n",
            "================================================================\n",
            "b1 0.3899627493086948 b2 0.22707399306205175 b3 0.1470466676760886 b4 0.10557090206974408\n",
            "rouge-L 0.4363464708990771\n",
            "m 0.5505260866263759\n",
            "c 0.9187843137254902\n"
          ]
        }
      ],
      "source": [
        "facet_test_file = 'data/clariq_f/ClariQ-FKw-dev.tsv'\n",
        "facet_test_data = pd.read_csv(facet_test_file, sep='\\t')\n",
        "_, facet_test_data = process_clariq_f(facet_test_data)\n",
        "\n",
        "b1, b2, b3, b4 = [], [], [], []\n",
        "m = []\n",
        "r = []\n",
        "c = []\n",
        "\n",
        "t_b1, t_b2, t_b3, t_b4 = [], [], [], []\n",
        "t_m = []\n",
        "t_r = []\n",
        "t_c = []\n",
        "\n",
        "temperature = 0\n",
        "use_examples = 3\n",
        "model_output = 'fewshot_gpt3' + '_examples' + str(use_examples) + '_temp' + str(temperature) + '.csv'\n",
        "model_output_all_templates = 'fewshot_gpt3' + '_examples' + str(use_examples) + '_temp' + str(temperature) +'_all'\n",
        "\n",
        "all_generations = [] # cache the generations to save time and load from calling gpt3\n",
        "\n",
        "if os.path.isfile(model_output):\n",
        "    b1, b2, b3, b4, m, r, c, t_b1, t_b2, t_b3, t_b4, t_m, t_r, t_c = evaluate_from_output(model_output)\n",
        "    \n",
        "else:\n",
        "    if os.path.isfile(model_output_all_templates):\n",
        "        generated_cq_all_templates = open(model_output_all_templates, 'r').readlines()\n",
        "        generated_cq_grouped = [[generated_cq_all_templates[len(starting_texts) * k + l] \n",
        "                                    for l in range(len(starting_texts))] \n",
        "                                    for k in range(int(len(generated_cq_all_templates)/8))]\n",
        "\n",
        "        for iter, row in facet_test_data.iterrows():\n",
        "            facet = facet_test_data.at[iter, 'facet_desc']\n",
        "            ref = facet_test_data.at[iter, 'question']\n",
        "            facet_list = facet.split()\n",
        "\n",
        "            generated_cqs = []\n",
        "            for full_sentence in generated_cq_grouped[iter]:\n",
        "                query = re.sub('\\[SEP\\]', '&', full_sentence).split('&')[0].strip()\n",
        "                generated_follow_up = re.sub('\\[SEP\\]', '&', full_sentence).split('&')[1].strip()\n",
        "                generated_cq = re.sub('[.?]', '&', generated_follow_up).split('&')[0].strip()\n",
        "                generated_cqs.append(generated_cq)\n",
        "            \n",
        "            noun_in_query = [token.text for token in pos_tagger(query) if token.pos_ == 'NOUN']\n",
        "            propn_in_query = [token.text.lower() for token in pos_tagger(query) if token.pos_ == 'PROPN']\n",
        "\n",
        "            template_scores = calculate_WSDM(query=' '.join(noun_in_query+propn_in_query+facet_list), doc_list=generated_cqs)\n",
        "            sorted_template_scores = sorted(template_scores.items(), key = lambda x: x[1], reverse=True)\n",
        "            generated_cq = sorted(template_scores.keys(), key = lambda x: template_scores[x], reverse=True)[0] \n",
        "            facet_test_data.at[iter, 'generated'] = generated_cq\n",
        "            \n",
        "            if iter % sample_every == 0: \n",
        "                print(iter, query, \"-\", facet, '-', generated_cq)\n",
        "                pprint.pprint(sorted_template_scores)\n",
        "\n",
        "            # full reference evaluation\n",
        "            hyp_b1, hyp_b2, hyp_b3, hyp_b4, hyp_m, hyp_r, hyp_c = auto_evaluation(ref, generated_cq, facet)\n",
        "\n",
        "            b1.append(hyp_b1)\n",
        "            b2.append(hyp_b2)\n",
        "            b3.append(hyp_b3)\n",
        "            b4.append(hyp_b4)\n",
        "            m.append(hyp_m)\n",
        "            r.append(hyp_r)\n",
        "            c.append(hyp_c)\n",
        "\n",
        "            # question body evaluation\n",
        "            truncate_ref = ' '.join(ref.split()[template_len:])\n",
        "            truncate_generated_cq = ' '.join(generated_cq.split()[template_len:])\n",
        "            \n",
        "            t_hyp_b1, t_hyp_b2, t_hyp_b3, t_hyp_b4, t_hyp_m, t_hyp_r, t_hyp_c = auto_evaluation(truncate_ref, truncate_generated_cq, facet)\n",
        "\n",
        "            t_b1.append(t_hyp_b1)\n",
        "            t_b2.append(t_hyp_b2)\n",
        "            t_b3.append(t_hyp_b3)\n",
        "            t_b4.append(t_hyp_b4)\n",
        "            t_m.append(t_hyp_m)\n",
        "            t_r.append(t_hyp_r)\n",
        "            t_c.append(t_hyp_c)\n",
        "\n",
        "    else:\n",
        "        for iter, row in facet_test_data.iterrows():\n",
        "            query = facet_test_data.at[iter, 'initial_request']\n",
        "            facet = facet_test_data.at[iter, 'facet_desc']\n",
        "            ref = facet_test_data.at[iter, 'question']\n",
        "            facet_list = facet.split()\n",
        "\n",
        "            generated_cqs = []\n",
        "            for s_t in starting_texts:\n",
        "                s_t = re.sub('\\[SEP\\]', ' ', s_t).strip()\n",
        "                prompt = ' '.join(gpt3_examples[:use_examples]) + ' ' + query + ' ' + \"Ask a question that contains words in the list\" + ' ' + \"[\" + \", \".join([\"'\"+f+\"'\" for f in facet.split()])  + '].' + s_t\n",
        "                response = openai.Completion.create(\n",
        "                    model=\"text-davinci-002\",\n",
        "                    prompt= prompt,\n",
        "                    temperature=temperature,\n",
        "                    max_tokens=32,\n",
        "                    top_p=1,\n",
        "                    frequency_penalty=0.0,\n",
        "                    presence_penalty=0.0,\n",
        "                    stop=[\"\\n\"]\n",
        "                )\n",
        "\n",
        "                generated_cq = s_t + response['choices'][0]['text']\n",
        "                generated_cq = re.sub('\\[SEP\\]', ' ', generated_cq).strip()\n",
        "                generated_cq = re.sub('[.?]', '&', generated_cq).split('&')[0].strip()\n",
        "                generated_cqs.append(generated_cq)\n",
        "                all_generations.append(generated_cq)\n",
        "\n",
        "            noun_in_query = [token.text for token in pos_tagger(query) if token.pos_ == 'NOUN']\n",
        "            propn_in_query = [token.text.lower() for token in pos_tagger(query) if token.pos_ == 'PROPN']\n",
        "\n",
        "            template_scores = calculate_WSDM(query=' '.join(noun_in_query+propn_in_query+facet_list), doc_list=generated_cqs)\n",
        "            sorted_template_scores = sorted(template_scores.items(), key = lambda x: x[1], reverse=True)\n",
        "            generated_cq = sorted(template_scores.keys(), key = lambda x: template_scores[x], reverse=True)[0] \n",
        "            facet_test_data.at[iter, 'generated'] = generated_cq\n",
        "\n",
        "            if iter % sample_every == 0: \n",
        "                print(iter, query, \"-\", facet_list, '-', ' '.join(tokenized_hyp))\n",
        "                pprint.pprint(sorted_template_scores)\n",
        "        \n",
        "            # full reference evaluation\n",
        "            hyp_b1, hyp_b2, hyp_b3, hyp_b4, hyp_m, hyp_r, hyp_c = auto_evaluation(ref, generated_cq, facet)\n",
        "\n",
        "            b1.append(hyp_b1)\n",
        "            b2.append(hyp_b2)\n",
        "            b3.append(hyp_b3)\n",
        "            b4.append(hyp_b4)\n",
        "            m.append(hyp_m)\n",
        "            r.append(hyp_r)\n",
        "            c.append(hyp_c)\n",
        "\n",
        "            # question body evaluation\n",
        "            truncate_ref = ' '.join(ref.split()[template_len:])\n",
        "            truncate_generated_cq = ' '.join(generated_cq.split()[template_len:])\n",
        "            \n",
        "            t_hyp_b1, t_hyp_b2, t_hyp_b3, t_hyp_b4, t_hyp_m, t_hyp_r, t_hyp_c = auto_evaluation(truncate_ref, truncate_generated_cq, facet)\n",
        "\n",
        "            t_b1.append(t_hyp_b1)\n",
        "            t_b2.append(t_hyp_b2)\n",
        "            t_b3.append(t_hyp_b3)\n",
        "            t_b4.append(t_hyp_b4)\n",
        "            t_m.append(t_hyp_m)\n",
        "            t_r.append(t_hyp_r)\n",
        "            t_c.append(t_hyp_c)\n",
        "\n",
        "    output_df = facet_test_data[['initial_request', 'facet_desc', 'question', 'generated']]\n",
        "    output_df.columns = ['query', 'facet', 'reference', 'candidate']\n",
        "    output_df.to_csv(model_output)\n",
        "\n",
        "with open(model_output_all_templates, 'w') as outputfile:\n",
        "    for generation in all_generations:\n",
        "        outputfile.write(generation)\n",
        "        outputfile.write('\\n')\n",
        "\n",
        "# full reference results\n",
        "print(\"================================================================\")\n",
        "print(\"Full reference evaluation\")\n",
        "print(\"================================================================\")\n",
        "print(\"b1\", np.mean(b1), \"b2\", np.mean(b2), \"b3\", np.mean(b3), \"b4\", np.mean(b4))\n",
        "print(\"rouge-L\", np.mean(r))\n",
        "print(\"m\", np.mean(m))\n",
        "print(\"c\", np.mean(c))\n",
        "\n",
        "gpt3_3_b1 = np.mean(b1)\n",
        "gpt3_3_b2 = np.mean(b2)\n",
        "gpt3_3_b3 = np.mean(b3)\n",
        "gpt3_3_b4 = np.mean(b4)\n",
        "gpt3_3_m = np.mean(m)\n",
        "gpt3_3_r = np.mean(r)\n",
        "gpt3_3_c = np.mean(c)\n",
        "\n",
        "# question body results\n",
        "print(\"================================================================\")\n",
        "print(\"Question body evaluation\")\n",
        "print(\"================================================================\")\n",
        "print(\"b1\", np.mean(t_b1), \"b2\", np.mean(t_b2), \"b3\", np.mean(t_b3), \"b4\", np.mean(t_b4))\n",
        "print(\"rouge-L\", np.mean(t_r))\n",
        "print(\"m\", np.mean(t_m))\n",
        "print(\"c\", np.mean(t_c))\n",
        "\n",
        "t_gpt3_3_b1 = np.mean(t_b1)\n",
        "t_gpt3_3_b2 = np.mean(t_b2)\n",
        "t_gpt3_3_b3 = np.mean(t_b3)\n",
        "t_gpt3_3_b4 = np.mean(t_b4)\n",
        "t_gpt3_3_m = np.mean(t_m)\n",
        "t_gpt3_3_r = np.mean(t_r)\n",
        "t_gpt3_3_c = np.mean(t_c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 5.5 Ours\n",
        "Same as 1.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 5.6 Comparing 3.1-3.5, get the conclusion for RQ3. \"Our approach is a good replacement for GPT3.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------------------------------\n",
            "|                               Full reference evaluation                               |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|MODEL                    BLEU1    BLEU2    BLEU3    BLEU4    METEOR   ROUGE    COVERAGE|\n",
            "-----------------------------------------------------------------------------------------\n",
            "|Ours-WSDM                41.8     17.57    9.71     6.45     38.52    44.19    98.96   |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|Ours-AutoScore           43.42    20.47    12.93    9.64     41.03    47.87    98.28   |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|Zero-shot GPT3           42.96    21.45    12.98    8.76     45.95    46.69    85.27   |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|One-shot GPT3            45.45    23.04    14.53    10.29    46.55    48.21    95.44   |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|Two-shot GPT3            42.32    21.97    13.48    9.18     52.39    45.78    93.56   |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|Three-shot GPT3          42.39    22.1     13.65    9.34     52.52    45.8     93.53   |\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "|                                Question body evaluation                               |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|MODEL                    BLEU1    BLEU2    BLEU3    BLEU4    METEOR   ROUGE    COVERAGE|\n",
            "-----------------------------------------------------------------------------------------\n",
            "|Ours-WSDM                38.51    18.17    10.81    8.46     37.47    43.79    98.67   |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|Ours-AutoScore           34.88    16.25    10.47    8.07     33.88    43.42    95.83   |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|Zero-shot GPT3           39.44    22.82    15.28    11.94    46.36    46.21    82.95   |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|One-shot GPT3            42.53    23.74    15.82    11.9     45.98    47.59    93.83   |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|Two-shot GPT3            38.94    22.64    14.55    10.37    55.01    43.63    91.92   |\n",
            "-----------------------------------------------------------------------------------------\n",
            "|Three-shot GPT3          39.0     22.71    14.7     10.56    55.05    43.63    91.88   |\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|                               Full reference evaluation                               |\")\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('MODEL', 'BLEU1','BLEU2','BLEU3','BLEU4','METEOR','ROUGE','COVERAGE'))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('Ours-WSDM', \n",
        "                                                            round_metric(zero_nd_wsdm_b1), \n",
        "                                                            round_metric(zero_nd_wsdm_b2), \n",
        "                                                            round_metric(zero_nd_wsdm_b3), \n",
        "                                                            round_metric(zero_nd_wsdm_b4), \n",
        "                                                            round_metric(zero_nd_wsdm_m), \n",
        "                                                            round_metric(zero_nd_wsdm_r), \n",
        "                                                            round_metric(zero_nd_wsdm_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('Ours-AutoScore', \n",
        "                                                            round_metric(zero_nd_auto_b1), \n",
        "                                                            round_metric(zero_nd_auto_b2), \n",
        "                                                            round_metric(zero_nd_auto_b3), \n",
        "                                                            round_metric(zero_nd_auto_b4), \n",
        "                                                            round_metric(zero_nd_auto_m), \n",
        "                                                            round_metric(zero_nd_auto_r), \n",
        "                                                            round_metric(zero_nd_auto_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('Zero-shot GPT3', \n",
        "                                                            round_metric(gpt3_0_b1), \n",
        "                                                            round_metric(gpt3_0_b2), \n",
        "                                                            round_metric(gpt3_0_b3), \n",
        "                                                            round_metric(gpt3_0_b4), \n",
        "                                                            round_metric(gpt3_0_m), \n",
        "                                                            round_metric(gpt3_0_r),\n",
        "                                                            round_metric(gpt3_0_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('One-shot GPT3', \n",
        "                                                            round_metric(gpt3_1_b1), \n",
        "                                                            round_metric(gpt3_1_b2), \n",
        "                                                            round_metric(gpt3_1_b3), \n",
        "                                                            round_metric(gpt3_1_b4), \n",
        "                                                            round_metric(gpt3_1_m), \n",
        "                                                            round_metric(gpt3_1_r),\n",
        "                                                            round_metric(gpt3_1_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('Two-shot GPT3', \n",
        "                                                            round_metric(gpt3_2_b1), \n",
        "                                                            round_metric(gpt3_2_b2), \n",
        "                                                            round_metric(gpt3_2_b3), \n",
        "                                                            round_metric(gpt3_2_b4), \n",
        "                                                            round_metric(gpt3_2_m), \n",
        "                                                            round_metric(gpt3_2_r),\n",
        "                                                            round_metric(gpt3_2_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('Three-shot GPT3', \n",
        "                                                            round_metric(gpt3_3_b1), \n",
        "                                                            round_metric(gpt3_3_b2), \n",
        "                                                            round_metric(gpt3_3_b3), \n",
        "                                                            round_metric(gpt3_3_b4), \n",
        "                                                            round_metric(gpt3_3_m), \n",
        "                                                            round_metric(gpt3_3_r),\n",
        "                                                            round_metric(gpt3_3_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|                                Question body evaluation                               |\")\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('MODEL', 'BLEU1','BLEU2','BLEU3','BLEU4','METEOR','ROUGE','COVERAGE'))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('Ours-WSDM', \n",
        "                                                            round_metric(t_zero_nd_wsdm_b1), \n",
        "                                                            round_metric(t_zero_nd_wsdm_b2), \n",
        "                                                            round_metric(t_zero_nd_wsdm_b3), \n",
        "                                                            round_metric(t_zero_nd_wsdm_b4), \n",
        "                                                            round_metric(t_zero_nd_wsdm_m), \n",
        "                                                            round_metric(t_zero_nd_wsdm_r), \n",
        "                                                            round_metric(t_zero_nd_wsdm_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('Ours-AutoScore', \n",
        "                                                            round_metric(t_zero_nd_auto_b1), \n",
        "                                                            round_metric(t_zero_nd_auto_b2), \n",
        "                                                            round_metric(t_zero_nd_auto_b3), \n",
        "                                                            round_metric(t_zero_nd_auto_b4), \n",
        "                                                            round_metric(t_zero_nd_auto_m), \n",
        "                                                            round_metric(t_zero_nd_auto_r), \n",
        "                                                            round_metric(t_zero_nd_auto_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('Zero-shot GPT3', \n",
        "                                                            round_metric(t_gpt3_0_b1), \n",
        "                                                            round_metric(t_gpt3_0_b2), \n",
        "                                                            round_metric(t_gpt3_0_b3), \n",
        "                                                            round_metric(t_gpt3_0_b4), \n",
        "                                                            round_metric(t_gpt3_0_m), \n",
        "                                                            round_metric(t_gpt3_0_r),\n",
        "                                                            round_metric(t_gpt3_0_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('One-shot GPT3', \n",
        "                                                            round_metric(t_gpt3_1_b1), \n",
        "                                                            round_metric(t_gpt3_1_b2), \n",
        "                                                            round_metric(t_gpt3_1_b3), \n",
        "                                                            round_metric(t_gpt3_1_b4), \n",
        "                                                            round_metric(t_gpt3_1_m), \n",
        "                                                            round_metric(t_gpt3_1_r),\n",
        "                                                            round_metric(t_gpt3_1_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('Two-shot GPT3', \n",
        "                                                            round_metric(t_gpt3_2_b1), \n",
        "                                                            round_metric(t_gpt3_2_b2), \n",
        "                                                            round_metric(t_gpt3_2_b3), \n",
        "                                                            round_metric(t_gpt3_2_b4), \n",
        "                                                            round_metric(t_gpt3_2_m), \n",
        "                                                            round_metric(t_gpt3_2_r),\n",
        "                                                            round_metric(t_gpt3_2_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")\n",
        "print(\"|{:<24} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}|\".format('Three-shot GPT3', \n",
        "                                                            round_metric(t_gpt3_3_b1), \n",
        "                                                            round_metric(t_gpt3_3_b2), \n",
        "                                                            round_metric(t_gpt3_3_b3), \n",
        "                                                            round_metric(t_gpt3_3_b4), \n",
        "                                                            round_metric(t_gpt3_3_m), \n",
        "                                                            round_metric(t_gpt3_3_r),\n",
        "                                                            round_metric(t_gpt3_3_c)))\n",
        "print(\"-----------------------------------------------------------------------------------------\")"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "hug",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13 (default, Oct 18 2022, 18:57:03) \n[GCC 11.2.0]"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "b52474cade4d616deb055367e9a91eaa1db3cb49ba2d9f58d2338a55e531acc2"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
